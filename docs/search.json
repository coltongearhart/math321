[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MATH 321 –Mathematical Statistics",
    "section": "",
    "text": "Overview",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "index.html#overview-1",
    "href": "index.html#overview-1",
    "title": "MATH 321 –Mathematical Statistics",
    "section": "Overview",
    "text": "Overview\n\nBelow is the course calendar. NOTE this calendar is for the SP-24 semester.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "index.html#course-pack",
    "href": "index.html#course-pack",
    "title": "MATH 321 –Mathematical Statistics",
    "section": "Course Pack",
    "text": "Course Pack\nFILES: BLANK lectures-all.pdf, lectures-all.tex, lectures-completed.tex, and style-lectures.sty",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "index.html#assessments",
    "href": "index.html#assessments",
    "title": "MATH 321 –Mathematical Statistics",
    "section": "Assessments",
    "text": "Assessments\nFILES: assessments-all.tex and style-assessments.sty\nInstructors can contact me for keys, solutions, and tests :)",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "index.html#source",
    "href": "index.html#source",
    "title": "MATH 321 –Mathematical Statistics",
    "section": "Source",
    "text": "Source\n\n\n\n\n\n\nQuarto blog publish details\n\n\n\nThis book was created using Quarto and published with Github Pages.\n\n\n\n\n\n\n\n\nGithub repository for code\n\n\n\nYou can find the code to reproduce this project at coltongearhart/math321.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "FILE: syllabus.tex and style-syllabus.sty",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "test-1.html",
    "href": "test-1.html",
    "title": "Test 1",
    "section": "",
    "text": "This section contains all of the content and assessments covered on the first test.",
    "crumbs": [
      "Test 1"
    ]
  },
  {
    "objectID": "lecture-14.html",
    "href": "lecture-14.html",
    "title": "Lecture 14 – Bivariate Distributions",
    "section": "",
    "text": "Guided Notes\nFILES: BLANK lecture-14.pdf, lecture-14.tex, and style-lectures.sty",
    "crumbs": [
      "Test 1",
      "Lecture 14 -- Bivariate Distributions"
    ]
  },
  {
    "objectID": "lecture-14.html#in-class-activities",
    "href": "lecture-14.html#in-class-activities",
    "title": "Lecture 14 – Bivariate Distributions",
    "section": "In-Class Activities",
    "text": "In-Class Activities\nFILES: in-class-14.tex and style-assessments.sty\nInstructors can contact me for keys :)",
    "crumbs": [
      "Test 1",
      "Lecture 14 -- Bivariate Distributions"
    ]
  },
  {
    "objectID": "lecture-14.html#homework",
    "href": "lecture-14.html#homework",
    "title": "Lecture 14 – Bivariate Distributions",
    "section": "Homework",
    "text": "Homework\nFILES: homework-14.tex and style-assessments.sty\nInstructors can contact me for solutions :)",
    "crumbs": [
      "Test 1",
      "Lecture 14 -- Bivariate Distributions"
    ]
  },
  {
    "objectID": "lecture-15.html",
    "href": "lecture-15.html",
    "title": "Lecture 15 – Conditional Distributions",
    "section": "",
    "text": "Guided Notes\nFILES: BLANK lecture-15.pdf, lecture-15.tex, and style-lectures.sty",
    "crumbs": [
      "Test 1",
      "Lecture 15 -- Conditional Distributions"
    ]
  },
  {
    "objectID": "lecture-15.html#in-class-activities",
    "href": "lecture-15.html#in-class-activities",
    "title": "Lecture 15 – Conditional Distributions",
    "section": "In Class Activities",
    "text": "In Class Activities\nFILES: in-class-15.tex and style-assessments.sty\nInstructors can contact me for keys :)",
    "crumbs": [
      "Test 1",
      "Lecture 15 -- Conditional Distributions"
    ]
  },
  {
    "objectID": "lecture-15.html#homework",
    "href": "lecture-15.html#homework",
    "title": "Lecture 15 – Conditional Distributions",
    "section": "Homework",
    "text": "Homework\nFILES: homework-15.tex and style-assessments.sty\nInstructors can contact me for solutions :)",
    "crumbs": [
      "Test 1",
      "Lecture 15 -- Conditional Distributions"
    ]
  },
  {
    "objectID": "lecture-16.html",
    "href": "lecture-16.html",
    "title": "Lecture 16 – Independence and the Correlation Coefficient",
    "section": "",
    "text": "Guided Notes\nFILES: BLANK lecture-16.pdf, lecture-16.tex, and style-lectures.sty",
    "crumbs": [
      "Test 1",
      "Lecture 16 -- Independence and the Correlation Coefficient"
    ]
  },
  {
    "objectID": "lecture-16.html#in-class-activity",
    "href": "lecture-16.html#in-class-activity",
    "title": "Lecture 16 – Independence and the Correlation Coefficient",
    "section": "In-Class Activity",
    "text": "In-Class Activity\nFILES: in-class-16.tex and style-assessments.sty\nInstructors can contact me for keys :)",
    "crumbs": [
      "Test 1",
      "Lecture 16 -- Independence and the Correlation Coefficient"
    ]
  },
  {
    "objectID": "lecture-16.html#homework",
    "href": "lecture-16.html#homework",
    "title": "Lecture 16 – Independence and the Correlation Coefficient",
    "section": "Homework",
    "text": "Homework\nFILES: homework-16.tex and style-assessments.sty\nInstructors can contact me for solutions :)",
    "crumbs": [
      "Test 1",
      "Lecture 16 -- Independence and the Correlation Coefficient"
    ]
  },
  {
    "objectID": "lecture-17.html",
    "href": "lecture-17.html",
    "title": "Lecture 17 – Several Random Variables",
    "section": "",
    "text": "Guided Notes\nFILES: BLANK lecture-17.pdf, lecture-17.tex, and style-lectures.sty",
    "crumbs": [
      "Test 1",
      "Lecture 17 -- Several Random Variables"
    ]
  },
  {
    "objectID": "lecture-17.html#in-class-activity",
    "href": "lecture-17.html#in-class-activity",
    "title": "Lecture 17 – Several Random Variables",
    "section": "In-Class Activity",
    "text": "In-Class Activity\nFILES: in-class-17.tex and style-assessments.sty\nInstructors can contact me for keys :)",
    "crumbs": [
      "Test 1",
      "Lecture 17 -- Several Random Variables"
    ]
  },
  {
    "objectID": "review-1.html",
    "href": "review-1.html",
    "title": "Review – Test 1",
    "section": "",
    "text": "Study Guide\nFILES: study-guide-1.tex and style-assessments.sty",
    "crumbs": [
      "Test 1",
      "Review -- Test 1"
    ]
  },
  {
    "objectID": "review-1.html#review-problems",
    "href": "review-1.html#review-problems",
    "title": "Review – Test 1",
    "section": "Review Problems",
    "text": "Review Problems",
    "crumbs": [
      "Test 1",
      "Review -- Test 1"
    ]
  },
  {
    "objectID": "test-2.html",
    "href": "test-2.html",
    "title": "Test 2",
    "section": "",
    "text": "This section contains all of the content and assessments covered on the second test.",
    "crumbs": [
      "Test 2"
    ]
  },
  {
    "objectID": "lecture-1.html",
    "href": "lecture-1.html",
    "title": "Lecture 1 – Random Samples and Common Statistics",
    "section": "",
    "text": "Guided Notes\nFILES: BLANK lecture-1.pdf, lecture-1.tex, and style-lectures.sty",
    "crumbs": [
      "Test 2",
      "Lecture 1 -- Random Samples and Common Statistics"
    ]
  },
  {
    "objectID": "lecture-1.html#in-class-activity",
    "href": "lecture-1.html#in-class-activity",
    "title": "Lecture 1 – Random Samples and Common Statistics",
    "section": "In-Class Activity",
    "text": "In-Class Activity\nFILES: in-class-1.tex and style-assessments.sty\nInstructors can contact me for keys :)",
    "crumbs": [
      "Test 2",
      "Lecture 1 -- Random Samples and Common Statistics"
    ]
  },
  {
    "objectID": "lecture-1.html#out-class-activity",
    "href": "lecture-1.html#out-class-activity",
    "title": "Lecture 1 – Random Samples and Common Statistics",
    "section": "Out-Class Activity",
    "text": "Out-Class Activity\nFILES: out-class-0.zip\n\nFollow along with the information (and watch the video) on the following page: amazing site 1\nWatch the video on the following page: amazing site 2\nUsing a .qmd file, create and render an html file that has the content shown below (optionally google how to insert pictures and insert cat meme :)\nSubmit your html file to canvas.",
    "crumbs": [
      "Test 2",
      "Lecture 1 -- Random Samples and Common Statistics"
    ]
  },
  {
    "objectID": "lecture-1.html#r-notes",
    "href": "lecture-1.html#r-notes",
    "title": "Lecture 1 – Random Samples and Common Statistics",
    "section": "R Notes",
    "text": "R Notes\n\nIntro to R\nFILES: intro-to-r-STARTER.qmd and intro-to-r.html\n\nOverview\nThis document is built to get us started with working in R.\n\n\nBasic operations\nCan use R as a fancy calculator.\n\n# perform some calculations\n2 + 2\n\n[1] 4\n\n5 * 3\n\n[1] 15\n\n15 / 3\n\n[1] 5\n\n10^3\n\n[1] 1000\n\nexp(10) # e^10\n\n[1] 22026.47\n\n\nGood programming practice is to name objects intuitively so that your code is readable.\n\n# name object\ngrade &lt;- 85\ngrade + 5\n\n[1] 90\n\n\n\n\nVectors\nA vector is a long string of values held together.\n\n# make a vector of names and vector of corresponding grades\ngrades &lt;- c(85,80,92,98,77)\ngrades\n\n[1] 85 80 92 98 77\n\nstudents &lt;- c(\"a\",\"b\",\"c\",\"d\",\"e\")\nstudents\n\n[1] \"a\" \"b\" \"c\" \"d\" \"e\"\n\n\nTo check data types, use the class() or str() functions. To check what each function does, we can bring up the help page with ?&lt;function name&gt;.\n\n# show help page\n?class\n?str\n\n\n# check data types\nclass(grades)\n\n[1] \"numeric\"\n\nstr(grades)\n\n num [1:5] 85 80 92 98 77\n\nclass(students)\n\n[1] \"character\"\n\nstr(students)\n\n chr [1:5] \"a\" \"b\" \"c\" \"d\" \"e\"\n\n\nWhen working with vectors, legal operations depend on the data types.\n\n# perform operations on vectors\ngrades + 5\n\n[1]  90  85  97 103  82\n\nmean(grades)\n\n[1] 86.4\n\nstudents + 5 # can't do addition with words\n\nError in students + 5: non-numeric argument to binary operator\n\npaste(\"Student\", students) # can append strings \n\n[1] \"Student a\" \"Student b\" \"Student c\" \"Student d\" \"Student e\"\n\n\nTo subset vectors, use “square bracket indexing”. We can get creative when indexing and also specify multiple indices using the c() function. Another quick trick for integer counting vector using : operator.\n\n# subset vectors in various ways\nstudents[1]\n\n[1] \"a\"\n\nstudents[-1] # print all except the first value\n\n[1] \"b\" \"c\" \"d\" \"e\"\n\ngrades[c(1,3,5)]\n\n[1] 85 92 77\n\ngrades[-c(2,4)]\n\n[1] 85 92 77\n\n# use \":\" operator to subset\n2:5\n\n[1] 2 3 4 5\n\nstudents[2:5]\n\n[1] \"b\" \"c\" \"d\" \"e\"\n\n\nBoolean (aka logical) vectors are another data type. These are super useful for referencing items with characteristics we want to keep/remove.\n\n# create a boolean vector and use it to subset a vector based on a condition\ngrades &gt; 90 # evaluate whether value is greater than 90\n\n[1] FALSE FALSE  TRUE  TRUE FALSE\n\ngrades[grades &gt; 90] # show only values that meet condition\n\n[1] 92 98\n\nstudents[grades &gt; 90]\n\n[1] \"c\" \"d\"\n\n\n\n\nFunctions\nFunctions take some object and do something with it.\n\n# example functions we have already used\nstr(names)\n\nfunction (x)  \n\nmean(grades)\n\n[1] 86.4\n\nc(1,3,5)\n\n[1] 1 3 5\n\n\nCan also define our own functions and then use them like normal.\n\n# write a function to add half of missed points back to all grades and calculate the average after the curve\ncurved_mean &lt;- function(x){\n  mean(0.5 * x + 50)\n}\n\n# check how much the curve impacted the average grade\ncurved_mean(grades) - mean(grades)\n\n[1] 6.8\n\n\nNesting functions is very useful for “chaining” results together.\n\n# example of nesting functions\nx &lt;- c(-1, 3, 1, 6, 4, 9, -3, -2)\nmean(abs(x))\n\n[1] 3.625\n\n# -&gt; this is equivalent to saving this first (inside) result, then using that as the argument for the next (outside) function\ny &lt;- abs(x)\nmean(y)\n\n[1] 3.625\n\n\n\n\nData frames\nWe can think of data frames as a rectangular spreadsheet, row = observation and column = variable. These can be created easily with data.frame().\n\n# create a dataframe by combining vectors of the same length\ndata_grades &lt;- data.frame(students, grades)\ndata_grades\n\n  students grades\n1        a     85\n2        b     80\n3        c     92\n4        d     98\n5        e     77\n\n# can specify the column names\ndata_grades &lt;- data.frame(Name = students, Before_Curve = grades)\ndata_grades\n\n  Name Before_Curve\n1    a           85\n2    b           80\n3    c           92\n4    d           98\n5    e           77\n\n\nTo focus on a subset of a data frame, use square brackets defining [rows,cols].\n\n# subset dataframe in various ways\ndata_grades[1:3, ] # rows 1-3, all columns\n\n  Name Before_Curve\n1    a           85\n2    b           80\n3    c           92\n\ndata_grades[ , 1] # all rows, column 1\n\n[1] \"a\" \"b\" \"c\" \"d\" \"e\"\n\ndata_grades[-c(1,3), -1] # exclude rows 1-3, exclude column 1\n\n[1] 80 98 77\n\n\nWe can also use dataframes that were read in from spreadsheets or R packages.\n\n# look at pre-loaded dataframe\n# -&gt; notice that each column is a vector with its own data type\ndata(\"iris\")\nstr(iris)\n\n'data.frame':   150 obs. of  5 variables:\n $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n $ Species     : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1 1 1 1 1 ...\n\n\n\n\n\nProbability\nFILES: probability-STARTER.qmd and probability.html\n\nOverview\nThis document will investigate probability calculations based on some of the discrete and continuous distributions we learned in class. We will calculate them manually to learn more about basic calculations and function writing in R and then use R distribution functions. Additionally we will make some basic plots.\nThe main notes include the binomial, geometric, exponential, poisson, and normal distributions. The extra notes include the hypergeometric, negative binomial and gamma distributions.\n\n\nBinomial distribution\nFirst we will calculate binomial probabilities manually using the pdf.\nIf \\(X \\sim \\text{Binomial}(n,p)\\), then \\(f(x) = {n \\choose x}\\, p^x\\, (1 - p)^{n - x}\\). If using this for calculations in R, we need to use the choose() function to do the combination, and then could write choose(n,x) * p^x * (1-p)^(n-x).\nTo practice this, let \\(X \\sim \\text{Binomial}(n = 10, p = 0.3)\\).\nIt is good programming practice to not hard-code anything, use variables (especially when repeating calculations).\n\n# initialize parameters\nn &lt;- 10\np &lt;- 0.3\n\nNow we can calculate several different probabilities the same way we do by hand:\n\\[P(X \\in A) = \\sum_{x \\in A} f(x)\\]\nWe just need to calculate find \\(f(x)\\) for all of the \\(x\\) values of interest and then add with sum(). Remember that we are working with a discrete random variable. So we need to be careful with \\(&gt;\\) and \\(&lt;\\) vs \\(\\ge\\) and \\(\\le\\).\nWe can also use the complement rule as needed:\n\\[P(X \\in A) = 1 - \\sum_{x \\notin A} f(x)\\]\n\n# P(X = 2)\nx &lt;- 2\nchoose(n,x) * p^x * (1-p)^(n-x)\n\n[1] 0.2334744\n\n# P(X &gt;= 2) = 1 - P(X &lt;= 1)\nx &lt;- 2:n\nsum(choose(n,x) * p^x * (1-p)^(n-x))\n\n[1] 0.8506917\n\nx &lt;- 0:1\n1 - sum(choose(n,x) * p^x * (1-p)^(n-x))\n\n[1] 0.8506917\n\n# P(X &lt; 5)\nx &lt;- 0:4\nsum(choose(n,x) * p^x * (1-p)^(n-x))\n\n[1] 0.8497317\n\n\nIf you notice you are doing the same thing over and over, we can write a function to make it easier to reuse.\n\n# write function for the binomial pdf\nf_x_binom &lt;- function(n,p,x) {\n  choose(n,x) * p^x * (1-p)^(n-x)\n}\n\nThen we can use this to find probabilities the same way.\nNote that when using the function arguments are used by position they are specified in if they are not named. So better practice is to name the arguments to be safe.\n\n# find P(X &lt; 5) using your function\nsum(f_x_binom(10, 0.3, 0:4))\n\n[1] 0.8497317\n\nsum(f_x_binom(n = 10, p = 0.3, x = 0:4))\n\n[1] 0.8497317\n\n\nR has lots of built in functions for distributions. This is the recommended way to do all the calculations.\nFor all of the distributions there are 4 common functions:\n\nd&lt;dist&gt;() gives the density \\(f(x)\\) value.\np&lt;dist&gt;() gives the cdf probability \\(F(x) = P(X \\le x)\\).\nq&lt;dist&gt;() gives the percentile value, the \\(x\\) such that \\(P(X \\le x) = p\\).\nr&lt;dist&gt;() generates a random value from the distribution.\n\nSo for the binomial distribution, to calculate probabilities using the pdf, we need to use dbinom(). Because this is a discrete distribution, this will return \\(f(x) = P(X = x)\\).\n\n# find P(X &lt; 5) with R function for the pdf\ndbinom(x = 0:4, size = 10, prob = 0.3)\n\n[1] 0.02824752 0.12106082 0.23347444 0.26682793 0.20012095\n\nsum(dbinom(x = 0:4, size = 10, prob = 0.3))\n\n[1] 0.8497317\n\n\nTo use the binomial cdf, which gives \\(F(x) = P(X \\le x)\\), use pbinom().\n\n# find P(X &lt; 5) with R function for the cdf\npbinom(q = 4, size = 10, prob = 0.3)\n\n[1] 0.8497317\n\n\nBy default, pbinom() gives the left-tail probability. We can also specify the argument lower.tail = FALSE to return the right-tail probability (survival probability) $S(x) = P(X &gt; x) = 1 - F(x) = $.\nNote that lower.tail = TRUE is the default value, which means that is what it is set to even if we don’t specify it.\n\n# find P(X &gt; 5) using pbinom() both ways (right tail and complement of cdf)\npbinom(q = 5, size = 10, prob = 0.3, lower.tail = FALSE)\n\n[1] 0.04734899\n\n1 - pbinom(q = 5, size = 10, prob = 0.3)\n\n[1] 0.04734899\n\n\nIt is also really easy to visualize distributions in R by creating simple plots. To create a plot, we use the plot() function. This function can make many different plots depending on the arguments we specify. To make a discrete pmf plot, we want type = \"h\".\nNow let \\(X \\sim \\text{Binomial}(n = 12, p = 0.4)\\).\n\n# initialize parameters and range of random variable\nn &lt;- 12\np &lt;- 0.4\nx &lt;- 0:n\n\n# now plot and add labels\nplot(x = x, y = dbinom(x = x, size = n, prob = p), type = \"h\", xlab = \"x\", ylab = \"f(x)\", main = \"Binomial(12,0.4) pmf\")\n\n\n\n\n\n\n\n\nTo practice more calculations, we can manually find the expected value of the binomial distribution, which we know should equal \\(E(X) = np\\).\nFor any discrete random variable, \\[E(X) = \\sum x f(x)\\]. This can easily be done with vector calculations.\n\n# calculate expected value (by definition)\n# -&gt; check against binomial shortcut formula\nsum(x * dbinom(x = x, size = n, prob = p))\n\n[1] 4.8\n\nn * p\n\n[1] 4.8\n\n\n\n\nGeometric distribution\nIf \\(X \\sim \\text{Geometric}(p)\\), then \\(f(x) = (1-p)^{x-1} p\\).\nTo practice this, let \\(X \\sim \\text{Geometric}(p = 0.3)\\).\nAgain we will initialize the parameter to reuse.\n\n# initialize parameter\np &lt;- 0.3\n\nNow we can calculate some probabilities manually using the pdf just like with the binomial distribution.\n\n# P(X = 3)\nx &lt;- 3\n(1-p)^(x-1) * p\n\n[1] 0.147\n\n\nKeep in mind that our geometric random variable is counting the Number of trials to get the first success, which means the range is \\(\\cal{X} = 1, 2, \\ldots\\) and is therefore unbounded. So to find a right-tailed probability manually, we need to use the complement.\n\n# P(X &gt;= 4) = 1 - P(X &lt;= 3)\nx &lt;- 1:3\n1 - sum((1-p)^(x-1) * p)\n\n[1] 0.343\n\n\nOr we can write our own geometric pmf function for geometric to use.\n\n# write your own function\nf_x_geo &lt;- function(p,x) {\n  (1-p)^(x-1) * p\n}\n\n# find P(X &lt; 5) = P(X &lt;= 4) using your function\nsum(f_x_geo(p = 0.3, x = 1:4))\n\n[1] 0.7599\n\n\nBuilt in R functions are however the recommended way to calculate probabilities for distributions. We just have to make sure we are using them correctly.\nJust like with the binomial, we will use the d&lt;dist&gt;() to find the \\(f(x)\\) probabilities of interest. For geometric, it is dgeom().\nBut first we need to look at the help documentation ?dgeom() in the ‘Details’ section. Here we see that R is using the alternate form of the geometric distribution that is counting the number of failures until the first success (\\(f(y) = (1-p)^y p, \\, y = 0,1,\\ldots\\)). As a result, we need to convert our \\(x\\) values to \\(y = x - 1\\) before using this function.\n\n# find P(X &lt; 5) using R functions\n# -&gt; transform x (number of trials) to y = x - 1 (number of failures) first\nx &lt;- 1:4\ny &lt;- x - 1\nsum(dgeom(x = y, prob = 0.3))\n\n[1] 0.7599\n\n\nThen we can use the cdf function pgeom() to find \\(F(x) = P(X \\le x) = P(Y \\le x - 1)\\).\n\n# P(X &lt; 5) = P(Y &lt; 4) = P(Y &lt;= 3)\npgeom(q = 3, prob = 0.3)\n\n[1] 0.7599\n\n\nWe can visualize the geometric pmf in the same way as the binomial. Because of the unbounded range, we can just define a “reasonable” range of \\(y = x - 1\\) values.\n\n# let X ~ geometric(p = 0.5)\n# define the (reasonable) range of the random variable\nx &lt;- 1:15\ny &lt;- x - 1\nplot(x = x, y = dgeom(x = y, prob = 0.5), type = \"h\", xlab = \"x\", ylab = \"f(x)\", main = \"Geometric(0.5) pmf\")\n\n\n\n\n\n\n\n\n\n\nExponential distribution\nIf \\(X \\sim \\text{Exponential}(\\lambda)\\), then \\(f(x) = \\lambda e^{-\\lambda x}\\).\nTo practice this, let \\(X \\sim \\text{Exponential}(\\lambda = 0.75)\\).\nFor continuous distributions, the code is more or less the same. Just a few things we have to take into account because of the change in type of variable.\nFor a continuous distribution like exponential, we have to integrate to find probabilities manually. R can do integrals with the integral() function, but much easier to use the specific distribution functions.\nFor continuous distributions, the density function d&lt;dist&gt;() still returns \\(f(x)\\), but now this value is just used to define the height of the density curve. So we do not want to use dexp() to find probabilities via the pdf.\nAll probabilities should be found using distribution (cdf) function p&lt;dist&gt;(), which for exponential is pexp(). Thus returns \\(F(x) = P(X \\le x)\\) and the survival probability with \\(S(x) = 1 - F(x)\\) with lower.tail = FALSE. The exponential functions use the rate parameterization of the distribution, so rate = lambda.\nWe can calculate all of the following different kinds of probabilities using the cdf:\n\nRemember continuous variable, so \\(&lt; &gt;\\) or \\(\\le \\ge\\) doesn’t matter: \\(P(X &lt; x) = P(X \\le x)\\):\nRight-tailed probability, there are two ways to get this: \\(P(X \\ge x) = S(x)\\).\nInterval probability: \\(P(a \\le X \\le b) = F(b) - F(a)\\).\n\n\n# P(X &lt; 3)\npexp(q = 3, rate = 0.75)\n\n[1] 0.8946008\n\n# P(X &gt;= 4)\npexp(q = 4, rate = 0.75, lower.tail = FALSE)\n\n[1] 0.04978707\n\n# P(2.5 &lt; X &lt; 7)\npexp(q = 7, rate = 0.75) - pexp(q = 2.5, rate = 0.75)\n\n[1] 0.1481074\n\n\nIf visualizing a continuous distribution, we can use seq() to specify more \\(x\\) values (with smaller “steps”) and use dexp() to specify the y = f(x) values of the density curve.\n\n# visualize the distribution\n# -&gt; specify reasonable range of the continuous random variable\nx &lt;- seq(from = 0, to = 5, by = 0.01)\nplot(x = x, y =  dexp(x, rate = 0.75), type = \"l\", xlab = \"x\", ylab = \"f(x)\", main = \"Exponential(0.75) pdf\")\n\n\n\n\n\n\n\n\n\n\nPoisson distribution\nIf \\(X \\sim \\text{Poisson}(\\lambda)\\), then \\(f(x) = \\frac{\\mathrm{e}^{-\\lambda}  \\lambda^x}{x!}\\).\n(Would need exp() and factorial() functions to calculate this manually)\nTo practice this, let \\(X \\sim \\text{Poisson}(\\lambda = 5)\\).\nFind the following probabilities using both dpois() and ppois() (answers should match).\n\n# P(X &lt;= 7)\nsum(dpois(x = 0:7, lambda = 5))\n\n[1] 0.8666283\n\nppois(q = 7, lambda = 5)\n\n[1] 0.8666283\n\n# P(X &gt; 4) = 1 - P(X &lt;= 4)\n1 - sum(dpois(x = 0:4, lambda = 5))\n\n[1] 0.5595067\n\n1 - ppois(q = 4, lambda = 5)\n\n[1] 0.5595067\n\nppois(q = 4, lambda = 5, lower.tail = FALSE)\n\n[1] 0.5595067\n\n\nNow we can visualize the Poisson pmf.\n\n# create a plot to visualize the pmf\n# define the (reasonable) range of the random variable\nx &lt;- 0:20\nplot(x = x, y = dpois(x = x, lambda = 5), type = \"h\", xlab = \"x\", ylab = \"f(x)\", main = \"Poisson(5) pmf\")\n\n\n\n\n\n\n\n\n\n\nNormal distribution\nLet \\(X \\sim \\text{Normal}(\\mu = 10, \\sigma^2 = 4)\\).\nCalculate the following probabilities.\n\n# P(X &lt; 8)\n# -&gt; pnorm() requires the sd, not the variance\npnorm(q = 8, mean = 10, sd = 2)\n\n[1] 0.1586553\n\n# P(X &gt;= 9)\npnorm(q = 9, mean = 10, sd = 2, lower.tail = FALSE)\n\n[1] 0.6914625\n\n1 - pnorm(q = 9, mean = 10, sd = 2, lower.tail = TRUE)\n\n[1] 0.6914625\n\n# P(7 &lt; X &lt; 13)\n# -&gt; interval probability -&gt; F(x2) - F(x1)\npnorm(q = 13, mean = 10, sd = 2) - pnorm(q = 7, mean = 10, sd = 2)\n\n[1] 0.8663856\n\n# P(Z &gt; 1)\n# -&gt; Z ~ Normal(mu = 1, sd = 1), so just change the arguments to match the new normal dist\npnorm(q = 1, mean = 0, sd = 1)\n\n[1] 0.8413447\n\n\nVisualize the distribution with dnorm() to specify the \\(y = f(x)\\) values of the density curve.\n\n# visualize the distribution\nx &lt;- seq(from = 3, to = 17, by = 0.01)\nplot(x = x, y =  dnorm(x, mean = 10, sd = 2), type = \"l\", xlab = \"x\", ylab = \"f(x)\", main = \"Normal(10,2) pdf\")\n\n\n\n\n\n\n\n\n\n\nHypergeometric distribution\nBelow are some brief notes on the a few of the distributions that we covered in class that were not covered above.\nNOTE that need to pay attention to the parameterization that R uses, slightly different than what we used in class.\nLet \\(X \\sim \\text{Hypergeometric}(N = 90, M = 9, K = 16)\\).\n\npmf: \\(f(x) = {M \\choose x} {{N-M} \\choose {k-x}} / {N \\choose k}\\)\nOur parameters: \\(N\\) = population size, \\(M\\) = number of objects of interest, \\(K\\) = sample size\nIn R, $m = $ number of objects of interest (= our \\(M\\)), \\(n\\) = number of objects not of interest = \\(N - M\\), \\(k\\) = sample size (= our \\(K\\)).\n\nCalculate probabilities using dhyper() and phyper().\n\n# P(X &lt;= 4)\n# -&gt; X is still counting the number of objects of interest selected\nsum(dhyper(x = 0:4, m = 9, n = 90 - 9, k = 16))\n\n[1] 0.9921035\n\nphyper(q = 4, m = 9, n = 81, k = 16)\n\n[1] 0.9921035\n\n# P(X &gt; 2) = 1 - P(X &lt;= 2)\n1 - sum(dhyper(x = 0:2, m = 9, n = 81, k = 16))\n\n[1] 0.1962971\n\n1 - phyper(q = 2, m = 9, n = 81, k = 16)\n\n[1] 0.1962971\n\nphyper(q = 2, m = 9, n = 81, k = 16, lower.tail = FALSE)\n\n[1] 0.1962971\n\n\nVisualize the distribution.\n\n# visualize the distribution\n# define the range of the random variable\nx &lt;- 0:9\nplot(x = x, y = dhyper(x = x, m = 15, n = 35, k = 9), type = \"h\", xlab = \"x\", ylab = \"f(x)\", main = \"Hypergeometric(50,15,9) pmf\")\n\n\n\n\n\n\n\n\n\n\nNegative binomial distribution\nLet \\(X \\sim \\text{Negative Binomial}(r = 3, p = 0.3)\\).\n\npmf: \\(f(x) = {{x-1} \\choose {r-1}} p^r (1-p)^{x-r}\\)\nIn R, (just like with the geometric distribution) negative binomial is counting the number of failures \\(\\Longrightarrow Y = X - r\\).\n\nCalculate the following probabilities using dnbinom() and pnbinom().\n\n# P(X &lt;= 11)\n# -&gt; range of x starts at r, which = 3\n# -&gt; size = r\n# -&gt; need to convert to y = number of failures (range is 0,1,2,...)\nr &lt;- 3\nx &lt;- r:11\ny &lt;- x - r\nsum(dnbinom(x = y, size = r, prob = 0.3))\n\n[1] 0.6872595\n\npnbinom(q = max(y), size = r, prob = 0.3) # cutoff is the highest x - r = max(y)\n\n[1] 0.6872595\n\n# P(X &gt; 6) = 1 - P(X &lt;= 6)\nx &lt;- r:6\ny &lt;- x - r\n1 - sum(dnbinom(x = y, size = r, prob = 0.3))\n\n[1] 0.74431\n\n1 - pnbinom(q = max(y), size = r, prob = 0.3)\n\n[1] 0.74431\n\npnbinom(q = max(y), size = r, prob = 0.3, lower.tail = FALSE)\n\n[1] 0.74431\n\n\nVisualize the distribution\n\nx &lt;- r:30\ny &lt;- x - r\n\n# going to plot our version of the NB\n# -&gt; so get probabilities according to Y, but match with the corresponding Xs\nplot(x = x, y = dnbinom(x = y, size = r, prob = 0.2), type = \"h\", xlab = \"x\", ylab = \"f(x)\", main = \"Negative Binomial(3,0.2) pmf\")\n\n\n\n\n\n\n\n\n\n\nGamma distribution\nLet \\(X \\sim \\text{Gamma}(\\alpha = 3.5, \\beta = 0.75)\\).\n\nIn R, shape = $ and rate = \\(\\beta\\).\nOur \\(\\beta\\) (just like in the exponential distribution) is a rate parameter, not a scale parameter = 1 / rate (so don’t specify scale =).\n\nCalculate the following probabilities using pgamma() (remember we couldn’t find gamma probabilities by hand unless \\(\\alpha\\) = whole number (but would require lots of integration by parts)).\n\n# P(X &lt; 5)\npgamma(q = 5, shape = 3.5, rate = 0.75)\n\n[1] 0.6212631\n\n# P(X &gt;= 4)\npgamma(q = 4, shape = 3.5, rate = 0.75, lower.tail = FALSE)\n\n[1] 0.5397494\n\n1 - pgamma(q = 4, shape = 3.5, rate = 0.75)\n\n[1] 0.5397494\n\n# P(2.5 &lt; X &lt; 7)\npgamma(q = 7, shape = 3.5, rate = 0.75) -  pgamma(q = 2.5, shape = 3.5, rate = 0.75)\n\n[1] 0.6461156\n\n\nVisualize the distribution.\n\n# visualize the distribution\nx &lt;- seq(from = 0, to = 20, by = 0.01)\nplot(x = x, y =  dgamma(x, shape = 3.5, rate = 0.75), type = \"l\", xlab = \"x\", ylab = \"f(x)\", main = \"Gamma(3.5,0.75) pdf\")\n\n\n\n\n\n\n\n\n\n\n\nSampling distributions\nFILES: sampling-dists-STARTER.qmd and sampling-dists.html\n\nOverview\nThis document will demonstrate the relationship between the different kinds of distributions we have now (population, sample and sampling distribution). This will involve learning to take random samples and conduct mini simulations to build a sampling distribution. We will demonstrate this for several of distributions / statistics.\n\n\nSampling distribution concept: Normal\nSampling distributions are the basis of the statistical inference we will learn (confidence intervals and hypothesis tests). So it will be important to conceptually understand what a sampling distribution is. To help with this, we are going to build (simulate) one! We are going to demonstrate this using the Normal distribution first.\nWith the addition of sampling distributions, we now have three different types of distributions that we need to understand.\n1) Population distribution\n\nThis is all the distributions we studied in MATH 320 (all the plots made in the ‘Probability.R’ file we are thinking of as population distributions).\nThese are theoretical distributions that we are assuming to be true.\n\nLet \\(X \\sim \\text{Normal}\\,(\\mu = 50, \\sigma = 10)\\).\nFirst we will initialize the parameters to reuse in the rest of the code.\n\n# initialize parameters\nmu &lt;- 50\nsigma &lt;- 10\n\nNow we can plot the population distribution.\n\n# visualize population distribution\nx &lt;- seq(from = mu - 3 * sigma, to = mu + 3 * sigma, by = 0.01)\nplot(x = x, y = dnorm(x, mean = mu, sd = sigma), type = \"l\", xlab = \"x\", ylab = \"f(x)\", main = \"Normal population distribution\")\n\n\n\n\n\n\n\n\n2) Sample (data) distribution\n\nThis is the result of randomly sampling from the population distribution (i.e. collecting data).\nWhen simulating a random sample, we are essentially just generating \\(x\\) values whose probabilities of being selected follow the population distribution.\n\nLets take a random sample of size \\(n = 30\\), which means \\(X \\overset{iid}\\sim \\text{Normal}\\,(\\mu = 50, \\sigma = 10)\\). Use rnorm() to randomly generating observations from a normal distribution.\n\n# initialize sample size and generate random sample\nn &lt;- 30\ndata_sample &lt;- rnorm(n = n, mean = mu, sd = sigma)\ndata_sample\n\n [1] 48.43166 54.85217 34.91403 48.21068 50.23690 52.69379 57.93790 37.72459\n [9] 53.29383 50.95798 38.52043 32.78676 44.68942 43.03404 50.05430 48.13014\n[17] 46.08644 45.39412 53.20952 45.85008 47.78619 41.31832 60.59329 56.47356\n[25] 49.96579 58.88280 46.75842 52.87937 67.85768 53.95503\n\n\nWe can plot the sample (data) distribution with a histogram by using hist().\n\n# visualize sample (data) distribution\nhist(x = data_sample, xlab = \"x\", main = \"Random sample from Normal population\")\n\n\n\n\n\n\n\n\nEvery random sample that we take will look slightly different of course because of the randomness. But all of the sample distributions should match the properties of the population distribution they were drawn from. This means \\(\\bar{X} \\approx \\mu\\), \\(S \\approx \\sigma\\), and roughly matches the shape. To check the summary statistics, use mean() and sd(). With larger sample sizes, the sample values will on average be closer to the population values.\n\n# check summary statistics for our random sample\nmean(data_sample)\n\n[1] 49.11597\n\nmu\n\n[1] 50\n\nsd(data_sample)\n\n[1] 7.667929\n\nsigma\n\n[1] 10\n\n\nTo compare the shapes, we can add the population distribution curve to the sample distribution histogram.\nFirst we need to plot the histogram like before, except specify freq = FALSE to make the y-axis relative frequency (probability) rather than frequency (count). Then use lines() to overlay the density curve.\n\n# plot relative frequency histogram and add population density curve\nhist(x = data_sample, freq = FALSE, xlab = \"x\", main = \"Random sample from Normal population\")\nlines(x = x, y = dnorm(x, mean = mu, sd = sigma), col = \"red\")\n\n\n\n\n\n\n\n\n3) SamplING distribution\n\nThis is also a theoretical distribution of a statistic (like sample mean, \\(\\bar{X}\\)) that occasionally ends up following a familiar distribution (depends on what population we start with).\nBut it can easily be simulated to get a really good approximation of the shape and summary statistics.\n\nNow lets build up to a sampling distribution.\nWe already have a random sample of data, so lets record the sample mean.\n\n# record the sample mean\nx_bar &lt;- c()\nx_bar[1] &lt;- mean(data_sample)\n\nWe have now started a vector (distribution) of sample means, all we need to do is add a bunch more sample means to it. So we need to generate several random samples of data, calculate and record their respective sample means. To repeat calculations / operations many times, we can use a for loop.\n\n# initialize our holding vector again\nx_bar &lt;- c()\nfor (i in 1:20) {\n  \n  # i is the index, the for loop runs for each value of the index\n  \n  # generate data\n  data_sample &lt;- rnorm(n = n, mean = mu, sd = sigma)\n  \n  # record sample mean\n  x_bar[i] &lt;- mean(data_sample)\n  \n  # once both of these are done, the for loop goes to the next index value and performs the loop again\n}\n\nNow we have \\(i = 20\\) sample means calculated from 20 random samples of data. Lets plot them to see the beginnings of the sampling distribution.\n\n# show sample mean values\nx_bar\n\n [1] 50.10146 49.35539 47.00313 52.64353 48.81512 49.19360 52.20879 51.94868\n [9] 50.02831 49.95325 50.61347 48.61472 51.99039 48.99885 52.57588 50.09879\n[17] 54.72978 50.46928 48.22803 49.13747\n\n# plot sample means\nhist(x = x_bar, freq = FALSE, xlab = \"Sample mean\", main = \"(Start of) Sampling distribution from Normal population\")\n\n\n\n\n\n\n\n\nLets continue building the sampling distribution. Theoretically the sampling distribution is the distribution of ALL POSSIBLE sample means. For a simulation, this just means calculate a very large number of them. So lets repeat the for loop, but for a very large \\(i\\).\n\n# simulate 100,000 samples and calculate mean\nx_bar &lt;- c()\nfor (i in 1:100000) {\n  \n  # generate data\n  data_sample &lt;- rnorm(n = n, mean = mu, sd = sigma)\n  \n  # record sample mean\n  x_bar[i] &lt;- mean(data_sample)\n}\n\nNow lets plot the sampling distribution of \\(\\bar{X}\\). We will see that it is perfectly centered at the population mean, and also much less variable than the sample distribution (much smaller spread). To verify that, we can add the population mean to the plot for reference. Use abline(v = ) to add a vertical $x = $ line.\n\n# plot sample means and add population mean reference line\nhist(x = x_bar, freq = FALSE, xlab = \"Sample mean\", main = \"Sampling distribution from Normal population\")\nabline(v = mu, col = \"red\")\n\n\n\n\n\n\n\n\nLets look at the summary statistics for this new distribution and compare them to the population values.\nBecause we are studying the sample mean, the results should match the theorem we learned that \\(E(\\bar{X}) = \\mu\\) and \\(V(\\bar{X}) = \\sigma^2 / n\\). This is true regardless of population distribution.\n\n# compare summary statistics of sampling distribution to theoretical results in terms of the population parameters\nmean(x_bar)\n\n[1] 50.00638\n\nmu\n\n[1] 50\n\nvar(x_bar)\n\n[1] 3.360183\n\nsigma^2 / n\n\n[1] 3.333333\n\n\nThen because we started with a normal distribution, we actually know exactly what distribution \\(\\bar{X}\\) follows from another theorem. If \\(X_i \\overset{iid}\\sim \\text{Normal}\\,(\\mu = 50, \\sigma = 10)\\), then \\(\\bar{X} \\sim \\text{Normal}\\,(\\mu, \\sigma^2 / n)\\). Lets verify this by adding the density curve to the histogram of sample means, it should line up perfectly.\nTo generalize our specification of x which we need for lines(), we can generate a sequence of values that are three standard deviations sd(x_bar) from the center of the simulated sampling distribution mean(x_bar) in either direction. For most distributions, this will cover the range of values we need.\n\n# add theoretical density curve to histogram of sampling distribution\nhist(x = x_bar, freq = FALSE, xlab = \"Sample mean\", main = \"Sampling distribution from Normal population\")\nx &lt;- seq(from = mean(x_bar) - 3 * sd(x_bar), to = mean(x_bar) + 3 * sd(x_bar), by = 0.01)\nlines(x = x, y = dnorm(x, mean = mu, sd = sigma/sqrt(n)), col = \"red\")\n\n\n\n\n\n\n\n\nWe can build sampling distributions for any statistic So lets repeat the process above for the sample variance \\(S^2\\) when drawing from a normal population. All we have to do different is calculate a different sample statistic inside the for loop for each of the generated samples.\n\n# simulate sampling distribution for sample variance\ns2 &lt;- c()\nfor (i in 1:100000) {\n  \n  # generate data\n  data_sample &lt;- rnorm(n = n, mean = mu, sd = sigma)\n  \n  # record sample mean\n  s2[i] &lt;- var(data_sample)\n  \n}\n\nTo show the first few observations, we can use head().\n\n# display first 10 values\nhead(s2)\n\n[1] 138.91252 109.04852 110.39264  74.39573  58.51212 142.37100\n\n\nNow we can plot this new sampling distribution and add the population variance to the plot for reference as well.\n\n# plot the sampling distribution of S^2 and add population value for reference\nhist(x = s2, freq = FALSE, xlab = \"Sample variance\", main = \"Sampling distribution from Normal population\")\nabline(v = sigma^2, col = \"red\")\n\n\n\n\n\n\n\n\nWe know from a theorem that \\(E(S^2) = \\sigma^2\\). So lets check the estimated mean of the sampling distribution and compare it to \\(\\sigma^2\\).\n\n# compare sampling distribution center to the population value\nmean(s2)\n\n[1] 99.9877\n\nsigma^2\n\n[1] 100\n\n\nWe don’t know what distribution \\(S^2\\) follows by itself, but we can modify it to match the theorem we have: \\(\\frac{n - 1}{\\sigma^2} S^2 \\sim \\chi^2 (n-1)\\).\n\n# calculate and display modified sample variances\ns2_modified &lt;- (n - 1) * s2 / sigma^2\nhead(s2_modified)\n\n[1] 40.28463 31.62407 32.01387 21.57476 16.96852 41.28759\n\n\nNow we can plot this and compare it to the theoretical density it should follow. For the specification of \\(x\\), we know \\(\\chi^2\\) is a special-case gamma density, which means it is right-skewed. So lets go to 4 standard deviations above to ensure our line has the correct range.\n\n# plot modified sample variances and add theoretical density curve\nhist(x = s2_modified, freq = FALSE, xlab = \"Modified sample variance\", main = \"Sampling distribution from Normal population\")\nx &lt;- seq(from = mean(s2_modified) - 3 * sd(s2_modified), to = mean(s2_modified) + 4 * sd(s2_modified), by = 0.01)\nlines(x = x, y = dchisq(x, df = n - 1), col = \"red\")\n\n\n\n\n\n\n\n\n\n\nExponential sample means\nGoal is simulate the sampling distribution for the sample mean based on an exponential distribution.\nLet \\(X_i \\overset{iid}\\sim \\text{Exp}\\,(\\lambda = 5)\\) and use a sample size of \\(n = 20\\).\nTo generate a random sample from the exponential distribution, use rexp(), where rate = lambda.\n\n# initialize necessary items\n# -&gt; parameter\n# -&gt; sample size\n# -&gt; holding vector for sample means\nlambda &lt;- 50\nn &lt;- 20\nx_bar &lt;- c()\n\n# simulate sampling distribution for sample mean\nfor (i in 1:100000) {\n  \n  # generate data\n  data_sample &lt;- rexp(n = n, rate = lambda)\n  \n  # record sample mean\n  x_bar[i] &lt;- mean(data_sample)\n}\n\n# display values\nhead(x_bar)\n\n[1] 0.03090034 0.01476551 0.02480843 0.01890802 0.02015736 0.01418337\n\n\nCompare the summary statistics to the populations values (what they should be). For exponential, \\(\\mu = E(X) = 1 / \\lambda\\) and \\(\\sigma^2 = V(X) = 1 / \\lambda^2\\). Then apply the \\(\\bar{X}\\) theorem.\n\n# compare summary statistics to the correct population values\nmean(x_bar)\n\n[1] 0.01999982\n\n1 / lambda\n\n[1] 0.02\n\nvar(x_bar)\n\n[1] 1.990227e-05\n\n(1 / lambda)^2 / n\n\n[1] 2e-05\n\n\nBecause the scale is so small for the variance, by default R uses scientific notation. We can disable this with the following line.\n\n# disable scientific notation\n# recalculate values\noptions(scipen = 999)\nvar(x_bar)\n\n[1] 0.00001990227\n\n(1 / lambda)^2 / n\n\n[1] 0.00002\n\n\nThen because we started with an exponential distribution, we have already derived exactly what distribution \\(\\bar{X}\\) follows: if \\(X_i \\overset{iid}\\sim \\text{Exp}\\,(\\lambda)\\) then \\(\\bar{X} \\sim \\text{Gamma}\\,(\\alpha = n, \\beta = n \\lambda)\\).\nLets verify this by adding the density curve to the histogram of sample means. Note for dgamma(), shape = alpha and rate = beta.\n\n# plot sampling distribution of sample means and add theoretical density curve\nhist(x = x_bar, freq = FALSE, xlab = \"Sample mean\", main = \"Sampling distribution from Exponential population\")\nx &lt;- seq(from = mean(x_bar) - 3 * sd(x_bar), to = mean(x_bar) + 4 * sd(x_bar), by = 0.001)\nlines(x = x, y = dgamma(x, shape = n, rate = n * lambda), col = \"red\")\n\n\n\n\n\n\n\n\n\n\n\nPractice sess\nFILES: practice-sess-STARTER.qmd and practice-sess.html\n\nOverview\nBelow are some basic problems to practice the concepts we have learned so far in R.\n\n\nProblem 1\nSuppose we have the following discrete pmf for X:\n\n\n\n\\(x\\)\n\\(f(x)\\)\n\n\n\n\n2\n0.20\n\n\n4\n0.13\n\n\n5\n0.07\n\n\n8\n0.21\n\n\n10\n0.14\n\n\n11\n0.09\n\n\n13\n0.14\n\n\n\n\nCreate two vectors, one for \\(x\\) and one for \\(f(x)\\).\n\n\n# initialize vectors\nx &lt;- c(2, 4, 5, 8, 10, 11, 13)\nf_x &lt;- c(0.20, 0.13, 0.07, 0.21, 0.14, 0.09, 0.16)\n\n\nCalculate the expected value of \\(X\\) using your vectors.\n\n\n# E(X) = weighted average of x and f(x)\nsum(x * f_x)\n\n[1] 7.42\n\n\n\nCalculate \\(P(X &gt; 7)\\) by subsetting the \\(f(x)\\) vector with a logical test.\n\n\n# calculate probability -&gt; add corresponding probabilities where X &gt; 7\nsum(f_x[x &gt; 7])\n\n[1] 0.6\n\n\n\nFind the mode of \\(X\\) by selecting the correct \\(x\\) value based on a logic check of \\(f(x)\\). NOTE: To check equality, use ==.\n\n\n# find the X value with the highest probability\nx[f_x == max(f_x)]\n\n[1] 8\n\n\n\nPlot the pmf of \\(X\\).\n\n\n# plot pmf\nplot(x = x, y = f_x, type = \"h\", ylab = \"f(x)\", main = \"Pmf of X\")\n\n\n\n\n\n\n\n\n\n\nProblem 2\nGenerate a random sample from of size \\(n = 100\\) from \\(\\text{Gamma}(\\text{shape } (\\alpha) = 5, \\text{rate } (\\beta) = 2.5)\\) and save it as a vector.\n\nCount the number of values in your random sample that are greater than 2 using sum() on a logical check of your vector.\n\n\n# generate sample\ndata_sample &lt;- rgamma(n = 100, shape = 5, rate = 2.5)\n\n# count number of obs &gt; 2\nsum(data_sample &gt; 2)\n\n[1] 44\n\n\n\nCalculate the sample probability of less than 1.5 using mean() on a logical check of your vector.\n\n\n# P(X &lt; 1.5)\nmean(data_sample &lt; 1.5)\n\n[1] 0.4\n\n# equivalent to \nsum(data_sample &lt; 1.5) / length(data_sample)\n\n[1] 0.4\n\n\n\nSearch how to and compute the min, max, median, IQR, and range of your random sample.\n\n\n# all summary measures at once\nsummary(data_sample)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.5855  1.2209  1.7753  1.9856  2.4498  5.2723 \n\n# IQR = Q3 - Q1\nquantile(data_sample, probs = 0.75) - quantile(data_sample, probs = 0.25)\n\n     75% \n1.228842 \n\n\n\nCreate a boxplot of your random sample.\n\n\n# create boxplot\nboxplot(data_sample, horizontal = TRUE)\n\n\n\n\n\n\n\n\n\nSearch how to extract any outliers based on your boxplot extract.\n\n\n# extract outliers\nboxplot(data_sample, horizontal = TRUE)$out\n\n\n\n\n\n\n\n\n[1] 5.272339 5.009112",
    "crumbs": [
      "Test 2",
      "Lecture 1 -- Random Samples and Common Statistics"
    ]
  },
  {
    "objectID": "lecture-1.html#homework",
    "href": "lecture-1.html#homework",
    "title": "Lecture 1 – Random Samples and Common Statistics",
    "section": "Homework",
    "text": "Homework\nFILES: hw-1-STARTER.qmd, homework-1.tex, and style-assessments.sty\nInstructors can contact me for solutions :)",
    "crumbs": [
      "Test 2",
      "Lecture 1 -- Random Samples and Common Statistics"
    ]
  },
  {
    "objectID": "lecture-2.html",
    "href": "lecture-2.html",
    "title": "Lecture 2 – Order Statistics",
    "section": "",
    "text": "Guided Notes\nFILES: BLANK lecture-2.pdf, lecture-2.tex, and style-lectures.sty",
    "crumbs": [
      "Test 2",
      "Lecture 2 -- Order Statistics"
    ]
  },
  {
    "objectID": "lecture-2.html#in-class-activity",
    "href": "lecture-2.html#in-class-activity",
    "title": "Lecture 2 – Order Statistics",
    "section": "In-Class Activity",
    "text": "In-Class Activity\nFILES: in-class-2.tex and style-assessments.sty\nInstructors can contact me for keys :)",
    "crumbs": [
      "Test 2",
      "Lecture 2 -- Order Statistics"
    ]
  },
  {
    "objectID": "lecture-2.html#excel-notes",
    "href": "lecture-2.html#excel-notes",
    "title": "Lecture 2 – Order Statistics",
    "section": "Excel Notes",
    "text": "Excel Notes\nFILES: qq-plots-STARTER.xlsx and qq-plots-COMPLETED.xlsx",
    "crumbs": [
      "Test 2",
      "Lecture 2 -- Order Statistics"
    ]
  },
  {
    "objectID": "lecture-2.html#homework",
    "href": "lecture-2.html#homework",
    "title": "Lecture 2 – Order Statistics",
    "section": "Homework",
    "text": "Homework\nFILES: hw-2-STARTER.xlsx, homework-2.tex, and style-assessments.sty\nInstructors can contact me for solutions :)",
    "crumbs": [
      "Test 2",
      "Lecture 2 -- Order Statistics"
    ]
  },
  {
    "objectID": "lecture-3.html",
    "href": "lecture-3.html",
    "title": "Lecture 3 – Explorartory Data Analysis",
    "section": "",
    "text": "Excel Notes\nFILES: eda-STARTER.xlsx and eda-COMPLETED.xlsx",
    "crumbs": [
      "Test 2",
      "Lecture 3 -- Explorartory Data Analysis"
    ]
  },
  {
    "objectID": "lecture-3.html#r-notes",
    "href": "lecture-3.html#r-notes",
    "title": "Lecture 3 – Explorartory Data Analysis",
    "section": "R Notes",
    "text": "R Notes\nFILES: eda-r-STARTER.zip (this contains both starter files below with the necessary data and images)\n\nEDA\nFILE: eda.html\n\nPrerequisites\n\n# load packages\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\nPiping\nOften, we want to perform multiple functions, i.e. take an object, do something to it, then do something to the output.\nOne way to do this is with nesting functions → f(x).\n\nx &lt;- rnorm(n = 10, mean = 10, sd = 2)\nsd(scale(x))\n\n[1] 1\n\n\nThe pipe is another (equivalent) way to combine functions that is more readable. We can think of this as chaining functions together.\n\nPiping → x %&gt;% f, read as “and then”.\nTakes the object / result on the left and uses (passes) it as the first argument of the function on the right.\n\n\nx &lt;- rnorm(n = 10, mean = 10, sd = 2)\nx %&gt;% scale %&gt;% sd\n\n[1] 1\n\n\nThere is a base R version of the pipe as well |&gt;, but we will continue to use %&gt;% (it has some better functionality that we won’t dive into).\n\n\nggplot\nStarting from scratch\n\nggplot2 builds plots based on an approach called the grammar of graphics (hence “gg”plot2).\nThe grammar of graphics approach requires explicit aesthetic mapping of data to geometric features.\nAll plots follow a similar structure that builds up from the ggplot() function, which creates a “blank canvas”.\nAnd the first thing we can do is specify the dataset we will be using and rerun the code.\n\n\nggplot(data = diamonds)\n\n\n\n\n\n\n\n\n\nNow it is primed with the data, but we haven’t told it to do anything.\n\nAesthetic mapping\n\nNext, we can add a layer of geometric features with geom_*(). This uses uses aesthetic mapping, which takes values of a variable and translates them into a visual feature.\nChoice of geometry depends on the data types of the variables of interest from the supplied dataset as well as the intent for creating the plot.\nIn the example below, both variables (carat and price) are continuous. So we can use a scatterplot visualize their relationship. This is created by adding a layer of points via geom_point().\nSimply use + between ggplot2 functions to add layers.\n\n\n# attempt to create scatterplot\nggplot(data = diamonds,\n       x = carat,\n       y = price) + \n  geom_point()\n\nError in `geom_point()`:\n! Problem while setting up geom.\nℹ Error occurred in the 1st layer.\nCaused by error in `compute_geom_1()`:\n! `geom_point()` requires the following missing aesthetics: x and y\n\n\n\nThe code above throws an error because R can’t find carat and price because it is looking for standalone objects (i.e. vectors named carat and price).\nSo to tell R that the attributes are from the diamonds dataset, use the aes() function. In other words, this function connects the plot features to the dataframe specified in the data argument. Lets correct the above code.\n\n\n# correctly create scatterplot\nggplot(data = diamonds,\n       aes(x = carat,\n           y = price)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\n\n\nExploratory data analysis\n\nIntroduction\nWe are now moving into the next part of our course: “So I have some data, what do I do with it?”\n\n# preview data\ndiamonds %&gt;% glimpse\n\nRows: 53,940\nColumns: 10\n$ carat   &lt;dbl&gt; 0.23, 0.21, 0.23, 0.29, 0.31, 0.24, 0.24, 0.26, 0.22, 0.23, 0.…\n$ cut     &lt;ord&gt; Ideal, Premium, Good, Premium, Good, Very Good, Very Good, Ver…\n$ color   &lt;ord&gt; E, E, E, I, J, J, I, H, E, H, J, J, F, J, E, E, I, J, J, J, I,…\n$ clarity &lt;ord&gt; SI2, SI1, VS1, VS2, SI2, VVS2, VVS1, SI1, VS2, VS1, SI1, VS1, …\n$ depth   &lt;dbl&gt; 61.5, 59.8, 56.9, 62.4, 63.3, 62.8, 62.3, 61.9, 65.1, 59.4, 64…\n$ table   &lt;dbl&gt; 55, 61, 65, 58, 58, 57, 57, 55, 61, 61, 55, 56, 61, 54, 62, 58…\n$ price   &lt;int&gt; 326, 326, 327, 334, 335, 336, 336, 337, 337, 338, 339, 340, 34…\n$ x       &lt;dbl&gt; 3.95, 3.89, 4.05, 4.20, 4.34, 3.94, 3.95, 4.07, 3.87, 4.00, 4.…\n$ y       &lt;dbl&gt; 3.98, 3.84, 4.07, 4.23, 4.35, 3.96, 3.98, 4.11, 3.78, 4.05, 4.…\n$ z       &lt;dbl&gt; 2.43, 2.31, 2.31, 2.63, 2.75, 2.48, 2.47, 2.53, 2.49, 2.39, 2.…\n\n\nEventually we will learn how to do some formal procedures / tests (i.e. inference) such as parameter estimation, hypothesis tests and confidence intervals, but first we want to explore the data in order to get a “feel” for it.\nThis is where Exploratory data analysis (EDA) comes into play. Here is the idea behind it:\n\nEDA is the process of using numerical summaries and visualizations to explore your data and to identify potential relationships between variables.\nIt is an investigative process in which you use summary statistics and graphical tools to get to know your data and understand what you can learn from them.\nBecause EDA involves exploring, it is iterative. You are likely to learn different aspects about your data from different graphs. Typical goals are understanding:\n\nThe distribution of variables in your data set. It can be as simple as determining the shape of your data (symmetric, skewed, unimodal, bimodal, etc.), or even testing distributions via q–q plots.\nThe relationships between variables (bivariate analysis), such as scatterplots and correlation.\nWhether or not your data have outliers or unusual points that may indicate data quality issues or lead to interesting insights.\nWhether or not your data have patterns over time.\n\n\nNow, we will mention some of the background behind common EDA practices, but focus mainly on the application of EDA techniques in both R and Excel.\n\n\nDescriptive statistics\n\nMost common statistics\nGoal: Summarize a whole dataset with a single or few measures (mean, standard deviation, etc.).\nSuppose we collect data \\(x_1, \\dots, x_n\\) (might be a realization of a random sample \\(X_1, \\dots, X_n\\) from \\(f(x)\\)). Then we can summarize this dataset with:\n(Notation: These are specific realizations of the random variables \\(\\bar{X}\\), \\(S^2\\) and \\(V(X)\\), so we switch to lower case letters)\n\nSample mean \\(\\displaystyle \\bar{x} = \\frac{1}{n} \\sum_{i = 1}^n x_i\\)\nSample variance: \\(\\displaystyle s^2 = \\frac{1}{n - 1} \\sum_{i = 1}^n (x_i - \\bar{x})^2\\)\nData (or population) variance: \\(\\displaystyle v = \\frac{1}{n} \\sum_{i = 1}^n (x_i - \\bar{x})^2\\)\n\nThis is an alternate version of the variance that comes from a change in perspective of the data.\nIf we view our dataset as a population now (i.e. this is every single data point of interest), rather than a sample (subset) of a larger population, we make an adjustment to the coefficient of \\(n\\) out front.\nTreating \\(x_1, \\ldots, x_n\\) as a population, we can artificially create probability distribution (called an empirical distribution since it is determined by the data) by placing the weight \\(1/n\\) on each value \\(x_i\\).\nThen we can find the mean and variance of this empirical distribution.\n\n\n\n\n\n\\(x\\):\n\\(x_1\\)\n\\(x_2\\)\n\\(\\ldots\\)\n\\(x_n\\)\n\n\n\\(f(x)\\)\n\\(\\frac{1}{n}\\)\n\\(\\frac{1}{n}\\)\n\\(\\ldots\\)\n\\(\\frac{1}{n}\\)\n\n\n\n\\[\n\\begin{align*}\n\\mu &= \\sum x_i f(x_i) = \\sum x_i \\frac{1}{n} = \\bar{x}\\\\\n\\sigma^2 &= \\sum(x_i - \\mu)^2 f(x) = \\sum(x_i - \\bar{x})^2 \\frac{1}{n}= v\n\\end{align*}\n\\]\nThis is why there is two different versions of the standard deviation on the TI-84 1-Var Stats and in excel (STDEV.S() (sample) and STDEV.P() (population)).\n\nExample: Data (\\(n = 15)\\): 10, 23, 4, 6, 9, 3, 15, 6, 12, 11, 19, 10, 6, 8, 15\n\n\nRationale: We use the sample variance \\(s^2\\) over the population variance \\(v\\) for sample data to make the estimates of \\(\\sigma^2\\) come out better on average (i.e. unbiased).\n\nRelationship: \\(s^2 = \\frac{n}{n-1}v\\)\n\nWe have already seen the common functions to calculate these summary measures: mean(), var() and sd().\n\n\nOrder statistics\n\nFive-number summary: To provide additional summary information, we can use sample order statistics. Recall some important facts about order statistics.\n\n\\(X_{(1)}, \\ldots, X_{(n)}\\) are the sample values placed in ascending order and realized values of these random variables are denoted \\(x_{(1)}, \\ldots, x_{(n)}\\).\nWe use \\(X_{(j)}\\) as an estimator of \\(x_p\\), where \\(p = j / (n+1)\\). The realized value of this parameter (parameters just have to be any population quantity) is \\(\\hat{x}_p\\).\nNote that there are different algorithms for computing sample percentiles (and each software may use something different), so we will not dig into the details behind each, rather just know the general interpretation.\n\n\nSample minimum \\(x_{(1)}\\)\nLower / first quartile \\(q_1 = \\hat{x}_{0.25}\\)\nMedian (second quartile) \\(m = \\hat{x}_{0.50}\\)\nUpper / third quartile \\(q_3 = \\hat{x}_{0.75}\\)\nSample maximum \\(x_{(n)}\\)\n\n\nExample: Compute the 5 number summary of the following observations.\n\n# load data\nx &lt;- c(60, 71, 73, 77, 80, 83, 86, 87, 90, 90, 90, 94)\n\n# compute 5 number summary\n# -&gt; compare these summaries to that of the TI-84\nsummary(x)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  60.00   76.00   84.50   81.75   90.00   94.00 \n\n# compute other sample percentiles\nquantile(diamonds$price, c(0.10, 0.90)) \n\n 10%  90% \n 646 9821 \n\n\n\nCommon statistics based on the five-number summary:\n\nSample range, \\(R = x_{(n)} - x_{(1)}\\)\n\\(IQR = q_3 - q_1\\).\n\n\nExample: Calculate the above statistics for the price of diamonds.\n\n# calculate statistics\nrange(diamonds$price) %&gt;% diff()\n\n[1] 18497\n\nIQR(diamonds$price)\n\n[1] 4374.25\n\n\n\n\n\nDisplaying data\n\nBig picture\n\nAs we saw with the diamonds data, displaying raw information is terrible and useless. It is just way too much… That is why we collapse it down to just a few measures with our descriptive statistics.\nThere are two aspects to any EDA that can apply in all fields: understanding and relating.\nWe are learning the “behind the scenes” of how the concepts work, the theory and derivations, and then how to apply them. All of that is great, but we need to be able to relate it to our audience in a way that they can comprehend and then use the information.\n\n\n\nFrequency (and relative frequency) tables\n\nUsed to organize qualitative or quantitative data sets.\nIf qualitative, simply count the number of observations in each category.\nIf quantitative, here is how to construct:\n\nData is binned, i.e. grouped together. All bins will have equal length.\nFrequency = count of observations in each bin.\nRelative frequency = proportion of observations in each bin = Freq / \\(n\\).\n\nChoosing bin width / number of bins.\n\nThis is a subjective choice; it depends on how granular (closely, exactly) you want to show the data. With more bins, each bin gets smaller. So the data gets more spread out across the bins (smaller frequencies and relative frequencies).\nAdvantage of frequency tables: Great for condensing and summarizing the raw data.\nDisadvantage: Lose information, we no longer know what the specific values were, only what bin (range of values) they are in.\nSo why not just use a lot of bins? Then we would be keeping more information right??\nBut then it loses the conciseness that made it a good representation in the first place. Hard to really get anything from this frequency table?\nSo it is a balancing act.\n\n\nExample: Create several frequency tables for the diamonds data. Convert one to a relative frequency table.\n\n# categorical frequency table\ntable(diamonds$cut)\n\n\n     Fair      Good Very Good   Premium     Ideal \n     1610      4906     12082     13791     21551 \n\n# numeric frequency table\n# -&gt; set breaks breaks\nbreaks &lt;- seq(from = 0, to = 6, length = 5)\n\n# -&gt; discretize into new variable\ndiamonds$carat_cat &lt;- cut(diamonds$carat, breaks = breaks, include.lowest = TRUE)\n\n# -&gt; create frequency table\ntable(diamonds$carat_cat)\n\n\n[0,1.5] (1.5,3] (3,4.5] (4.5,6] \n  48498    5410      31       1 \n\n# -&gt; convert to relative frequency table\nround(table(diamonds$carat_cat) / length(diamonds$carat_cat), 5)\n\n\n[0,1.5] (1.5,3] (3,4.5] (4.5,6] \n0.89911 0.10030 0.00057 0.00002 \n\n\n\n\nBar graphs\n\nA bar graph (also known as bar chart or bar plot) is used for categorical data and assigns a height of a bar to the count (or relative count) of a group.\nComparisons are made easier with visuals than just numbers (although keep in mind that sometimes simpler is better and a table suffices).\nIf the data is setup so that there is one row per observation (i.e. not already summarized), then we can use geom_bar() to make our bar graph, where it will transform the data to counts automatically.\n\n\n# create bar graph\nggplot(data = diamonds,\n       aes(x = cut)) + \n  geom_bar()\n\n\n\n\n\n\n\n\n\n\nHistograms, density histograms and density curves\n\nA basic histogram is a univariate plot that can be used for continuous variables.\nIt is used to visualize the “shape” of data (i.e. which type of observations are more or less prevalent).\n\n\n\nThe shape also gives us insight into the relationship between several common descriptive statistics:\n\n\\[\n\\begin{align*}\n\\text{Right-skewed} &: \\text{mean} &gt; \\text{median} &gt; \\text{mode}\\\\\n\\text{Symmetric} &: \\text{mean} \\approx \\text{median} \\approx \\text{mode}\\\\\n\\text{Left-skewed} &:\\text{mean} &lt; \\text{median} &lt; \\text{mode}\\\\\n\\end{align*}\n\\]\n\nTo create a histogram, use geom_histogram().\n\nExample: Create a histogram of the diamond prices. Modify the visual appearance of the histogram.\n\n# create histogram\nggplot(data = diamonds,\n       aes(x = price)) + \n  geom_histogram(bins = 20,\n                 color = \"black\",\n                 fill = \"white\")\n\n\n\n\n\n\n\n\n\nA density histogram is a visual of a relative frequency table that is scaled to match the corresponding pdf. Here is the calculation that goes on behind the scenes:\n\n\\[P(c_1 &lt; X \\le c_2) \\approx \\frac{\\text{Freq}}{n} \\hspace{10pt} \\text{on } (c_1, c_2]\\]\n\n\\[\n\\begin{align*}\n\\text{Area} &= \\text{base} \\times \\text{height}\\\\\n\\frac{\\text{Freq}}{n} &= (c_2 - c_1) \\times h(x)\\\\\n\\end{align*}\n\\]\n\\[\\Longrightarrow h(x) = \\frac{\\text{Freq}}{n(c_2 - c_1)}\\]\n\nThen do this process for each bin, essentially scaling the heights so that the total area = 1 (like area under curve (AUC) of a pdf).\n\nExample: Convert the previous histogram to a density histogram.\n\n# create density histogram\nggplot(data = diamonds,\n       aes(x = price,\n           y = after_stat(density))) + \n  geom_histogram(bins = 20,\n                 color = \"black\",\n                 fill = \"white\")\n\n\n\n\n\n\n\n\n\nA more objective way to represent the data is called a density curve, which is a smooth curve based off the observed data that has an AUC of 1.\nWe can think of this smooth curve as a blanket thrown over the top of our density histogram as shown below.\n\nExample: Add a density curve to the previous density histogram.\n\n# overlay density curve to density histogram\nggplot(data = diamonds,\n       aes(x = price,\n           y = after_stat(density))) + \n  geom_histogram(color = \"black\",\n                 fill = \"white\") +\n  geom_density(adjust = 2,\n               col = \"blue\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\nDensity curves provide us with a way to visualize a quantitative distribution by group. This allows us to see differences in shape / spread for each level of the group.\n\nExample: Create multiple density curves for the price of each cut.\n\n# multiple density curves\nggplot(data = diamonds,\n       aes(x = price,\n           y = after_stat(density),\n           color = cut)) + \n  geom_density()\n\n\n\n\n\n\n\n\n\n\nBoxplots\n\nBoxplots are another common plot, which are used to visualize the distribution of a numeric variable. However, they no longer map the raw data; this is another example of when an aesthetic has an implicit transformation, which is then used to build the plot rather than straight from the raw data.\nInstead, boxplot() and geom_boxplot() map the five number summary that is computed from the raw data.\n\nExample: Create a boxplot of the carats of diamonds.\n\n# create single boxplot\nboxplot(diamonds$carat, horizontal = TRUE)\n\n\n\n\n\n\n\n\n\ngeom_boxplot() requires a continuous variable to be mapped to either the x or y argument, depending on the desired orientation of the boxplot.\nWe can also make comparative (side-by-side) boxplots by mapping a categorical variable to the other axis. This results in boxplots based on a single continuous variable, but grouped by the levels of the categorical variable. This is another way to plot a numerical response with a categorical explanatory variable.\n\nExample: Create boxplots of the carats of diamonds for each cut.\n\n# create comparative boxplots\nggplot(data = diamonds,\n       aes(x = carat,\n           y = cut)) + \n  geom_boxplot()\n\n\n\n\n\n\n\n\n\nBoxplots can be used to identify outliers, which are observations that lie outside the overall pattern in a distribution.\nSuspected outliers can significantly impact subsequent analyses, and thus if found, much consideration should be given for how to properly handle them.\nRule: A point is classified as an outlier if it is:\n\nBelow \\(q_1 - 1.5 \\times IQR\\) (low outlier) or above \\(q_3 +1.5 \\times IQR\\) (high outlier).\n\n\n\nExample: Create boxplot based on the following data and extract the outliers. Then calculate the lower and upper fences for determining outliers.\n\n# load data\nx &lt;- c(50, 61, 73, 77, 80, 83, 86, 87, 90, 90, 90, 100)\n\n# create boxplot and extract outlier\nboxplot(x, horizontal = TRUE)\nboxplot(x, horizontal = TRUE)$out\n\n\n\n\n\n\n\n\n[1] 50\n\n# calculate lower and upper fences\nas.numeric(quantile(x, 0.25) - 1.5 * IQR(x))\n\n[1] 55\n\nas.numeric(quantile(x, 0.75) + 1.5 * IQR(x))\n\n[1] 111\n\n\n\n\nScatterplots\n\nNow that we have explored each variable individually or incorporated categorical explanatory variables for numeric responses, we want to explore the pairwise relationships between our variables.\nThis is done via scatterplots, where we are looking for the visual dependence.\nTo create scatterplots, use geom_point().\n\n\n# create scatterplot\nggplot(data = diamonds,\n       aes(x = table,\n           y = depth)) +\n  geom_point(color = \"blue\",\n             alpha = 0.05)\n\n\n\n\n\n\n\n\n\nThen we can quantify the linear dependence by calculating the sample correlation as an estimator of the population correlation. Recall:\n\n\\[\\text{Cov}(X,Y) = E[(X - \\mu_X)(Y - \\mu_Y)]\\]\n\\[\\rho_{XY} = \\text{Corr}(X,Y) = \\frac{\\text{Cov}(X,Y)}{\\sqrt{V(X) V(Y)}}\\]\n\\[\nr = \\frac{1}{n - 1} \\frac{\\sum_{i = 1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{s_x \\, s_y}\n\\] Example: Calculate the correlation for the variables in the above scatterplot.\n\n# calculate correlation\ncor(diamonds$table, diamonds$depth)\n\n[1] -0.2957785\n\n\n\n\n\nTesting distributions\n\nq-q plots\n\nOnce we have made histogram and have a decent idea of the shape of a distribution, we could use our sample order statistics to test potential models that they originated from.\n\nExample: See if the following data came from a gamma distribution.\n\n# load data\n# -&gt; generated from x &lt;- rgamma(n = 37, shape = 5, rate = 1)\nx &lt;- c(3.158,2.475,3.894,4.157,4.844,1.141,2.344,3.873,2.89,9.029,3.593,4.452,4.239,5.922,4.486,7.015,7.53,3.585,2.95,3.244,6.101,4.171,3.463,9.789,4.248,5.132,2.181,8.296,2.632,8.239,5.09,4.742,5.817,8.546,7.317,8.745,3.555)\n\n# check shape\nhist(x, freq = FALSE)\n\n\n\n\n\n\n\n# create qqplot\n# -&gt; order data\nx_ordered &lt;- sort(x)\n\n# -&gt; calculate theoretical percentiles\ntheoretical_percentiles &lt;- qgamma(p = ppoints(length(x), a = 0), shape = 3, rate = 0.5)\n\n# qqplot\nqqplot(x = x_ordered, y = theoretical_percentiles)\nabline(a = 0, b = 1)\n\n\n\n\n\n\n\n\n\nNote that this follows a straight line, which means gamma is a good fit. Except it isn’t on the \\(y = x\\) line, which means we need better estimates of the parameters.\n\n\n# recalculate theoretical percentiles with correct values and plot\ntheoretical_percentiles &lt;- qgamma(p = ppoints(length(x), a = 0), shape = 5, rate = 1)\nqqplot(x = x_ordered, y = theoretical_percentiles)\nabline(a = 0, b = 1)\n\n\n\n\n\n\n\n\n\n\nEmpirical rule\n\nWe can also test for the normal distribution by computing interval probabilities and seeing if they match up to the empirical rule. Using sample information:\n\n\n\\(\\approx\\) 68% of data is in \\((\\bar{x} - s, \\bar{x} + s)\\).\n\\(\\approx\\) 95% of data is in \\((\\bar{x} - 2s, \\bar{x} + 2s)\\).\n\\(\\approx\\) 99.7% of data is in \\((\\bar{x} - 3s, \\bar{x} + 3s)\\).\n\n\nIf so, this suggests a normal model may be appropriate if the shape also is roughly bell-shaped.\n\n\n\nAnother way to identify outliers (based on mean and standard deviation)\n\nThree-sigma rule: Based on the empirical rule, nearly all data lies within three standard deviations of the mean. Thus, points that lie outside of this interval can be considered outliers.\n\n\n\n\n\n\n\nWorking with data\nFILE: working-with-data.html\n\nOverview\nThe goal of these notes is to demonstrate common data manipulation tasks using packages from the tidyverse, with a focus on dplyr.\n\n# load packages\nlibrary(tidyverse)\n\n\n\nData analysis 1\nWe will be working with hypothetical student grade data data-grades.csv, which contains information on two test scores from students of multiple sections for a single professor, and enrollment data data-majors.csv, which has the students major.\nTo read in the data, first download it from Canvas and then change the path in the read_csv() statement above to where you have saved the data relative to the location of the your homework file.\n\n# load data\ndata_raw &lt;- read_csv(file = \"r/files/data/data-grades.csv\")\n\nRows: 65 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): ID, Student\ndbl (2): Test_1, Test_2\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndata_majors &lt;- read_csv(file = \"r/files/data/data-majors.csv\")\n\nRows: 65 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): ID, Major\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# preview data\nglimpse(data_raw)\n\nRows: 65\nColumns: 4\n$ ID      &lt;chr&gt; \"1381\", \"2027\", \"6077\", \"6869\", \"4171\", \"3054\", \"8639\", \"2970\"…\n$ Student &lt;chr&gt; \"LZ-1\", \"KO-1\", \"DD-1\", \"IG-2\", \"VS-1\", \"BF-1\", \"VU-1\", \"XZ-1\"…\n$ Test_1  &lt;dbl&gt; 20, 30, 28, 14, 28, 20, 14, 19, 18, 23, 25, 30, 30, 25, 28, 16…\n$ Test_2  &lt;dbl&gt; 55, 40, 68, 42, 50, 49, 69, 55, 46, 46, 47, 55, 43, 59, 55, 64…\n\nglimpse(data_majors)\n\nRows: 65\nColumns: 2\n$ ID    &lt;chr&gt; \"9810\", \"9718\", \"9685\", \"9618\", \"9520\", \"9310\", \"9203\", \"9149\", …\n$ Major &lt;chr&gt; \"Physics\", \"Physics\", \"Chemisty\", \"Chemisty\", \"Physics\", \"Physic…\n\n\nNow we will go through steps to clean, organize and analyze these the test scores.\n\nData manipulations\nThe goal is to have a single dataset with the following columns:\n\nID\nStudent: just two initials\nMajor\nClass: 1 or 2\nTest_1: % out of 100\nTest_2: % out of 100\n\n\nFor the grades data, we need to split the current student column into two variables, one of their initials and one for the class they are in. Use tidyr::separate_wider_delim() to do so.\n\n\n# separate Student into initials and class\ndata_grades &lt;- data_raw %&gt;% \n  separate_wider_delim(cols = Student, delim = \"-\", names = c(\"Student\", \"Class\"))\nhead(data_grades)\n\n# A tibble: 6 × 5\n  ID    Student Class Test_1 Test_2\n  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 1381  LZ      1         20     55\n2 2027  KO      1         30     40\n3 6077  DD      1         28     68\n4 6869  IG      2         14     42\n5 4171  VS      1         28     50\n6 3054  BF      1         20     49\n\n\n\nConvert the test scores to percentages (Test 1 is out of 30 points and Test 2 is out of 70 points). Use mutate() to do transformations.\n\n\n# convert test grades to %\n# -&gt; Test 1 = points out of 30\n# -&gt; Test 2 = points out of 70\ndata_grades &lt;- data_grades %&gt;% \n  mutate(Test_1 = round(Test_1 / 30 * 100, 1),\n         Test_2 = round(Test_2 / 70 * 100, 1))\nhead(data_grades)\n\n# A tibble: 6 × 5\n  ID    Student Class Test_1 Test_2\n  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 1381  LZ      1       66.7   78.6\n2 2027  KO      1      100     57.1\n3 6077  DD      1       93.3   97.1\n4 6869  IG      2       46.7   60  \n5 4171  VS      1       93.3   71.4\n6 3054  BF      1       66.7   70  \n\n\n\nCombine grades data and majors data and then sort alphabetically by student initials within each class. Use arrange() to sort the data.\n\n\n# combine grades data and majors data\n# then sort by name within class\ndata_grades &lt;- data_grades %&gt;% \n  left_join(y = data_majors,\n            join_by(ID)) %&gt;% \n  arrange(Class, Student)\nhead(data_grades)\n\n# A tibble: 6 × 6\n  ID    Student Class Test_1 Test_2 Major   \n  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;   \n1 3054  BF      1       66.7   70   Physics \n2 8236  CD      1       40     94.3 Chemisty\n3 6077  DD      1       93.3   97.1 Chemisty\n4 7035  EJ      1       83.3   84.3 Chemisty\n5 7267  EU      1       86.7   68.6 Physics \n6 5228  EZ      1       63.3   85.7 Chemisty\n\n\n\n\nVisualize data\nNow that the data is cleaned and organized, lets visualize the Test 1 scores to start with.\nCreate two polished plots to visualize Test 1 scores, at least one of which should include a class comparison or major comparison.\n\n# create histogram of test 1 grades\nggplot(aes(x = Test_1),\n       data = data_grades) + \n  geom_histogram(bins = 8,\n                 col = \"black\",\n                 fill = \"grey70\") + \n  labs(x = \"Test 1 %\")\n\n\n\n\n\n\n\n# create boxplots by class\nggplot(aes(x = Test_1,\n           y = Class),\n       data = data_grades) + \n  geom_boxplot(fill = \"lightblue\") + \n  labs(x = \"Test 1 Grades\") + \n  theme_bw()\n\n\n\n\n\n\n\n# create boxplots by major\nggplot(aes(x = Test_1,\n           y = Major),\n       data = data_grades) + \n  geom_boxplot(fill = \"lightgreen\") + \n  labs(x = \"Test 1 Grades\") + \n  theme_bw()\n\n\n\n\n\n\n\n\n\n\nSummarize data\nNow that we have an idea of the distributions for Test 1, let’s summarize them, specifically we want to create an overall summary and a summary by class.\n\nCreate a test 1 dataset that contains only the Class, Student initials and Test 1 score. Use select() to select specific columns.\n\n\n# create test 1 dataset\ndata_test1 &lt;- data_grades %&gt;% \n  select(Student, Class, Test_1)\nhead(data_test1)\n\n# A tibble: 6 × 3\n  Student Class Test_1\n  &lt;chr&gt;   &lt;chr&gt;  &lt;dbl&gt;\n1 BF      1       66.7\n2 CD      1       40  \n3 DD      1       93.3\n4 EJ      1       83.3\n5 EU      1       86.7\n6 EZ      1       63.3\n\n\n\nCreate an object named data_summary_overall that summarizes Test 1 scores with the sample size, average and standard deviation. Then add an indicator column and rearrange the columns. Use summarize() to aggregate the data.\n\n\n#  aggregate and organize data\ndata_summary_overall &lt;- data_test1 %&gt;% \n  summarize(n = n(),\n            avg = round(mean(Test_1), 1),\n            sd = round(sd(Test_1), 1)) %&gt;% \n  mutate(Class = \"Overall\") %&gt;% \n  select(4, 1:3)\ndata_summary_overall\n\n# A tibble: 1 × 4\n  Class       n   avg    sd\n  &lt;chr&gt;   &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Overall    65  67.4  20.3\n\n\n\nCreate another object called data_summary_section that performs the same summary functions the previous step, except by Class. To do this, add .by = argument to aggregate data within another variable.\n\n\n# summarize data by class\ndata_summary_section &lt;- data_test1 %&gt;% \n  summarize(.by = Class,\n            n = n(),\n            avg = round(mean(Test_1), 1),\n            sd = round(sd(Test_1), 1))\ndata_summary_section\n\n# A tibble: 2 × 4\n  Class     n   avg    sd\n  &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 1        39  69.8  18.4\n2 2        26  63.8  22.6\n\n\n\nCombine the overall summary with the class summary rowwise using bind_rows().\n\n\n# combine overall summary with class summary\ndata_summary &lt;- bind_rows(data_summary_overall,\n                          data_summary_section)\ndata_summary\n\n# A tibble: 3 × 4\n  Class       n   avg    sd\n  &lt;chr&gt;   &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Overall    65  67.4  20.3\n2 1          39  69.8  18.4\n3 2          26  63.8  22.6\n\n\n\n\n\nData analysis 2\nNow we will work with the shuffled-playlist-clean.csv data, which contains information about songs in a Spotify playlist. Lets read in and preview the data.\n\n# load data\ndata_music &lt;- read.csv(file = \"r/files/data/shuffled-playlist-clean.csv\")\n\n# preview data\nhead(data_music)\n\n                 artist                 X.Deluxe.        genre\n1 Streetlight Manifesto      Everything Goes Numb          ska\n2                 Adele                        25          pop\n3            Pink Floyd The Dark Side of the Moon classic rock\n4      Childish Gambino        \"Awaken, My Love!\"      hip hop\n5            Ed Sheeran                  (Deluxe)          pop\n6        Kendrick Lamar       To Pimp A Butterfly      hip hop\n                              name danceability energy loudness speechiness\n1             A Moment of Violence        0.609 0.9180   -5.998      0.0894\n2           Water Under the Bridge        0.596 0.8380   -6.520      0.0704\n3                      Speak to Me        0.592 0.0196  -33.350      0.0358\n4                         Baby Boy        0.529 0.4030   -7.103      0.0349\n5                   Nancy Mulligan        0.680 0.8520   -4.350      0.0349\n6 You Ain't Gotta Lie (Momma Said)        0.501 0.6460   -5.422      0.2310\n  acousticness instrumentalness liveness valence   tempo\n1       0.0087         0.00e+00   0.2940  0.5410  89.975\n2       0.0189         1.54e-05   0.1080  0.4790  94.982\n3       0.3620         8.54e-01   0.1080  0.0322 119.504\n4       0.1230         4.96e-01   0.0647  0.7230 199.928\n5       0.1170         0.00e+00   0.0866  0.8520 101.993\n6       0.4820         1.43e-05   0.2740  0.6480  94.746\n\n\nNow we will ask several questions about the music dataset that can be answered by working with the data.\n\nSummarize data\nFor each of the following questions, use pipes %&gt;% (or a series of pipes) to display a mini-dataframe that answers the question (there is no need to save the results into an object).\n\nWhat song has the highest valence score? Display only the categorical information about the song (i.e. artist, album, genre, name) and its respective valence score. HINT: use a select() statement in conjunction with where() to display only the character variables.\n\n\n# determine song with highest valence score\n# -&gt; filter to row with max value and then display only necessary info\ndata_music %&gt;%\n  filter(valence == max(valence)) %&gt;%\n  select(where(is.character), valence)\n\n       artist              X.Deluxe.        genre                      name\n1 The Beatles Let It Be (Remastered) classic rock For You Blue - Remastered\n  valence\n1   0.958\n\n\n\nWhat is the average energy for hip hop songs?\n\n\n# determine average energy for hip hop songs\n# -&gt; filter to genre of interest and then calculate summary\ndata_music %&gt;%\n  filter(genre == \"hip hop\") %&gt;%\n  summarize(avg_energy = mean(energy))\n\n  avg_energy\n1  0.6566667\n\n\n\nWhat percentage of songs in the data are by the artist Dessa?\n\n\n# determine percentage of songs that are by the artist Dessa\ndata_music %&gt;%\n  summarize(proportion = mean(artist == \"Dessa\"))\n\n  proportion\n1 0.06179775\n\n\n\n\nGrouped and specific summaries\n\nCalculate the average energy, tempo and loudness for each genre. First do this by having a separate mean(&lt; variable &gt;) statement for each.\n\n\n# summarize a few music measures by genre\ndata_music %&gt;% \n  summarize(.by = genre,\n            avg_energy = mean(energy),\n            avg_tempo = mean(tempo),\n            avg_loudness = mean(loudness))\n\n         genre avg_energy avg_tempo avg_loudness\n1          ska  0.8864103  132.4416    -5.472897\n2          pop  0.5964750  116.5934    -5.963550\n3 classic rock  0.5327784  130.3288   -10.819353\n4      hip hop  0.6566667  121.2545    -7.638500\n\n\n\nNow repeat the calculation using across(), which allows us to apply the same function to a set of variables. This is much less cumbersome than typing each mean statement out like before.\n\n\n# use summarize() to aggregate data with across() to specify the list of variables\ndata_music %&gt;% \n  summarize(.by = genre,\n            across(c(energy, tempo, loudness), mean))\n\n         genre    energy    tempo   loudness\n1          ska 0.8864103 132.4416  -5.472897\n2          pop 0.5964750 116.5934  -5.963550\n3 classic rock 0.5327784 130.3288 -10.819353\n4      hip hop 0.6566667 121.2545  -7.638500\n\n\n\nUse the same technique as above to calculate the mean of all numeric variable by genre. HINT: Use across() in conjunction with where().\n\n\n# use where() function to specify which columns to summarize\ndata_music %&gt;% \n  summarize(.by = genre,\n            across(where(is.numeric), mean))\n\n         genre danceability    energy   loudness speechiness acousticness\n1          ska    0.5079231 0.8864103  -5.472897  0.08975641   0.04448036\n2          pop    0.5995500 0.5964750  -5.963550  0.06233000   0.29199075\n3 classic rock    0.4990784 0.5327784 -10.819353  0.05376667   0.30427843\n4      hip hop    0.5907917 0.6566667  -7.638500  0.17566667   0.20510001\n  instrumentalness  liveness   valence    tempo\n1     0.0005529697 0.2082308 0.6734103 132.4416\n2     0.0002870968 0.1485525 0.4636750 116.5934\n3     0.1844926584 0.3240745 0.5117824 130.3288\n4     0.0285232308 0.2477062 0.4617292 121.2545\n\n\n\nDetermine which pop song is the most danceable (highest danceability). Display only the categorical information about the song and its respective danceability score. HINT: Think about the series of pipes that we need to do.\n\n\n# find the most danceable pop song and only display the needed info\n# -&gt; use filter() to subset the dataframe by row multiple times (highest dance within only pop)\ndata_music %&gt;% \n  filter(genre == \"pop\") %&gt;% \n  filter(danceability == max(danceability)) %&gt;% \n  select(where(is.character), danceability)\n\n      artist X.Deluxe. genre            name danceability\n1 Ed Sheeran  (Deluxe)   pop What Do I Know?        0.838\n\n\n\nUse the same strategy as 5 to determine which which Adele song has the lowest energy.\n\n\n# find minimum energy within Adele songs\ndata_music %&gt;% \n  filter(artist == \"Adele\") %&gt;% \n  filter(energy == min(energy)) %&gt;% \n  select(where(is.character), energy)\n\n  artist X.Deluxe. genre              name energy\n1  Adele        25   pop Million Years Ago  0.273\n\n\n\nCHALLENGE: Find the artists with the lowest average acousticness for each genre.\n\n\n# find the artists with the lowest average acousticness for each genre\ndata_music %&gt;% \n  summarize(.by = c(genre, artist),\n            avg_acousticness = mean(acousticness)) %&gt;% \n  filter(.by  = genre,\n         avg_acousticness == min(avg_acousticness))\n\n         genre         artist avg_acousticness\n1 classic rock         Eagles       0.21793778\n2      hip hop       Doomtree       0.04253855\n3          ska Less Than Jake       0.03212957\n4          pop   Taylor Swift       0.11372077",
    "crumbs": [
      "Test 2",
      "Lecture 3 -- Explorartory Data Analysis"
    ]
  },
  {
    "objectID": "lecture-3.html#in-class-activity",
    "href": "lecture-3.html#in-class-activity",
    "title": "Lecture 3 – Explorartory Data Analysis",
    "section": "In-Class Activity",
    "text": "In-Class Activity\nUpload your completed (to the best of your ability) the completed starter file for the notes ‘working-with-data’.",
    "crumbs": [
      "Test 2",
      "Lecture 3 -- Explorartory Data Analysis"
    ]
  },
  {
    "objectID": "lecture-3.html#homework",
    "href": "lecture-3.html#homework",
    "title": "Lecture 3 – Explorartory Data Analysis",
    "section": "Homework",
    "text": "Homework\nFILES: Excel high-school-data.xlsx, R high-school-data.csv, homework-3.tex, and style-assessments.sty\nInstructors can contact me for solutions :)",
    "crumbs": [
      "Test 2",
      "Lecture 3 -- Explorartory Data Analysis"
    ]
  },
  {
    "objectID": "lecture-4.html",
    "href": "lecture-4.html",
    "title": "Lecture 4 – Point Estimation",
    "section": "",
    "text": "Guided Notes\nFILES: BLANK lecture-4.pdf, lecture-4.tex, and style-lectures.sty",
    "crumbs": [
      "Test 2",
      "Lecture 4 -- Point Estimation"
    ]
  },
  {
    "objectID": "lecture-4.html#in-class-activity",
    "href": "lecture-4.html#in-class-activity",
    "title": "Lecture 4 – Point Estimation",
    "section": "In-Class Activity",
    "text": "In-Class Activity\nFILES: in-class-4.tex and style-assessments.sty\nInstructors can contact me for keys :)",
    "crumbs": [
      "Test 2",
      "Lecture 4 -- Point Estimation"
    ]
  },
  {
    "objectID": "lecture-4.html#homework",
    "href": "lecture-4.html#homework",
    "title": "Lecture 4 – Point Estimation",
    "section": "Homework",
    "text": "Homework\nFILES: homework-4.tex and style-assessments.sty\nInstructors can contact me for solutions :)",
    "crumbs": [
      "Test 2",
      "Lecture 4 -- Point Estimation"
    ]
  },
  {
    "objectID": "review-2.html",
    "href": "review-2.html",
    "title": "Review – Test 2",
    "section": "",
    "text": "Study Guide\nFILES: study-guide-2.tex and style-assessments.sty",
    "crumbs": [
      "Test 2",
      "Review -- Test 2"
    ]
  },
  {
    "objectID": "review-2.html#review-problems",
    "href": "review-2.html#review-problems",
    "title": "Review – Test 2",
    "section": "Review Problems",
    "text": "Review Problems",
    "crumbs": [
      "Test 2",
      "Review -- Test 2"
    ]
  },
  {
    "objectID": "test-3.html",
    "href": "test-3.html",
    "title": "Test 3",
    "section": "",
    "text": "This section contains all of the content and assessments covered on the third test.",
    "crumbs": [
      "Test 3"
    ]
  },
  {
    "objectID": "lecture-5.html",
    "href": "lecture-5.html",
    "title": "Lecture 5 – The Central Limit Theorem",
    "section": "",
    "text": "Guided Notes\nFILES: BLANK lecture-5.pdf, lecture-5.tex, and style-lectures.sty",
    "crumbs": [
      "Test 3",
      "Lecture 5 -- The Central Limit Theorem"
    ]
  },
  {
    "objectID": "lecture-5.html#in-class-activity",
    "href": "lecture-5.html#in-class-activity",
    "title": "Lecture 5 – The Central Limit Theorem",
    "section": "In-Class Activity",
    "text": "In-Class Activity\nFILES: in-class-5.tex and style-assessments.sty\nInstructors can contact me for keys :)",
    "crumbs": [
      "Test 3",
      "Lecture 5 -- The Central Limit Theorem"
    ]
  },
  {
    "objectID": "lecture-6.html",
    "href": "lecture-6.html",
    "title": "Lecture 6 – Confidence Intervals",
    "section": "",
    "text": "Guided Notes\nFILES: BLANK lecture-6.pdf, lecture-6.tex, and style-lectures.sty",
    "crumbs": [
      "Test 3",
      "Lecture 6 -- Confidence Intervals"
    ]
  },
  {
    "objectID": "lecture-6.html#in-class-activity",
    "href": "lecture-6.html#in-class-activity",
    "title": "Lecture 6 – Confidence Intervals",
    "section": "In-Class Activity",
    "text": "In-Class Activity\nFILES: in-class-6.tex and style-assessments.sty\nInstructors can contact me for keys :)",
    "crumbs": [
      "Test 3",
      "Lecture 6 -- Confidence Intervals"
    ]
  },
  {
    "objectID": "lecture-6.html#homework",
    "href": "lecture-6.html#homework",
    "title": "Lecture 6 – Confidence Intervals",
    "section": "Homework",
    "text": "Homework\nFILES: Excel homework-6-data.xlsx, R homework-6-data.RData, homework-6.tex and style-assessments.sty\n\nIf doing the assignment in R: After saving the .RData file in the same directory as your R script, run the following command to load the data.\n\n\n# load data\nload(\"r/files/data/homework-6-data.RData\")\n\n\n\n--- data_40_yard_dash ---\n'data.frame':   24 obs. of  3 variables:\n $ Girl  : int  1 2 3 4 5 6 7 8 9 10 ...\n $ Before: num  14.9 13 13.2 14 10 ...\n $ After : num  14.6 13 13.1 13.7 10.1 ...\n--- data_bacteria ---\n int [1:19] 93 140 33 70 91 61 7 14 94 57 ...\n--- data_earnings ---\n'data.frame':   11 obs. of  2 variables:\n $ X1: int  1612 1352 1456 1222 1350 1560 1456 1924 NA NA ...\n $ X2: int  1082 1300 1092 1040 910 1248 1092 1040 1003 1092 ...\n--- data_soybean ---\n num [1:5] 37.6 48.9 54.5 45 55\n\n\nInstructors can contact me for solutions :)",
    "crumbs": [
      "Test 3",
      "Lecture 6 -- Confidence Intervals"
    ]
  },
  {
    "objectID": "lecture-7.html",
    "href": "lecture-7.html",
    "title": "Lecture 7 – Hypothesis Tests",
    "section": "",
    "text": "Guided Notes\nFILES: BLANK lecture-7.pdf, lecture-7.tex, and style-lectures.sty",
    "crumbs": [
      "Test 3",
      "Lecture 7 -- Hypothesis Tests"
    ]
  },
  {
    "objectID": "lecture-7.html#in-class-activity",
    "href": "lecture-7.html#in-class-activity",
    "title": "Lecture 7 – Hypothesis Tests",
    "section": "In-Class Activity",
    "text": "In-Class Activity\nFILES: in-class-7-6.tex and style-assessments.sty\nInstructors can contact me for keys :)",
    "crumbs": [
      "Test 3",
      "Lecture 7 -- Hypothesis Tests"
    ]
  },
  {
    "objectID": "lecture-7.html#homework",
    "href": "lecture-7.html#homework",
    "title": "Lecture 7 – Hypothesis Tests",
    "section": "Homework",
    "text": "Homework\nFILES: homework-7.tex and style-assessments.sty\nInstructors can contact me for solutions :)",
    "crumbs": [
      "Test 3",
      "Lecture 7 -- Hypothesis Tests"
    ]
  },
  {
    "objectID": "review-3.html",
    "href": "review-3.html",
    "title": "Review – Test 3",
    "section": "",
    "text": "Study Guide\nFILES: study-guide-3.tex and style-assessments.sty\nNote this test uses the same distribution table as in Study Guide.",
    "crumbs": [
      "Test 3",
      "Review -- Test 3"
    ]
  },
  {
    "objectID": "review-3.html#review-problems",
    "href": "review-3.html#review-problems",
    "title": "Review – Test 3",
    "section": "Review Problems",
    "text": "Review Problems",
    "crumbs": [
      "Test 3",
      "Review -- Test 3"
    ]
  },
  {
    "objectID": "test-3-after.html",
    "href": "test-3-after.html",
    "title": "After Test 3",
    "section": "",
    "text": "This section contains all of the content and assessments included on the final after test 3.",
    "crumbs": [
      "After Test 3"
    ]
  },
  {
    "objectID": "lecture-8.html",
    "href": "lecture-8.html",
    "title": "Lecture 8 – Regression",
    "section": "",
    "text": "Guided Notes\nFILES: BLANK lecture-8.pdf, lecture-8.tex, and style-lectures.sty",
    "crumbs": [
      "After Test 3",
      "Lecture 8 -- Regression"
    ]
  },
  {
    "objectID": "lecture-8.html#r-notes",
    "href": "lecture-8.html#r-notes",
    "title": "Lecture 8 – Regression",
    "section": "R Notes",
    "text": "R Notes\nFILES: regression-STARTER.qmd and regression.html\n\n# load packages\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(corrplot)\n\ncorrplot 0.92 loaded\n\nlibrary(olsrr)\n\n\nAttaching package: 'olsrr'\n\nThe following object is masked from 'package:datasets':\n\n    rivers\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.1.1 ──\n✔ broom        1.0.5     ✔ rsample      1.2.0\n✔ dials        1.2.0     ✔ tune         1.1.2\n✔ infer        1.0.5     ✔ workflows    1.1.3\n✔ modeldata    1.2.0     ✔ workflowsets 1.0.1\n✔ parsnip      1.1.1     ✔ yardstick    1.2.0\n✔ recipes      1.0.8     \n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n• Learn how to get started at https://www.tidymodels.org/start/\n\n\n\nOverview\nThe goal of these notes is to demonstrate how to fit linear regression models in R and perform other common tasks related to modelling.",
    "crumbs": [
      "After Test 3",
      "Lecture 8 -- Regression"
    ]
  },
  {
    "objectID": "lecture-8.html#fitting-slr-regression-model",
    "href": "lecture-8.html#fitting-slr-regression-model",
    "title": "Lecture 8 – Regression",
    "section": "Fitting SLR regression model",
    "text": "Fitting SLR regression model\nTo demonstrate how to fit a simple linear regression model in R, we will use the msleep dataset, specifically we will predict the total amount of sleep (sleep_total) from length of sleep cycle (sleep_cycle).\n\n# preview data\nglimpse(msleep)\n\nRows: 83\nColumns: 11\n$ name         &lt;chr&gt; \"Cheetah\", \"Owl monkey\", \"Mountain beaver\", \"Greater shor…\n$ genus        &lt;chr&gt; \"Acinonyx\", \"Aotus\", \"Aplodontia\", \"Blarina\", \"Bos\", \"Bra…\n$ vore         &lt;chr&gt; \"carni\", \"omni\", \"herbi\", \"omni\", \"herbi\", \"herbi\", \"carn…\n$ order        &lt;chr&gt; \"Carnivora\", \"Primates\", \"Rodentia\", \"Soricomorpha\", \"Art…\n$ conservation &lt;chr&gt; \"lc\", NA, \"nt\", \"lc\", \"domesticated\", NA, \"vu\", NA, \"dome…\n$ sleep_total  &lt;dbl&gt; 12.1, 17.0, 14.4, 14.9, 4.0, 14.4, 8.7, 7.0, 10.1, 3.0, 5…\n$ sleep_rem    &lt;dbl&gt; NA, 1.8, 2.4, 2.3, 0.7, 2.2, 1.4, NA, 2.9, NA, 0.6, 0.8, …\n$ sleep_cycle  &lt;dbl&gt; NA, NA, NA, 0.1333333, 0.6666667, 0.7666667, 0.3833333, N…\n$ awake        &lt;dbl&gt; 11.9, 7.0, 9.6, 9.1, 20.0, 9.6, 15.3, 17.0, 13.9, 21.0, 1…\n$ brainwt      &lt;dbl&gt; NA, 0.01550, NA, 0.00029, 0.42300, NA, NA, NA, 0.07000, 0…\n$ bodywt       &lt;dbl&gt; 50.000, 0.480, 1.350, 0.019, 600.000, 3.850, 20.490, 0.04…\n\n\nLets first see if a linear regression is appropriate by creating scatterplot of the two variables.\n\n# create scatterplot\nplot(x = msleep$sleep_cycle, y = msleep$sleep_total)\n\n\n\n\n\n\n\n\nThere does appear to be a linear relationship between the two variables, so we can continue.\nWe learned the formulas for the point estimators of \\(\\beta_0\\) and \\(\\beta_1\\):\n\\[\n\\begin{align*}\n  \\text{Intercept} \\hspace{10pt} \\hat{\\beta}_0 \\hspace{10pt} &= \\hspace{10pt} \\frac{1}{n}\\sum Y_i + \\hat{\\beta}_1 \\frac{1}{n} \\sum X_i \\hspace{10pt} = \\hspace{10pt} \\bar{Y}- \\hat{\\beta}_1 \\bar{X} \\\\\n  \\text{Slope} \\hspace{10pt} \\hat{\\beta_1} \\hspace{10pt} &= \\hspace{10pt} \\frac{\\sum X_i Y_i -\\frac{1}{n} \\sum X_i Y_i}{\\sum X_i^2 - \\frac{1}{n}(\\sum X_i)^2} \\hspace{10pt} = \\hspace{10pt} \\frac{\\sum(X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum(X_i - \\bar{X})^2} \\hspace{10pt} = \\hspace{10pt} \\frac{S_{XY}}{S_{XX}}\n\\end{align*}\n\\]\nUsing R functions, we can calculate these with the using lm(formula = &lt; y &gt; ~ &lt; x &gt;, data = &lt; data &gt; ), and then extract them with coef(&lt; mod &gt;).\n\n# fit regression model\nmod_sleep &lt;- lm(sleep_total ~ sleep_cycle, data = msleep)\n\n# view estimated coefficients\ncoef(mod_sleep)\n\n(Intercept) sleep_cycle \n  13.528008   -5.374142 \n\n\nFitted model: \\(\\widehat{\\text{sleep total}}\\) = 13.53 - 5.37 \\(\\times \\text{ sleep cycle}\\)\nInterpretation of \\(\\hat{\\beta}_1\\): As the sleep cycle increases by 1 hours, the total amount of sleep decreases by 5.37 hours on average.\nHere is the resulting estimated regression line visualized:\n\n# add regression line to scatterplot\nplot(x = msleep$sleep_cycle, y = msleep$sleep_total)\nabline(mod_sleep, col = \"red\")\n\n\n\n\n\n\n\n\n\nTesting a SLR model\nTo see if the estimated slope is significantly different than zero (horizontal regression line), we can test the following hypotheses:\n\\[\n\\begin{align*}\n  H_0 &: \\beta_1 = 0 \\\\\n  H_A &: \\beta_1 \\ne 0\n\\end{align*}\n\\]\nThen use the test statistic:\n\\[\nTS = t^* = \\frac{\\hat{\\beta}_1 - 0}{S_{\\hat{\\beta}_1}} = \\frac{\\hat{\\beta}_1}{\\sqrt{MSE / S_{XX}}}\n\\]\nNext, make the conclusion based on:\n\\[\n\\begin{align*}\n  RR &= \\{\\lvert t^* \\rvert &gt; t_{\\alpha/2, n - 2}\\} \\\\\n  p\\text{-value} &= 2 \\cdot P(t_{n-2} \\ge \\lvert t^* \\rvert)\n\\end{align*}\n\\]\nAnd finally write the correct interpretation.\nR gives us the result of this test in the output of summary(&lt; mod &gt;).\n\n# view model summary\nsummary(mod_sleep)\n\n\nCall:\nlm(formula = sleep_total ~ sleep_cycle, data = msleep)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.9452 -2.7981 -0.4731  1.9073  7.4468 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   13.528      1.028  13.154 5.44e-14 ***\nsleep_cycle   -5.374      1.824  -2.946  0.00617 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.643 on 30 degrees of freedom\n  (51 observations deleted due to missingness)\nMultiple R-squared:  0.2244,    Adjusted R-squared:  0.1986 \nF-statistic:  8.68 on 1 and 30 DF,  p-value: 0.006169\n\n\nWe can easily verify the results for the test statistic and p-value using the some of the formulas above and other R results.\n\n\\(\\sqrt{MSE} = S = \\text{Residual standard error} = \\text{sigma}\\)\n\n\n# save model summary\nsumm &lt;- summary(mod_sleep)\n\n# calculate needed values\nbeta1_hat &lt;- coef(mod_sleep)[2] %&gt;% as.numeric\nmse &lt;- summ$sigma^2\ns_xx &lt;- sum((mod_sleep$model$sleep_cycle - mean(mod_sleep$model$sleep_cycle))^2)\nse &lt;- sqrt(mse / s_xx)\n\n# calculate test statistic and p-value\n(t &lt;- beta1_hat / se)\n\n[1] -2.94617\n\n(p_value &lt;- 2 * pt(t, df = df.residual(mod_sleep)))\n\n[1] 0.006168546\n\n\nThe summary also gives us the coefficient of determination \\(R^2\\), which we would like to maximize.\n\n# model summary\n# -&gt; looking for 'Multiple R-squared'\nsummary(mod_sleep)\n\n\nCall:\nlm(formula = sleep_total ~ sleep_cycle, data = msleep)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.9452 -2.7981 -0.4731  1.9073  7.4468 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   13.528      1.028  13.154 5.44e-14 ***\nsleep_cycle   -5.374      1.824  -2.946  0.00617 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.643 on 30 degrees of freedom\n  (51 observations deleted due to missingness)\nMultiple R-squared:  0.2244,    Adjusted R-squared:  0.1986 \nF-statistic:  8.68 on 1 and 30 DF,  p-value: 0.006169\n\n\n\n\nDiagnostics\nTo see if our model is appropriate, we need to perform residual analysis and look for the specific patterns of interest in LINE (Linearity, Independence, Normality, Equal variance).\nThis is really easy to do in R: simply run plot(&lt; mod &gt;).\n\n# create residual plots\nplot(mod_sleep, which = 1:2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# plot distribution plots of residuals\nboxplot(residuals(mod_sleep), horizontal = TRUE)\n\n\n\n\n\n\n\nhist(residuals(mod_sleep))\n\n\n\n\n\n\n\n\nFrom these plots, it seems that the only issue is with non-constant variance. One potential remedy for this is to transform the response variable and refit the model.\n\n\nYOUR TURN\nUse the the mpg dataset to obtain a regression for predicting highway mpg hwy from city mpg cty. Then assess its adequacy.\n\n# preview data\nglimpse(mpg)\n\nRows: 234\nColumns: 11\n$ manufacturer &lt;chr&gt; \"audi\", \"audi\", \"audi\", \"audi\", \"audi\", \"audi\", \"audi\", \"…\n$ model        &lt;chr&gt; \"a4\", \"a4\", \"a4\", \"a4\", \"a4\", \"a4\", \"a4\", \"a4 quattro\", \"…\n$ displ        &lt;dbl&gt; 1.8, 1.8, 2.0, 2.0, 2.8, 2.8, 3.1, 1.8, 1.8, 2.0, 2.0, 2.…\n$ year         &lt;int&gt; 1999, 1999, 2008, 2008, 1999, 1999, 2008, 1999, 1999, 200…\n$ cyl          &lt;int&gt; 4, 4, 4, 4, 6, 6, 6, 4, 4, 4, 4, 6, 6, 6, 6, 6, 6, 8, 8, …\n$ trans        &lt;chr&gt; \"auto(l5)\", \"manual(m5)\", \"manual(m6)\", \"auto(av)\", \"auto…\n$ drv          &lt;chr&gt; \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"4\", \"4\", \"4\", \"4\", \"4…\n$ cty          &lt;int&gt; 18, 21, 20, 21, 16, 18, 18, 18, 16, 20, 19, 15, 17, 17, 1…\n$ hwy          &lt;int&gt; 29, 29, 31, 30, 26, 26, 27, 26, 25, 28, 27, 25, 25, 25, 2…\n$ fl           &lt;chr&gt; \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p…\n$ class        &lt;chr&gt; \"compact\", \"compact\", \"compact\", \"compact\", \"compact\", \"c…\n\n# create scatterplot\nplot(x = mpg$cty, y = mpg$hwy)\n\n\n\n\n\n\n\n# fit regression model\nmod_mpg &lt;- lm(hwy ~ cty, data = mpg)\n\n# view estimated coefficients\ncoef(mod_mpg)\n\n(Intercept)         cty \n  0.8920411   1.3374556 \n\n# perform residual analysis\nplot(mod_mpg, which = 1:2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFrom the residual plots, there does not appear to be any issues with the assumptions.\n\n\nMultiple linear regression (MLR)\nIt is very easy to extend SLR to multiple predictor variables. This allows us to improve our models by taking into account more predictors.\nSome other issues arise when we do this, such as which predictor variables to include and the relationship between the predictors. We will focus on the former and learn some easily implemented algorithms for variable selection.\nTo see how to do this in R, lets perform some more EDA on the mpg dataset, specifically for the numeric predictors.\n\n# select numeric variables\nmpg_numeric &lt;- mpg %&gt;% \n  select(where(is.numeric)) \n\n# create scatterplot matrix\npairs(mpg_numeric)\n\n\n\n\n\n\n\n# create correlation matrix\n(r &lt;- mpg_numeric %&gt;% \n  as.matrix %&gt;% \n  cor %&gt;% \n  round(3))\n\n       displ   year    cyl    cty    hwy\ndispl  1.000  0.148  0.930 -0.799 -0.766\nyear   0.148  1.000  0.122 -0.037  0.002\ncyl    0.930  0.122  1.000 -0.806 -0.762\ncty   -0.799 -0.037 -0.806  1.000  0.956\nhwy   -0.766  0.002 -0.762  0.956  1.000\n\n# visualize correlation matrix\ncorrplot::corrplot(r)\n\n\n\n\n\n\n\n\nFrom this it looks like we would want to include the following predictors as well: disp, and cyl. Now lets see which categorical predictors are relevant for predicting highway mpg.\n\n# EDA for categorical predictors\n\n# see number of categories for each variable\nmpg %&gt;% \n  select(where(is.character)) %&gt;% \n  map(\\(col) length(unique(col)))\n\n$manufacturer\n[1] 15\n\n$model\n[1] 38\n\n$trans\n[1] 10\n\n$drv\n[1] 3\n\n$fl\n[1] 5\n\n$class\n[1] 7\n\n# -&gt; comparative boxplots\nggplot(data = mpg,\n       aes(x = hwy,\n           y = class)) + \n  geom_boxplot()\n\n\n\n\n\n\n\n\nIt appears all of the categorical variables have a relationship on highway mpg. For now, lets just pick a few with a small number of categories (we will see why): drv and class.\nNow lets fit a MLR model using the results of our EDA.\n\n# fit MLR model\nmod_mpg_mlr &lt;- lm(hwy ~ cty + cyl + drv + class, data = mpg)\n\n# view model summary\nsummary(mod_mpg_mlr)\n\n\nCall:\nlm(formula = hwy ~ cty + cyl + drv + class, data = mpg)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.3523 -0.6388 -0.0466  0.5908  3.9197 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      7.70879    1.31486   5.863 1.63e-08 ***\ncty              1.06802    0.03956  27.001  &lt; 2e-16 ***\ncyl             -0.02922    0.09139  -0.320 0.749448    \ndrvf             0.69361    0.30424   2.280 0.023563 *  \ndrvr             0.87755    0.34415   2.550 0.011447 *  \nclasscompact    -1.28980    0.66486  -1.940 0.053646 .  \nclassmidsize    -0.93540    0.67766  -1.380 0.168864    \nclassminivan    -2.76281    0.76941  -3.591 0.000405 ***\nclasspickup     -4.50876    0.68094  -6.621 2.62e-10 ***\nclasssubcompact -1.83763    0.63681  -2.886 0.004289 ** \nclasssuv        -3.95005    0.63486  -6.222 2.40e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.207 on 223 degrees of freedom\nMultiple R-squared:  0.9607,    Adjusted R-squared:  0.9589 \nF-statistic: 544.7 on 10 and 223 DF,  p-value: &lt; 2.2e-16\n\n\nNotice the following:\n\nReduction in MSE: SLR = 3.07 and MLR = 1.46\nIncrease in \\(R^2\\): SLR = 0.914 and MLR = 0.961\n\nRecall that \\(R^2\\) always increases when additional predictors are included, so it is better to look at the adjusted-\\(R^2\\), which takes into account the inclusion of more variables.\nIncrease in \\(R^2_{adj}\\): SLR = 0.913 and MLR = 0.959\n\nIncrease in the number of coefficients: SLR = 2 and MLR = 11\n\nEven though the MLR model only had 4 predictor variables, there is many more coefficients because each categorical variable gets \\(k - 1\\) coefficients per \\(k\\) levels.\n\n\nNow lets assess the adequacy of our new model.\n\n# perform residual analysis\nplot(mod_mpg_mlr, which = 1:2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThere does not appear to be any issues with the assumptions.\n\n\nVariable selection\nIf we take a look at the previous MLR model summary, the majority of the predictors are significant, but some are not.\n\n# check significance of terms in model summary \nsummary(mod_mpg_mlr)\n\n\nCall:\nlm(formula = hwy ~ cty + cyl + drv + class, data = mpg)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.3523 -0.6388 -0.0466  0.5908  3.9197 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      7.70879    1.31486   5.863 1.63e-08 ***\ncty              1.06802    0.03956  27.001  &lt; 2e-16 ***\ncyl             -0.02922    0.09139  -0.320 0.749448    \ndrvf             0.69361    0.30424   2.280 0.023563 *  \ndrvr             0.87755    0.34415   2.550 0.011447 *  \nclasscompact    -1.28980    0.66486  -1.940 0.053646 .  \nclassmidsize    -0.93540    0.67766  -1.380 0.168864    \nclassminivan    -2.76281    0.76941  -3.591 0.000405 ***\nclasspickup     -4.50876    0.68094  -6.621 2.62e-10 ***\nclasssubcompact -1.83763    0.63681  -2.886 0.004289 ** \nclasssuv        -3.95005    0.63486  -6.222 2.40e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.207 on 223 degrees of freedom\nMultiple R-squared:  0.9607,    Adjusted R-squared:  0.9589 \nF-statistic: 544.7 on 10 and 223 DF,  p-value: &lt; 2.2e-16\n\n\nWe could probably remove cyl to get a more parsimonious model, which means fewer predictors. In general, we want the best, smallest model that is possible.\nLets remove it and see the result:\n\n# fit new MLR model\nmod_mpg_mlr2 &lt;- lm(hwy ~ cty + drv + class, data = mpg)\n\n# view model summary\nsummary(mod_mpg_mlr2)\n\n\nCall:\nlm(formula = hwy ~ cty + drv + class, data = mpg)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.3867 -0.6405 -0.0257  0.5591  3.9171 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      7.38398    0.83321   8.862 2.48e-16 ***\ncty              1.07596    0.03073  35.013  &lt; 2e-16 ***\ndrvf             0.68243    0.30161   2.263  0.02462 *  \ndrvr             0.84631    0.32933   2.570  0.01083 *  \nclasscompact    -1.25076    0.65224  -1.918  0.05643 .  \nclassmidsize    -0.90452    0.66939  -1.351  0.17798    \nclassminivan    -2.72243    0.75745  -3.594  0.00040 ***\nclasspickup     -4.49261    0.67770  -6.629 2.49e-10 ***\nclasssubcompact -1.80645    0.62804  -2.876  0.00441 ** \nclasssuv        -3.93049    0.63064  -6.233 2.25e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.205 on 224 degrees of freedom\nMultiple R-squared:  0.9607,    Adjusted R-squared:  0.9591 \nF-statistic: 607.7 on 9 and 224 DF,  p-value: &lt; 2.2e-16\n\n\nRemoving this had virtually no effect on the quality of the fit, so it was a good decision.\nRecall however, we eliminated some predictor variables from the start after our EDA, which was kind of subjective. Now we will learn an algorithm to make the decisions for us in a prescribed manner.\nStepwise variable selection is a search method that develops a sequence of regression models, at each step adding or deleting an \\(X\\) variable (iterative procedure). Several criterion can be used to make the decision to include or delete a variable; we will use p-values.\nForward stepwise regression\n\nStep 0: Start with intercept-only model.\nStep 1: Fit all one variable models and evaluate criteria. Find the best. For example, the largest or equivalently smallest p-value.\nStep 2: Start with the variable from the previous step and fit all 2 variable models. Find the best second variable and see if it meets the keep criteria.\nStep 3: Check to see if a variable should be deleted. Fit model with all predictors currently kept and see if one variable should be dropped (i.e. see if criteria is on wrong side of the keep criteria).\nStep 4: Continue adding and checking to see if previous variable should be dropped until adding a variable doesn’t improve the model and dropping a variable doesn’t improve the model. Then algorithm is done.\nNote that the stepwise regression algorithm allows any variable, brought into the model at an earlier stage, to be dropped subsequently if it is no longer helpful in conjunction with variables added at later stages.\n\nHere is how to do it in R (using the olsrr package). For illustration purposes, we will only use the numeric predictors, although this process also works with categorical predictors.\n\n# specify full model\nmod_full &lt;- lm(hwy ~ ., data = mpg_numeric)\n\n# perform forward stepwise regression based on p-values\nmod_step &lt;- olsrr::ols_step_both_p(mod_full)\n\n# show model summary\nsummary(mod_step$model)\n\n\nCall:\nlm(formula = paste(response, \"~\", paste(preds, collapse = \" + \")), \n    data = l)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-5.138 -1.138 -0.065  1.256  4.274 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -99.14248   50.74026  -1.954   0.0519 .  \ncty           1.33942    0.02683  49.931   &lt;2e-16 ***\nyear          0.04991    0.02532   1.972   0.0499 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.741 on 231 degrees of freedom\nMultiple R-squared:  0.9152,    Adjusted R-squared:  0.9145 \nF-statistic:  1247 on 2 and 231 DF,  p-value: &lt; 2.2e-16\n\n\nNow we can add the categorical predictors to the algorithm and see the results.\n\n# respecify full model to include categorical predictors\nmod_full2 &lt;- lm(hwy ~ . - model, data = mpg)\n\n# perform forward stepwise regression based on p-values\nmod_step2 &lt;- olsrr::ols_step_both_p(mod_full2)\n\n# show model summary\nsummary(mod_step2$model)\n\n\nCall:\nlm(formula = paste(response, \"~\", paste(preds, collapse = \" + \")), \n    data = l)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.45795 -0.57469  0.01054  0.51679  2.52522 \n\nCoefficients:\n                         Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)            -139.48382   44.38094  -3.143 0.001934 ** \ncty                       0.97490    0.04906  19.872  &lt; 2e-16 ***\nclasscompact             -3.13686    0.73666  -4.258 3.20e-05 ***\nclassmidsize             -2.31056    0.68969  -3.350 0.000970 ***\nclassminivan             -4.61590    0.89732  -5.144 6.52e-07 ***\nclasspickup              -4.95537    0.75945  -6.525 5.73e-10 ***\nclasssubcompact          -3.18029    0.70112  -4.536 1.00e-05 ***\nclasssuv                 -4.59677    0.68885  -6.673 2.53e-10 ***\nfld                      -1.62891    1.24272  -1.311 0.191479    \nfle                      -4.97737    1.12366  -4.430 1.57e-05 ***\nflp                      -3.96375    1.07269  -3.695 0.000285 ***\nflr                      -3.46911    1.06135  -3.269 0.001278 ** \nyear                      0.07719    0.02237   3.450 0.000687 ***\ndrvf                      2.16542    0.39969   5.418 1.77e-07 ***\ndrvr                      1.71180    0.38582   4.437 1.53e-05 ***\nmanufacturerchevrolet    -2.82578    0.59161  -4.776 3.50e-06 ***\nmanufacturerdodge        -2.26467    0.53004  -4.273 3.02e-05 ***\nmanufacturerford         -2.54081    0.50929  -4.989 1.34e-06 ***\nmanufacturerhonda        -2.86661    0.66258  -4.326 2.42e-05 ***\nmanufacturerhyundai      -2.90812    0.58728  -4.952 1.59e-06 ***\nmanufacturerjeep         -2.96208    0.57733  -5.131 6.95e-07 ***\nmanufacturerland rover   -1.18301    0.65423  -1.808 0.072110 .  \nmanufacturerlincoln      -2.42810    0.83745  -2.899 0.004167 ** \nmanufacturermercury      -2.25766    0.67751  -3.332 0.001030 ** \nmanufacturernissan       -2.94845    0.48790  -6.043 7.54e-09 ***\nmanufacturerpontiac      -1.29376    0.69319  -1.866 0.063487 .  \nmanufacturersubaru       -1.51562    0.49764  -3.046 0.002643 ** \nmanufacturertoyota       -2.74390    0.46076  -5.955 1.19e-08 ***\nmanufacturervolkswagen   -2.49673    0.43555  -5.732 3.72e-08 ***\ntransauto(l3)             0.24109    0.93082   0.259 0.795900    \ntransauto(l4)             1.37284    0.56959   2.410 0.016871 *  \ntransauto(l5)             1.81882    0.54962   3.309 0.001114 ** \ntransauto(l6)             1.87811    0.69343   2.708 0.007361 ** \ntransauto(s4)             0.46530    0.84886   0.548 0.584214    \ntransauto(s5)             2.81847    0.76664   3.676 0.000306 ***\ntransauto(s6)             1.25146    0.58140   2.153 0.032586 *  \ntransmanual(m5)           1.49335    0.55958   2.669 0.008256 ** \ntransmanual(m6)           1.32873    0.55027   2.415 0.016673 *  \ncyl                      -0.21126    0.09301  -2.271 0.024211 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9806 on 195 degrees of freedom\nMultiple R-squared:  0.9773,    Adjusted R-squared:  0.9729 \nF-statistic:   221 on 38 and 195 DF,  p-value: &lt; 2.2e-16\n\n# check diagnostics one last time\nplot(mod_step2$model, which = 1:2)\n\nWarning: not plotting observations with leverage one:\n  107\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel validation\nThe final step in the model-building process is the validation of the selected regression models. Model validation usually involves checking a candidate model against independent data. Three basic ways of validating a regression model are:\n\nCollection of new data to check the model and its predictive ability.\nComparison of results with theoretical expectations, earlier empirical results, and simulation results.\nUse of a holdout sample to check the model and its predictive ability.\n\nIf our model is generalizable (meaning applicable to new data), then we should see relatively small prediction errors.\nCollecting more data in practice is often costly, so we will use a holdout sample, which means a portion of the data is held out from the original model training and reserved for testing (a common ratio for training / testing split is 75/25).\nOnce candidate models are trained on the say 75% of the data, we apply that model to the 25% and calculate some measure of predictive accuracy. A common measure is root mean square prediction error RMSE:\n\\[\\text{RMSE} = \\bigg[\\frac{1}{n_{\\text{test}}} \\sum^{n_{\\text{test}}}_{i = 1} (y_{\\text{new, }i} - \\hat{y}_{\\text{new, }i})^2\\bigg]^{1/2}\\]\nThis value can be compared to the estimated \\(\\sqrt{MSE} = S\\) from the models for the training data to get an idea of its magnitude.\nTo practice this, now lets split our mpg data and fit the candidate models from earlier on the training data, then apply each of them with the testing data.\n\n# set seed on a split that works\nset.seed(1)\n\n# make train / test split\n# -&gt; note using a higher proportion in the training data to ensure appearance of all unique levels of the categorical variables\nnrow(mpg)\n\n[1] 234\n\nsplit_mpg &lt;- rsample::initial_split(data = mpg, prop = .90)\n\n# save train data\ndata_train &lt;- split_mpg %&gt;% rsample::training() \nnrow(data_train)\n\n[1] 210\n\n# save test data\ndata_test &lt;- split_mpg %&gt;% rsample::testing() \nnrow(data_test)\n\n[1] 24\n\n\nNow we can refit the models.\n\n# refit several candidate models on the training data\n\n# -&gt; SLR model\nmod_mpg_slr &lt;- lm(formula(mod_mpg), data = data_train)\n\n# -&gt; stepwise MLR with only numeric predictors\nmod_mpg_mlr_num &lt;- lm(formula(mod_step$model), data = data_train)\n\n# -&gt; stepwise MLR now including categorical predictors\nmod_mpg_mlr_cat &lt;- lm(formula(mod_step2$model), data = data_train)\n\nNow we can make predictions and calculate the RMSEs.\n\n# easily make predictions\npreds_slr &lt;- predict(mod_mpg_slr, newdata = data_test)\n\n# calculate RMSE manually\nmean((data_test$hwy - preds_slr)^2) %&gt;% sqrt\n\n[1] 1.495231\n\n# compare to S of the fitted model\nsummary(mod_mpg_slr)$sigma\n\n[1] 1.77967\n\n# get an idea of the predictions compared to the original values\ndata_test_slr &lt;- broom::augment(mod_mpg_slr, newdata = data_test)\n\n# recalculate RMSE to verify it\nmean(data_test_slr$.resid^2) %&gt;% sqrt\n\n[1] 1.495231\n\n\nThis is a very common modelling task, so there are functions to calculate RMSE for us once we make the predictions. Lets try this, then use it to find the RMSE for the other candidate models.\n\n# recalculate RMSE again\nyardstick::rmse_vec(truth = data_test$hwy, estimate = preds_slr)\n\n[1] 1.495231\n\n# calculate RMSE for the other two candidate models\n\n# -&gt; stepwise MLR with only numeric predictors\nyardstick::rmse_vec(truth = data_test$hwy, estimate = predict(mod_mpg_mlr_num, newdata = data_test))\n\n[1] 1.472075\n\n# -&gt; stepwise MLR now including categorical predictors\nyardstick::rmse_vec(truth = data_test$hwy, estimate = predict(mod_mpg_mlr_cat, newdata = data_test))\n\n[1] 1.145359\n\n\nIt appears that the stepwise regression model was the best with an \\(RMSE\\) = 1.145, compared to the original \\(S\\) = 0.975.",
    "crumbs": [
      "After Test 3",
      "Lecture 8 -- Regression"
    ]
  },
  {
    "objectID": "lecture-8.html#in-class-activity",
    "href": "lecture-8.html#in-class-activity",
    "title": "Lecture 8 – Regression",
    "section": "In-Class Activity",
    "text": "In-Class Activity\nUse the accompanying starter file to complete the modelling process for at least TWO different models; this includes:\n\nEDA to specify a candidate model.\nTrain the model.\nCalculate predictions for the testing dataset using the trained model from the prior step.\nCalculate the \\(RMSE\\).\n\nThen determine which of your models was the best based on the minimum \\(RMSE\\).\nFILES: in-class-8-STARTER.qmd and regression-data.RData\nBelow is a preview of the the work that has already been done in the starter file.\n\nLoad data\nThe data we will be working with contains information about the housing market in Ames, TX.\nWe need to load the following RData file, which contains a training and testing dataset.\n\n# read in training and testing data\nload('r/files/data/regression-data.RData')\n\n# preview training data\nglimpse(data_train)\n\nRows: 2,344\nColumns: 61\n$ lot_frontage    &lt;dbl&gt; 46, 60, 0, 78, 74, 43, 0, 0, 80, 80, 100, 60, 21, 75, …\n$ lot_area        &lt;int&gt; 20544, 7200, 9555, 15600, 11988, 3182, 10464, 4426, 92…\n$ year_built      &lt;int&gt; 1986, 1949, 1979, 1949, 1934, 2005, 1980, 2004, 1965, …\n$ year_remod_add  &lt;int&gt; 1991, 1950, 1979, 2005, 1995, 2006, 1980, 2004, 1965, …\n$ mas_vnr_area    &lt;dbl&gt; 123, 0, 0, 0, 0, 16, 130, 169, 0, 252, 0, 0, 0, 0, 0, …\n$ bsmt_fin_sf_1   &lt;dbl&gt; 7, 5, 5, 2, 4, 3, 3, 3, 6, 1, 3, 7, 3, 3, 1, 7, 7, 7, …\n$ bsmt_unf_sf     &lt;dbl&gt; 791, 0, 0, 248, 389, 1357, 138, 186, 244, 467, 172, 85…\n$ total_bsmt_sf   &lt;dbl&gt; 791, 0, 0, 1067, 715, 1373, 988, 848, 1136, 1165, 924,…\n$ first_flr_sf    &lt;int&gt; 1236, 1040, 1100, 986, 849, 1555, 1102, 848, 1136, 116…\n$ second_flr_sf   &lt;int&gt; 857, 0, 1133, 537, 811, 0, 0, 0, 0, 896, 0, 0, 546, 14…\n$ gr_liv_area     &lt;int&gt; 2093, 1040, 2233, 1523, 1660, 1555, 1102, 848, 1136, 2…\n$ bsmt_full_bath  &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, …\n$ full_bath       &lt;int&gt; 2, 2, 2, 2, 1, 2, 1, 1, 1, 2, 1, 1, 1, 2, 2, 2, 2, 2, …\n$ half_bath       &lt;int&gt; 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, …\n$ bedroom_abv_gr  &lt;int&gt; 3, 2, 5, 3, 3, 2, 2, 1, 3, 4, 2, 2, 3, 4, 4, 4, 3, 3, …\n$ kitchen_abv_gr  &lt;int&gt; 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ tot_rms_abv_grd &lt;int&gt; 7, 6, 11, 7, 6, 7, 5, 3, 5, 8, 6, 5, 5, 11, 8, 8, 10, …\n$ fireplaces      &lt;int&gt; 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, …\n$ garage_cars     &lt;dbl&gt; 2, 2, 2, 1, 1, 2, 2, 2, 1, 2, 2, 2, 0, 2, 2, 2, 3, 3, …\n$ garage_area     &lt;dbl&gt; 542, 420, 579, 295, 240, 430, 582, 420, 384, 498, 528,…\n$ wood_deck_sf    &lt;int&gt; 364, 0, 0, 0, 0, 143, 140, 160, 426, 0, 0, 0, 200, 208…\n$ open_porch_sf   &lt;int&gt; 63, 0, 0, 0, 0, 20, 22, 0, 0, 77, 36, 0, 26, 364, 207,…\n$ enclosed_porch  &lt;int&gt; 0, 0, 0, 81, 0, 0, 0, 0, 0, 0, 0, 116, 0, 0, 0, 0, 0, …\n$ screen_porch    &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 196, 0, 0, 0, 0, 224, 0, 0,…\n$ sale_price      &lt;int&gt; 215000, 90000, 141000, 158000, 188700, 192500, 169000,…\n$ longitude       &lt;dbl&gt; -93.63915, -93.60890, -93.67433, -93.64076, -93.64141,…\n$ latitude        &lt;dbl&gt; 42.05602, 42.03584, 42.01917, 42.01494, 42.01844, 42.0…\n$ ms_zoning       &lt;fct&gt; Residential_Low_Density, Residential_Low_Density, Resi…\n$ street          &lt;fct&gt; Pave, Pave, Pave, Pave, Pave, Pave, Pave, Pave, Pave, …\n$ alley           &lt;fct&gt; No_Alley_Access, No_Alley_Access, No_Alley_Access, No_…\n$ lot_shape       &lt;fct&gt; Slightly_Irregular, Regular, Slightly_Irregular, Regul…\n$ land_contour    &lt;fct&gt; Lvl, Lvl, Lvl, Bnk, HLS, Lvl, Lvl, Lvl, Lvl, Lvl, Lvl,…\n$ lot_config      &lt;fct&gt; CulDSac, Inside, CulDSac, Inside, Inside, Inside, FR3,…\n$ land_slope      &lt;fct&gt; Gtl, Gtl, Gtl, Gtl, Mod, Gtl, Gtl, Gtl, Gtl, Gtl, Gtl,…\n$ condition_1     &lt;fct&gt; Norm, Norm, Norm, Norm, Norm, Norm, Norm, Norm, Norm, …\n$ condition_2     &lt;fct&gt; Norm, Norm, Norm, Norm, Norm, Norm, Norm, Norm, Norm, …\n$ bldg_type       &lt;fct&gt; OneFam, Duplex, Duplex, OneFam, OneFam, TwnhsE, OneFam…\n$ house_style     &lt;fct&gt; Two_Story, One_Story, Two_Story, One_and_Half_Fin, Two…\n$ overall_cond    &lt;fct&gt; Above_Average, Average, Above_Average, Good, Good, Ave…\n$ roof_style      &lt;fct&gt; Gable, Gable, Gable, Gable, Hip, Gable, Gable, Gable, …\n$ roof_matl       &lt;fct&gt; CompShg, CompShg, CompShg, CompShg, CompShg, CompShg, …\n$ mas_vnr_type    &lt;fct&gt; BrkFace, None, None, None, None, BrkFace, BrkFace, Brk…\n$ exter_cond      &lt;fct&gt; Good, Typical, Typical, Typical, Typical, Typical, Typ…\n$ foundation      &lt;fct&gt; CBlock, Slab, Slab, BrkTil, CBlock, PConc, CBlock, PCo…\n$ bsmt_cond       &lt;fct&gt; Typical, No_Basement, No_Basement, Typical, Typical, T…\n$ bsmt_exposure   &lt;fct&gt; No, No_Basement, No_Basement, No, No, Av, Av, Av, No, …\n$ bsmt_fin_type_1 &lt;fct&gt; Unf, No_Basement, No_Basement, BLQ, LwQ, GLQ, GLQ, GLQ…\n$ bsmt_fin_type_2 &lt;fct&gt; Unf, No_Basement, No_Basement, Rec, Unf, Unf, Unf, Unf…\n$ heating         &lt;fct&gt; GasA, Wall, GasA, GasW, GasA, GasA, GasA, GasA, GasA, …\n$ heating_qc      &lt;fct&gt; Good, Fair, Typical, Fair, Fair, Excellent, Typical, E…\n$ central_air     &lt;fct&gt; Y, N, Y, N, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, …\n$ electrical      &lt;fct&gt; SBrkr, FuseF, SBrkr, SBrkr, FuseA, SBrkr, SBrkr, SBrkr…\n$ functional      &lt;fct&gt; Typ, Typ, Typ, Maj2, Typ, Typ, Typ, Typ, Typ, Typ, Typ…\n$ garage_type     &lt;fct&gt; Attchd, Detchd, Attchd, Attchd, Detchd, Attchd, Attchd…\n$ garage_finish   &lt;fct&gt; Fin, Unf, Fin, Unf, Unf, Fin, RFn, RFn, RFn, RFn, Unf,…\n$ garage_cond     &lt;fct&gt; Typical, Typical, Good, Typical, Typical, Typical, Typ…\n$ paved_drive     &lt;fct&gt; Paved, Paved, Paved, Paved, Paved, Paved, Paved, Paved…\n$ pool_qc         &lt;fct&gt; No_Pool, No_Pool, No_Pool, No_Pool, No_Pool, No_Pool, …\n$ fence           &lt;fct&gt; Minimum_Privacy, No_Fence, No_Fence, No_Fence, No_Fenc…\n$ misc_feature    &lt;fct&gt; None, None, None, None, None, None, None, None, None, …\n$ sale_condition  &lt;fct&gt; Normal, Normal, Normal, Normal, Normal, Normal, Normal…\n\n\n\n\nIntial EDA\nThe model we want to create is: sale_price ~ &lt; set of Xs &gt;. Let’s first inspect the response variable.\n\n# plot response variable\n# -&gt; seems skewed right and not normal\nhist(data_train$sale_price)\n\n\n\n\n\n\n\nqqnorm(data_train$sale_price)\n\n\n\n\n\n\n\n# -&gt; try log transformation and recheck\nhist(log(data_train$sale_price))\n\n\n\n\n\n\n\nqqnorm(log(data_train$sale_price))\n\n\n\n\n\n\n\n\nThe distribution of sale_price appears to be much more normal after transforming. So let’s use log(sale_price) ~ &lt; set of Xs &gt; as our model.\nThis is technically a regression with a transformed response, so it is a generalized linear model (GLM). Once we make the transformation, everything works as usual, only the units and interpretations change.\n\n\nVariable selection\nNow try to find some \\(X\\) variables to use in a model.\n\n# &lt; your code &gt;\n\n\n\nPrediction\nOnce you have some candidate models, see which one is the best by calculating the \\(RMSE\\).\n\n# &lt; your code &gt;",
    "crumbs": [
      "After Test 3",
      "Lecture 8 -- Regression"
    ]
  },
  {
    "objectID": "review-final.html",
    "href": "review-final.html",
    "title": "Review – Final",
    "section": "",
    "text": "Study Guide\nFILES: study-guide-final.tex and style-assessments.sty\nNote this test uses the same distribution table as in Study Guide.",
    "crumbs": [
      "After Test 3",
      "Review -- Final"
    ]
  },
  {
    "objectID": "review-final.html#review-problems",
    "href": "review-final.html#review-problems",
    "title": "Review – Final",
    "section": "Review Problems",
    "text": "Review Problems\nFILES: BLANK review-part-1.pdf and review-part-1.tex\n\n\nFILES: BLANK review-part-2.pdf and review-part-2.tex\n\n\nFILES: BLANK review-part-3.pdf and review-part-3.tex",
    "crumbs": [
      "After Test 3",
      "Review -- Final"
    ]
  }
]