\documentclass{article}
\usepackage{style-assessments}

% define macros (/shortcuts)
\newcommand{\bu}[1]{\textbf{\ul{#1}}}				% shortcut bold and underline text in one command
\newcommand{\ind}{\perp \!\!\! \perp}			% define independence symbol (it basically makes two orthogonal symbols very close to each other. The number of \! controls the space between each of the orthogonal symbols)
\newcommand{\e}{\mathrm{e}}		% shortcut for non-italic e in math mode
\newcommand{\gam}[1]{\Gamma(#1)}		% shortcut for gamma function (variable)
\newcommand{\Beta}[2]{B(#1, #2)}		% shortcut for beta function B(variable1, variable 2)
\newcommand{\integral}[4]{\displaystyle \int_{#1}^{#2} #3 \,\mathrm{d} #4}		% shortcut for large integral with limits and appending formatted dx (variable x)
\newcommand{\follow}[1]{\sim \text{#1}\,}		% shortcut for ~ 'Named dist ' in normal font with space before parameters would go
\newcommand{\followsp}[2]{\overset{#1}\sim \text{#2}\,}		% (followsp is short for 'follow special') shortcut that can be used for iid or ind ~ 'Named dist ' in normal font with space before parameters would go
\newcommand{\vecn}[2]{#1_1, \ldots, #1_{#2}}	% define vector (without parentheses, so when writing out in like a definition) of the form X_1, ..., X_n, where X and n are variable. NOTE: to call use $\vecn{X}{k}$
\newcommand{\chisq}{\raisebox{2pt}{$\chi^2$}}		% shortcut for chi-square distribution (better formatted chi letter in math mode with square added)
\newcommand{\order}[2]{#1_{(#2)}}		% shortcut for order stat notation X_{(j)} (random variable and subscript variable)
\newcommand{\convp}[2]{#1 \overset{p} \to #2}		% shortcut for Xn converges to X in probability (with arrow notation)


%Confusing what to put for values of r for chi-square def (can have only integer values when used in gamma (for some reason??), but df must be whole numbers (practically), but in r can have decimal df)

% NOTE: F dist is not included on dist table at end
% AND, the derivations of t (Z / sqrt chi-square / r)) are not included (referenced to study guide above), which means they don't have that for the exam

\begin{document}

\begin{center}
{\Huge MATH 321: Test 2 Study Guide}

\end{center}

\bigskip\bigskip

{\large \bu{Lecture 1 -- Random Samples and Common Statistics}} (5.5)\bigskip

Basic concepts of random samples
\begin{itemize}
    \item Random sample definition: $\vecn{X}{n}$ are a random sample of size $n$ from the population $f(x)$ if they are $iid$ random variables.
    \item Statistic (estimator) definition: The random variable / vector for any function of a random sample $Y = T(\vecn{X}{n})$ is called a statistic, and it's distribution is called a sampling distribution.
\end{itemize}\bigskip

Sample mean and variance
\begin{itemize}
    \item Definitions
    \begin{itemize}
        \item Sample mean: The arithmetic average of the values in a random sample
        \item[] $\displaystyle \bar{X} = \frac{X_1 + \cdots + X_n}{n} = \frac{1}{n} \sum_{i = 1}^n X_i$
        \item Sample variance: The statistic defined by $\displaystyle S^2 = \frac{1}{n - 1} \sum_{i = 1}^n (X_i - \bar{X})^2$
        \item Sample standard deviation: The statistic defined by $S = \sqrt{S^2}$
    \end{itemize}
    \item Theorem: Let $\vecn{X}{n}$ be a random sample of size $n$ from a population with mean $\mu$ and variance $\sigma^2 < \infty$. Then
    \item[] (a) $\mu_{\bar{X}} = E(\bar{X}) = \mu$ \hspace{20pt} (b) $\sigma_{\bar{X}}^2 = V(\bar{X}) = \frac{\sigma^2}{n}$ \hspace{20pt} (c) $E(S^2) = \sigma^2$
    \item Sampling distribution of $\bar{X}$ from random sample $\vecn{X}{n}$
    \item[] Theorem: Mgf of the sample mean is $M_{\bar{X}}(t) = [M_X(t / n)]^n$
\end{itemize}\bigskip
     
Sampling from the normal distribution
\begin{itemize}
    \item Let $\vecn{X}{n}$ be a random sample of size $n$ from a $\text{Normal }(\mu, \sigma^2)$ distribution. Then
    \item[] (a) $\bar{X} \ind S^2$ \hspace{20pt} (b) $\bar{X} \follow{Normal}(\mu, \frac{\sigma^2}{n})$ \hspace{20pt} (c) $\displaystyle \frac{(n-1)}{\sigma^2} S^2  \follow{\chisq}(n-1)$
\end{itemize}\bigskip

Chi-square random variables
\begin{itemize}
    \item If $Z \follow{Normal}(0, 1)$, then $Z^2 \follow{\chisq}(1)$ $\rightarrow$ $\displaystyle \big(\frac{\bar{X} - \mu}{\sigma}\big)^2 = Z^2 \follow{\chisq}(1)$
    \item Additive $df$: If $\vecn{X}{n}$ are mutually independent and $X_i \follow{\chisq}(r_i)$ for $i = 1, \ldots, n$, then \\ $Y = X_1 + \cdots + X_n \follow{\chisq}(r_1 + \cdots + r_n)$
    \item Result / extension of this: If $\vecn{X}{n}$ are mutually independent random variables with \\ $X_i \follow{Normal}(\mu_i, \sigma_i)$ for $i = 1, \ldots, n$, then
    \item[] $\displaystyle \sum_{i=1}^n \big(\frac{\bar{X} - \mu}{\sigma}\big)^2 = \sum_{i=1}^n Z^2 \follow{\chisq}(n)$
\end{itemize}\bigskip

\newpage

$t$ distribution
\begin{itemize}
    \item Definition: Let $\vecn{X}{n}$ be a random sample from a $N(\mu, \sigma^2)$ population. Then $\displaystyle \frac{\bar{X} - \mu}{S / \sqrt{n}} \follow{$t$}_{n-1}$
    \item Derivation: $\displaystyle \frac{Z}{\sqrt{\chisq_r / r}} \follow{$t$}_r$
\end{itemize}\bigskip

$F$ distribution
\begin{itemize}
    \item Definition: Let $\vecn{X}{n}$ be a random sample from a $N(\mu_X, \sigma^2_X)$ population, and let $\vecn{Y}{m}$ be a random sample from an independent $N(\mu_Y, \sigma^2_Y)$ population. If
    \item[] $\displaystyle W = \frac{S^2_X / S^2_Y}{\sigma^2_X / \sigma^2_Y} \hspace{20pt} \text{then} \hspace{20pt} W \follow{$F$}(n-1, m-1)$. In general, $W \follow{$F$}(r_1, r_2)$.
    \item Derivation: $\displaystyle \frac{\chisq_{r_1} / r_1}{\chisq_{r_2} / r_2} \follow{$F$}(r_1, r_2)$
    \item Relationship to other distributions theorem
    \item[] (a) If $X \follow{$F$}(r_1,r_2)$ then $1/X \follow{$F$}(r_2,r_1)$ \hspace{20pt} (b) If $X \follow{$t$}_r$ then $X^2 \follow{$F$}(1,r)$
\end{itemize}\bigskip

\vspace{50pt}

{\large \bu{Lecture 2 -- Order Statistics}} (6.3)\bigskip

Order statistics definition and distributions
\begin{itemize}
    \item Definition: The order statistics are random variables that satisfy $\order{X}{1} \le \cdots \le X_{(n)}$. In particular
    \begin{align*}
    \order{X}{1} &= \min_{1 \le i \le n} X_i,\\
    \order{X}{2} &= \text{second smallest } X_i\\
    &\vdots\\
    \order{X}{n} &= \max_{1 \le i \le n} X_i.
    \end{align*}
    \item Distribution theorems
    \begin{itemize}
        \item Cdf: 
        \begin{align*}
        F_{\order{X}{j}}(x) &= P(\order{X}{j} \le x) = \sum_{k = j}^n {n \choose k} [F_{X}(x)]^k [1- F_X(x)]^{n - k}\\
        &= P(Y \le j), \quad \text{where} \quad Y \follow{Binomial}(n, p = P(X \le x) = F_X(x))
        \end{align*}
        \item Pdf:
        \begin{align*}
        f_{\order{X}{j}}(x) &= \frac{n!}{(j - 1)!(n - j)!} \, [F_X(x)]^{j - 1} \, f_X(x) \, [1 - F_X(x)]^{n - j}\\
        &= [\text{multinomial coefficient}] \times [j-1 \text{ RVs } \le x] \times [1 \text{ RV} \approx x] \times [n-j \text{ RVs } > x] 
        \end{align*}
    \end{itemize}
    \item $f_{\order{X}{j}}(x) = F'_{\order{X}{j}}(x)$
    \item Extreme order stats\medskip\\
    \begin{tabular}{l l l l }
    Min \hspace{10pt} & $\rightarrow$ \hspace{10pt} & $F_{\order{X}{1}}(x) = 1 - [1 - F_X(x)]^n$; & $f_{\order{X}{1}}(x) = n f_X(x) [1 - F_X(x)]^{n-1}$\\\\
    Max \hspace{10pt} & $\rightarrow$ \hspace{10pt} & $F_{\order{X}{n}}(x) = [F_X(x)]^n$; & $f_{\order{X}{n}}(x) = n [F_X(x)]^{n-1} f_X(x)$
    \end{tabular}
\end{itemize}\bigskip

Specific order statistics and functions of order statistics
\begin{itemize}
    \item Sample median $M$
    \[M =
    \left\{
    \begin{array}{ll}
        \order{X}{\frac{n+1}{2}} & \text{if $n$ is odd}\\
        \big[\order{X}{\frac{n}{2}} + \order{X}{\frac{n}{2} + 1}\big] / 2 & \text{if $n$ is even}\\
    \end{array}
    \right.\]
    \item Sample range, $R = \order{X}{n} - \order{X}{1} = max(\vecn{X}{n}) - min(\vecn{X}{n})$
    \item $IQR = Q_3 - Q_1$
    \item $\displaystyle \text{Midrange} = \frac{\order{X}{1} + \order{X}{n}}{2}$
\end{itemize}\bigskip

Order statistics as estimators of population percentiles
\begin{itemize}
    \item Expected value of the ``position'' of order statistics theorem
    \item[] If $\order{X}{1}, \ldots, \order{X}{n}$ are order statistics, then $\displaystyle E[F_X(\order{X}{j})] = \frac{j}{n+1}, \quad j = 1, \ldots, n$
    \item[] Can use $\order{X}{j}$ as an estimator of $x_p$, where $p = j / (n+1)$.
\end{itemize}\bigskip

q--q plots
\begin{itemize}
    \item Expected probability between two adjacent order statistics theorem:
    \item[] $E[F_X(\order{X}{j}) - F_X(\order{X}{j-1})] = \frac{1}{n+1}$; \hspace{20pt}$E[F_X(\order{X}{1})] = \frac{1}{n+1}$; \hspace{20pt} $E[1 - F_X(\order{X}{n})] = \frac{1}{n+1}$
    \item q--q plot definition: Let $\order{x}{1}, \dots, \order{x}{n}$ be the observed sample order statistics and $x_{\frac{1}{n+1}}, \ldots, x_{\frac{n}{n+1}}$ be the percentiles from some particular distribution. A q--q plot is a plot of the points
    \item[] $(\order{x}{1}, x_{\frac{1}{n+1}}) \,\, , \,\, \ldots \,\, , \,\, (\order{x}{n}, x_{\frac{n}{n+1}})$
    \item Interpretation of a q--q plot
    \item[] Good model $\rightarrow$ Follows $y = x$ line.
    \item[] Bad model $\rightarrow$ Strong deviation from this line.
    \item q--q plots for the normal distribution.
    \item[] If plot $(\order{x}{1}, z_{\frac{1}{n+1}}) \,\, , \,\, \ldots \,\, , \,\, (\order{x}{n}, z_{\frac{n}{n+1}})$, then $\frac{1}{\text{slope}} \approx \sigma$
\end{itemize}\bigskip

\newpage

{\large \bu{Lecture 3 -- Exploratory Data Analysis}} (6.2)\bigskip

Univariate EDA
\begin{itemize}
    \item Descriptive statistics: Goal is to summarize a whole dataset wtih a single or few measures
    \begin{itemize}
        \item Sample mean $\displaystyle \bar{x} = \frac{1}{n} \sum_{i = 1}^n x_i$
        \item Sample variance: $\displaystyle s^2 = \frac{1}{n - 1} \sum_{i = 1}^n (x_i - \bar{x})^2 = \frac{n}{n - 1} v$
        \item Data (or population) variance: $\displaystyle v = \frac{1}{n} \sum_{i = 1}^n (x_i - \bar{x})^2$
    \end{itemize}
    \item Displaying data
    \begin{itemize}
        \item Frequency tables: Data is grouped into intervals of equal length (bins)
        \item[] Freq = count of observations in each; Relative freq = proportion of observations in each bin = Freq / $n$
        \item Histograms: Shape and summary stats\smallskip\\
        \begin{tabular}{rlllll}
            Right-skewed: & mean & > & median & > & mode \\
            Symmetric: & mean & $\approx$ & median & $\approx$ & mode \\
            Left-skewed: & mean & < & median & < & mode \\
        \end{tabular}
        \item Density histograms: Estimate underlying pdf
        \item[] For constants $c_1$ and $c_2$, $P(c_1 \le X < c_2) \approx  \frac{\text{Freq}}{n}$ on $(c_1, c_2]$
        \item[] Height of bar $h(x) = \frac{\text{Freq}}{n (c_2 - c_1)}$
    \end{itemize}
    \item Empirical rule:
    \begin{enumerate}
        \item $\approx$ 68\% of data is in $(\bar{x} - s, \bar{x} + s)$.
        \item $\approx$ 95\% of data is in $(\bar{x} - 2s, \bar{x} + 2s)$.
        \item $\approx$ 99.7\% of data is in $(\bar{x} - 3s, \bar{x} + 3s)$.
    \end{enumerate}
    \item Order statistics:
    \begin{itemize}
        \item 5 number summary
        \begin{enumerate}
            \item Sample minimum $\order{x}{1}$
            \item Lower quartile or First (lower) quartile $q_1 = \hat{x}_{0.25}$
            \item Median (second quartile) $m = \hat{x}_{0.5}$
            \item Third (upper) quartile $q_3 = \hat{x}_{0.75}$
            \item Sample maximum $\order{x}{n}$
        \end{enumerate}
        \item Other statistics
        \item[] Sample range, $R = \order{x}{n} - \order{x}{1}$; \hspace{10pt} $IQR = q_3 - q_1$; \hspace{10pt} $\displaystyle \text{Midrange} = \frac{\order{x}{1} + \order{x}{n}}{2}$
        \item Boxplots: Visual of 5-number summary, also used to identify outliers
        \item[] Suspected outlier $\rightarrow$ Below $q_1 - 1.5 \times IQR$ (low outlier) or above $q_3 +1.5 \times IQR$
        \item[] Outlier $\rightarrow$ Below $q_1 - 3 \times IQR$ (low outlier) or above $q_3 +3 \times IQR$
        \item Another way to identify outliers: Three-sigma rule
        \item[] Outlier if outside $(\bar{x} - 3s, \bar{x} + 3s)$
        \item q--q plots can be used to test potential models
    \end{itemize}
\end{itemize}\bigskip

Bivariate EDA
\begin{itemize}
    \item Goal: Examine pairwise relationships between variables
    \item Visualizing dependence: Scatterplots can be used to look for positive, negative or no association.
    \item Quantifying linear dependence:
    \item[] Sample correlation $\displaystyle r = \frac{1}{n - 1} \frac{\sum_{i = 1}^n (x_i - \bar{x})(y_i - \bar{y})}{s_x \, s_y}$
\end{itemize} 

\vspace{40pt}

{\large \bu{Lecture 4 -- Point Estimation}} (5.8 and 6.4)\bigskip

Point estimators
\begin{itemize}
    \item Definition: A point estimator is any function $\hat{\theta} = W(\vecn{X}{n})$ of a sample;\\ that is, any statistic is a point estimator
    \item An estimator is a random variable (a function of the sample); an estimate is the realized value of the random variable once data is collected
\end{itemize}\bigskip

Evaluate estimators
\begin{itemize}
    \item Unbiased definition: Point estimator $\hat{\theta}$ is unbiased if $E(\hat{\theta}) = \theta$; otherwise it is biased.
    \item[] This tells us the mean of a statistic, regardless of $n$.
    \item Consistency definition: The property summarized by the WLLN that says if a sequence of the ``same'' sample quantity approaches a constant as $n \to \infty$, then it is consistent.
    \item[] In other words, ff a statistic is consistent, then as $n \to \infty$, there is no variation in what the statistic converges to; the entire distribution converges to a constant.
    \begin{itemize}
        \item Convergence in probability
        \begin{itemize}
            \item Definition: A sequence of random variables, $Y_1, Y_2, \dots$, converges in probability to a random variable $Y$ if, for every $\epsilon > 0$,
            \[\lim_{n \to \infty} P(\lvert Y_n - Y \rvert \ge \epsilon) = 0 \hspace{20pt} \text{or, equivalently,} \hspace{20pt} \lim_{n \to \infty} P(\lvert Y_n - Y \rvert < \epsilon) = 1\]
            \item Notation: $\convp{Y_n}{Y}$
        \end{itemize}
        \item (Weak) Law of Large Numbers (WLLN)
        \begin{itemize}
            \item WLLN theorem: Let $X_1 ,X_2, \ldots$ be $iid$ random variable with $E(X_i) = \mu$ and \\ $V(X_i) = \sigma^2 < \infty$. Define $\displaystyle \bar{X}_n = \frac{1}{n} \sum_{i = 1}^n X_i$. Then for every $\epsilon > 0$,
            \[\lim_{n \to \infty} P(\lvert \bar{X}_n - \mu \rvert < \epsilon) = 1 \hspace{40pt} \lim_{n \to \infty} P(\lvert \bar{X}_n - \mu \rvert \ge \epsilon) = 0\] 
            \item[] that is, $\convp{\bar{X}}{\mu}$.
        \end{itemize}
    \end{itemize}
\end{itemize}\bigskip

Method of moments
\begin{itemize}
    \item Types of moments:
    \begin{itemize}
        \item $k^\text{th}$ (population) moment of the distribution (about the origin) = $\mu'_k = E(X^k)$
        \item The corresponding sample moment is the average = $\displaystyle m'_k = \frac{1}{n} \sum_{i = 1}^n {X^k_i}$
    \end{itemize}
    \item Official statement of Method of Moments:
    \item[] Choose as estimates those values of the parameters that are solutions of the equations $\mu'_k = m'_k$, for $k = 1, 2 \ldots, t$, where $t$ is the number of parameters to be estimated
    \item Steps to find MME
    \begin{enumerate}
        \item Write $E(X^k)$ as a function of the parameters of interest (may have to integrate)
        \item Then estimate the parameter of interest by equating the population moment with the sample moment and solving for the parameter
    \end{enumerate}
\end{itemize}\bigskip

Maximum Likelihood Estimation
\begin{itemize}
    \item Needed items:
    \begin{itemize}
        \item Parameter space: Set of all possible values for $\theta_1, \ldots, \theta_k$ in pdf (or pmf) $f(x \mid \theta_1, \ldots, \theta_k)$ 
        \item Likelihood function: $L(\boldsymbol{\theta} \mid \mathbf{x}) = f(\mathbf{x} \mid \boldsymbol{\theta}) = \displaystyle \prod_{i = 1}^n f(x_i \mid \boldsymbol{\theta})$
        \item[] Equivalent to the joint pdf or pmf of the data, just with different information considered known.
    \end{itemize}
    \item MLE definition: For each sample point $\mathbf{x}$, let $\hat{\theta}(\boldsymbol{x})$ be a parameter value at which $L(\theta \mid \mathbf{x})$ attains its maximum as a function of $\theta$, with $\mathbf{x}$ held fixed. A maximum likelihood estimator (MLE) of the parameter $\theta$ based on a sample $\mathbf{X}$ is $\hat{\theta}(\mathbf{X})$.
    \item Steps to find MLEs
    \begin{enumerate}
        \item Write the likelihood function (i.e. joint density function) and the log-likelihood,
        \[L(\theta \mid \mathbf{x}) = \prod_{i = 1}^n f(\mathbf{x} \mid \theta) \hspace{20pt} \rightarrow \hspace{20pt} \ell(\theta) = \ln[L(\theta \mid \mathbf{x})]\]
        \item Optimize the log-likelihood function by taking the derivatives with respect to the parameter of interest.
        \item[] Set to zero and solve for the parameter of interest.
        \[\ell'(\theta) = \frac{d}{d \theta} \ell(\theta) = 0 \hspace{20pt} \rightarrow \hspace{20pt} \hat{\theta} = \text{potential MLE}\]
        \item Verify that the global maximum of the log-likelihood function occurs at $\theta = \hat{\theta}$.
        \item[] Find the second derivative of the log-likelihood function, then plug in $\hat{\theta}$ and see if less than zero.
        \[\ell''(\theta) = \frac{d^2}{d \theta^2} \, \ell(\theta) \hspace{20pt} \rightarrow \hspace{20pt} \ell''(\hat{\theta}) \overset{?}< 0\]
        \item[] If so, then we have $\hat{\theta}_{MLE}$.
    \end{enumerate}\bigskip
    \item Finding MLEs for functions of parameters
    \item[] Invariance property of MLEs: If $\hat{\theta}$ is the MLE of $\theta$, then for any function $\tau(\theta)$, the MLE of $\tau(\theta)$ is $\tau(\hat\theta)$
\end{itemize}\bigskip

\newpage

{\large \bu{Distributions}}\bigskip

{\renewcommand{\arraystretch}{2}
\begin{tabular}{l l}
    \hline\hline
    \multicolumn{2}{l}{\hspace{150pt}\textbf{Discrete Distributions}}\\
    \hline\hline
    
    \multicolumn{2}{l}{\textbf{Discrete uniform}$\,(N_0, N_1)$} \\
    Pmf & $P(X = x \mid N_0, N_1) = \frac{1}{N_1 - N_0 + 1}; \quad\quad x = N_0, \ldots, N_1; \quad\quad N_0 \le N_1$ \\
    \Centerstack[l]{Mean and \\ Variance} & $E(X) = \frac{N_0 + N_1}{2}$, \quad\quad $V(X) = \frac{(N_1 - N_0 + 1)^2 -1}{12}$\\
    Mgf & $M_X(t) = \frac{1}{N_1 - N_0 + 1} \sum_{x = N_0}^{N_1} \e^{tx}$\\
    Notes & \\
        
    \hline
    \multicolumn{2}{l}{\textbf{Bernoulli}$\,(p)$} \\
    Pmf & $P(X = x \mid p) = p^x (1 - p)^{1 - x}; \quad\quad\mbox{$x = 0, 1$; \quad\quad 0 < p < 1}$ \\
    \Centerstack[l]{Mean and \\ Variance} & $E(X) = p$, \quad\quad $V(X) = p(1 - p) = pq$ \\
    Mgf & $M_X(t) = (1 - p) + p\e^t = q + p\e^t$\\
    Notes & Special case of binomial with $n = 1$.\\
    
    \hline
    \multicolumn{2}{l}{\textbf{Binomial}$\,(n, p)$} \\
    Pmf & $P(X = x \mid n, p) = {n \choose x}\, p^x\, (1 - p)^{n - x}; \quad\quad x = 0, 1, \ldots, n; \quad\quad  0 < p < 1$ \\
    \Centerstack[l]{Mean and \\ Variance} & $E(X) = np$, \quad\quad $V(X) =  np(1 - p) = npq$ \\
    Mgf & $M_X(t) = (q + p\e^t)^n$\\
    Notes & Sum of \textit{iid} bernoulli RVs. \\
    \hline
    
    \multicolumn{2}{l}{\textbf{Geometric}$\,(p)$} \\
    Pmf & $P(X = x \mid p) = q^{x - 1} \, p; \quad\quad x = 1, 2, \ldots; \quad\quad 0 < p < 1$ \\
    Cdf & $F_X(x \mid p) = 1 - q^x$ \\
    \Centerstack[l]{Mean and \\ Variance} & $E(X) = \frac{1}{p}$, \quad\quad $V(X) = \frac{1 - p}{p^2} = \frac{q}{p^2}$ \\
    Mgf & $M_X(t) = \frac{p\e^t}{1 - q\e^t}; \quad\quad t < -\ln(q)$\\
    Notes & \Centerstack[l]{Special case of negative binomial with $r = 1$. \\ * See other geometric probabilities. \\ Alternate form $Y = X - 1$. \\ This distribution is \textit{memoryless}: $P(X > s \mid X > t) = P(X > s - t); \quad\quad s > t$.} \\
    \hline
\end{tabular}
}\bigskip

{\renewcommand{\arraystretch}{2}
\begin{tabular}{l l}
    \hline
    \multicolumn{2}{l}{\textbf{Negative binomial}$\,(r, p)$} \\
    Pmf & $P(X = x \mid r, p) = P(X = x \mid r, p) = {{x - 1} \choose {r - 1}} \, p^r \, q^{x - r}; \quad\quad x = r, r + 1, \ldots; \quad\quad 0 < p < 1$ \\
    \Centerstack[l]{Mean and \\ Variance} & $E(X) = \frac{r}{p}$, \quad\quad $V(X) = \frac{r(1 - p)}{p^2} = \frac{rq}{p^2}$ \\
    Mgf & $M_X(t) = \big[\frac{p\e^t}{1 - q\e^t}\big]^r; \quad\quad t < -\ln(q)$\\
    Notes & Sum of \textit{iid} geometric RVs.\\
    
    \hline
    \multicolumn{2}{l}{\textbf{Hypergeometric}$\,(N, M, K)$} \\
    Pmf & $P(X = x \mid r, p) = P(X = x \mid N, M, K) = \frac{{M \choose x}{{N - M} \choose {K - x}}}{{N \choose K}}; \quad\quad x = 0, 1, \ldots, \text{min}(M, K)$ \\
    \Centerstack[l]{Mean and \\ Variance} & $E(X) = K \big(\frac{M}{N}\big)$, \quad\quad $V(X) = K \big(\frac{M}{N}\big) \big(\frac{N - M}{N}\big) \big(\frac{N - K}{N - 1}\big)$ \\
    Mgf & \\
    Notes & \Centerstack[l]{If do not require $M \ge K$, ${\cal X} = \{\text{max}(0, K + M - N), \ldots, \text{min}(M, K)\}$, \\ mean and variance converge to that of binomial $(n = K, p = M / K)$ when $N \to \infty$.}\\

    \hline
    \multicolumn{2}{l}{\textbf{Poisson}$\,(\lambda)$} \\
    Pmf & $P(X = x \mid \lambda) = \frac{\e^{-\lambda} \lambda^x }{x!}, \quad\quad x = 0, 1, 2, \ldots; \quad\quad \lambda > 0$ \\
    \Centerstack[l]{Mean and \\ Variance} & $E(X) = \lambda$, \quad\quad $V(X) = \lambda$ \\
    Mgf & $M_X(t) = \e^{\lambda (\e^t - 1)}$\\
    Notes & If $X_i \followsp{\ind}{Poisson}(\lambda_i)$, then $\sum X_i  \sim \text{Poisson}(\lambda = \sum \lambda_i)$. \\
    \hline
\end{tabular}
}\vspace{100pt}

Other geometric probabilities
\begin{itemize}
    \item Let $X \sim \text{Geometric}(p)$.\bigskip\\
    {\renewcommand{\arraystretch}{1.3}
    \begin{tabular}{l l l}
    $P(X < \infty)$ & $=$ & $1$ \\
    $P(X > x)$ & $=$ & $q^x$\\
    $P(X \ge x)$ & $=$ & $q^{x - 1}$\\
    $P(a < X \le b)$ & $=$ & $q^a - q^b$\\
    $P(a \le X \le b)$ & $=$ & $q^{a - 1} - q^b$
    \end{tabular}
    }
\end{itemize}\vspace{100pt}

{\renewcommand{\arraystretch}{2}
\begin{tabular}{l l}
    \hline\hline
    \multicolumn{2}{l}{\hspace{150pt}\textbf{Continuous Distributions}}\\
    \hline\hline

    \multicolumn{2}{l}{\textbf{Continuous uniform}$\,(a, b)$} \\
    Pdf & $f(x \mid a, b) = \frac{1}{b - a}, \quad\quad a \le x \le b; \quad\quad a, b \in \mathbb{R}, \quad\quad a \le b$ \\
    Cdf & $F(x) = \frac{x - a}{b - a} \quad\quad a \le x \le b$ \\
    Survival & $S(t) = \frac{b - t}{b - a} \quad\quad a \le t \le b$ \quad\quad if $T \follow{Uniform}(a, b)$\\
    \Centerstack[l]{Mean and \\ Variance} & $E(X) = \frac{a + b}{2}$; \quad\quad $V(X) = \frac{(b - a)^2}{12}$\\
    Mgf & $M_X(t) = \frac{\e^{tb} - \e^{ta}}{t(b - a)} \quad\quad t \ne 0$\\
    Notes & \\
    
    \hline
    \multicolumn{2}{l}{\textbf{Exponential}$\,(\lambda)$} \\
    Pdf & $f(t \mid \lambda) = \lambda \e^{-\lambda t}, \quad\quad t \ge 0; \quad\quad \lambda > 0$ \\
    Cdf & $F(t) = 1 - \e^{-\lambda t} \quad\quad t \ge 0$ \\
    Survival & $S(t) = \e^{-\lambda t} \quad\quad t \ge 0$ \\
    \Centerstack[l]{Mean and \\ Variance} & $E(X) = \frac{1}{\lambda}$; \quad\quad $V(X) = \frac{1}{\lambda^2}$\\
    Mgf & $M_X(t) = \frac{\beta}{\beta - t} \quad\quad t < \beta$; \quad\quad if $T \follow{Exp}(\beta)$\\
    Notes & \Centerstack[l]{Special case of gamma with $\alpha = 1, \beta$. \\ This distribution is \textit{memoryless}: $P(T > a + b \mid T > a) = P(T > b); \quad\quad a,b > 0$. \\ Alternate parameterization is with scale $\theta = 1 / \lambda$.} \\
    
    \hline
    \multicolumn{2}{l}{\textbf{Gamma}$\,(\alpha, \beta)$} \\
    Pdf & $f(x \mid \alpha, \beta) = \frac{\beta^\alpha}{\gam{\alpha}} \, x^{\alpha - 1} \, \e^{-\beta x}, \quad\quad x \ge 0; \quad\quad \alpha, \beta > 0$ \\
    Cdf & N/A \\
    \Centerstack[l]{Mean and \\ Variance} & $E(X) = \frac{\alpha}{\beta}$ \quad\quad $V(X) = \frac{\alpha}{\beta^2}$\\
    Mgf & $M_X(t) = \big(\frac{\beta}{\beta - t}\big)^\alpha \quad\quad t < \beta$\\
    Notes & \Centerstack[l]{Sum of \textit{iid} exponential RVs. \\ A special case is exponential $(\alpha = 1, \beta)$. \\ Alternate parameterization is with scale $\theta = 1 / \beta$.}\\

    \hline
    \multicolumn{2}{l}{\textbf{Normal}$\,(\mu, \sigma^2)$} \\
    Pdf & $f(x \mid \mu, \sigma^2) = \frac{1}{\sqrt{2 \pi \sigma^2}} \, \exp\big[-\frac{(x - \mu)^2}{2\sigma^2}\big], \quad\quad -\infty < x < \infty; \quad\quad -\infty < \mu < \infty, \quad \sigma > 0$ \\
    Cdf & N/A \\
    \Centerstack[l]{Mean and \\ Variance} & $E(X) = \mu$, \quad\quad $V(X) = \sigma^2$\\
    Mgf & $M_X(t) = \exp\big[\mu t + \frac{\sigma^2 t^2}{2}\big]$\\
    Notes & Special case: Standard normal $Z \follow{Normal}(\mu = 0, \sigma^2 = 1)$.\\
    \hline
    \end{tabular}
}

\newpage

{\renewcommand{\arraystretch}{2}
\begin{tabular}{l l}
    \hline
    \multicolumn{2}{l}{\textbf{Lognormal}$\,(\mu, \sigma^2)$} \\
    Pdf & $f(y \mid \mu, \sigma^2) = \frac{1}{y \sqrt{2 \pi \sigma^2}} \, \exp\big[-\frac{(\ln(y) - \mu)^2}{2\sigma^2}\big]; \quad\quad  y \ge 0; \quad\quad -\infty < \mu < \infty; \quad\quad \sigma > 0$ \\
    \Centerstack[l]{Mean and \\ Variance} & $E(Y) = \e^{\mu + \frac{\sigma^2}{2}}$, \quad\quad $V(Y) = \e^{2\mu + \sigma^2} (\e^{\sigma^2} - 1)$ \\
    Mgf & \\
    Notes &  \Centerstack[l]{If $Y \follow{Lognormal} \Longrightarrow \ln(Y) \follow{Normal}(\mu, \sigma^2)$;\\equivalently, if $X \follow{Normal}(\mu, \sigma^2)$ and $Y = \e^X \Longrightarrow Y \follow{Lognormal}$.\\$\mu$ and $\sigma^2$ represent the mean and variance of the normal random variable $X$ which appears in the exponent.}\\
    
    \hline
    \multicolumn{2}{l}{\textbf{Beta}$\,(\alpha, \beta)$} \\
    Pdf & $f(x \mid \alpha, \beta) = \frac{1}{\Beta{\alpha}{\beta}} \, x^{\alpha-1} (1 - x)^{\beta-1}; \quad\quad  0  \le x \le 1; \quad\quad \alpha, \beta > 0$ \\
    \Centerstack[l]{Mean and \\ Variance} & $E(X) = \frac{\alpha}{\alpha + \beta}$, \quad\quad $V(X) = \frac{\alpha \beta}{(\alpha + \beta)^2 (\alpha + \beta + 1)}$ \\
    Mgf & \\
    Notes & \Centerstack[l]{$\Beta{\alpha}{\beta} = \integral{0}{1}{x^{\alpha-1} (1 - x)^{\beta-1}}{x} = \frac{\gam{\alpha}\gam{\beta}}{\gam{\alpha + \beta}}$}\\

    \hline
    \multicolumn{2}{l}{\textbf{Chi-square, $\boldsymbol{\chisq}$}$\,(r)$} \\
    Pdf & $f(x \mid r) = \frac{1}{\gam{\frac{r}{2}} 2^{r/2}} \, x^{\frac{r}{2} - 1} \, \e^{-\frac{x}{2}}, \quad\quad x \ge 0; \quad\quad r = 0.5, 1, 1.5, 2, \ldots$ \\%Confusing what to put for values of r
    Cdf & N/A \\
    \Centerstack[l]{Mean and \\ Variance} & $E(X) = r$, \quad\quad $V(X) = 2r$\\
    Mgf & $M_X(t) = \big(\frac{\theta}{\theta - 2t}\big)^{r/2} \quad\quad t < 1/2$\\
    Notes & Special case of (scale) gamma with $\alpha = r/2, \theta = 2$.\\
    \hline
    
    \hline
    \multicolumn{2}{l}{$\boldsymbol{t}\,(r)$} \\
    Pdf & $f(t \mid r) = f_T(t) = \frac{\gam{\frac{r + 1}{2}}}{\frac{1}{\sqrt{r \pi}} \gam{\frac{r}{2}}} \big(\frac{1}{(1 + t^2 / r)^{(r + 1) / 2}}\big), \quad\quad -\infty < t < \infty$\\
    Cdf & N/A \\
    \Centerstack[l]{Mean and \\ Variance} & $E(T) = 0$ \quad if $r > 1$, \quad\quad $V(X) = \frac{r}{r - 2}$ \quad if $r > 2$\\
    Mgf & N/A\\
    Notes & See derivation notes above.\\
    \hline
    
    \hline
    \multicolumn{2}{l}{$\boldsymbol{F}\,(r_1, r_2)$} \\
    Notes & See derivation notes above.\\
    \hline
\end{tabular}
}

\end{document}