\documentclass{article}
\usepackage{style-notes}

\newcounter{lecnum} 	% define counter for lecture number
\renewcommand{\thepage}{\thelecnum-\arabic{page}}	% define how page number is displayed (< lecture number > - < page number >)
% define lecture header and page numbers
% NOTE: to call use \lecture{< Lecture # >, < Lecture name >, < Chapter # >, < Chapter name >, < Section #s >}
\newcommand{\lecture}[5]{

    % define headers for first page
    \thispagestyle{empty} % removes page number from page where call is made

    \setcounter{lecnum}{#1}		% set lecture counter to argument specified

    % define header box
    \begin{center}
    \framebox{
      \vbox{\vspace{2mm}
    \hbox to 6.28in {\textbf{MATH 321: Mathematical Statistics} \hfill}
       \vspace{4mm}
       \hbox to 6.28in {{\hfill \Large{Lecture #1: #2} \hfill}}
       \vspace{2mm}
       \hbox to 6.28in {\hfill Chapter #3: #4 \small{(#5)}}
      \vspace{2mm}}
    }
    \end{center}
    \vspace{4mm}
    
    % define headers for subsequent pages
    \fancyhead[LE]{\textit{#2} \hfill \thepage} 		% set left header for even pages
    \fancyhead[RO]{\hfill \thepage}		% set right header for odd pages

}



% define macros (/shortcuts)
\newcommand{\bu}[1]{\textbf{\ul{#1}}}				% shortcut bold and underline text in one command
\newcommand{\blankul}[1]{\rule[-1.5mm]{#1}{0.15mm}}	% shortcut for blank underline, where the only option needed to specify is length (# and units (cm or mm, etc.)))
\newcommand{\integral}[4]{\displaystyle \int_{#1}^{#2} #3 \,\mathrm{d} #4}		% shortcut for large integral with limits and appending formatted dx (variable x)
\newcommand{\e}{\mathrm{e}}		% shortcut for non-italic e in math mode
\newcommand{\ind}{\perp \!\!\! \perp}			% define independence symbol (it basically makes two orthogonal symbols very close to each other. The number of \! controls the space between each of the orthogonal symbols)
\newcommand{\follow}[1]{\sim \text{#1}\,}		% shortcut for ~ 'Named dist ' in normal font with space before parameters would go
\newcommand{\cov}[1]{\mathrm{Cov}(#1)}		% shortcut for Cov(X,Y) with formatting for Cov
\newcommand{\corr}[1]{\mathrm{Corr}(#1)}		% shortcut for Corr(X,Y) with formatting for Corr



% NOTES on what didn't cover

% Independence

% multinomial distribution
% This is just an application of independent events (Actex 10.5)
% book and Drew cover trinomial in 4.1
% Theory lecture 20, page 20-3 and 4

% additional notes for my knowledge
% proof of E(g(X)h(Y)) = times individual exp values from theory textbook (can prove this without bivariate transformation (see below)
% maybe the proof for the probability too, uses indicator function

% not sure if this is ever stated somewhere in theory notes
% if X and Y are independent, then rvs g(X) and h(Y) are also independent?? This is in theory lecture 17-2 (bivariate transformation) (theorem 4.3.5 in theory textbook)
% -> it is introduced later cause need bivariate transformation to prove it
%--> once proved, then obviously 'E(g(X)h(Y)) = times individual exp values' is evident

% Correlation

% rewriting cov(X,Y) = corr(X,Y)[sigma_x sigma_Y]
% then writing E(XY) = corr(X,Y)[sigma_x sigma_Y] + mu_x mu_y
% -> drew notes (4.2, in the properties of cov)

% correlation does not imply causation (drew remark 4.2.3)
% maybe come back to this for the regression applications of correlation

% all regression application
% e.g. finding LSRL

% bivariate normal
% tiny sections in actex (11.2.8) and end of Theory lecture 19)
% section 4.5 in textbook, has a lot of regression applications, so will come back to?


\begin{document}

\lecture{16}{Independence and the Correlation Coefficient}{4}{Bivariate Distributions}{4.1, 4.2, and 4.4}

\bu{Independence for random variables}\bigskip

Definition\bigskip
\begin{itemize}
    \item Events $A$ and $B$ are independent if and only if\vspace{40pt}
    \item The definition of independence for two discrete random variables relies on this multiplication rule.
    \item[] Applying the idea of independence between two events to random variables, we say that $X$ and $Y$ are independent random variable if and only if the events \blankul{1.5cm} and \blankul{1.5cm} are independent for all $x \in R$ and 
all $y \in R$.
    \item Definition: Let $(X,Y)$ be a bivariate random vector with joint pdf or pmf $f(x,y)$ and marginal pdfs or pmfs $f_X(x)$ and $f_Y(y)$. Then $X$ and $Y$ are called \textbf{independent random variables} if, for every $x \in \mathbb{R}$ and $y \in \mathbb{R}$,
    \[f(x,y) = f_X(x) \cdot f_Y(y)\]
    \item If $X$ and $Y$ are not independent, they are said to be \blankul{3cm}.
\end{itemize}\bigskip

Checking independence\bigskip
\begin{itemize}
    \item In general, if given $f(x,y)$ and checking to see if $X$ and $Y$ are independent (in notation: $X \ind Y$), we have to find $f_X(x)$ and $f_Y(y)$ and then multiply and check.
    \item Example: Define the joint pmf of $(X,Y)$ by\bigskip\\
    \begin{tabular}{| c || c | c |}
        \hline
        \backslashbox{$y$}{$x$} & 0 & 1\\
	\hline\hline
        0 & 1/4 & 1/4\\
        \hline
        1 & 1/4 & 0 \\
        \hline
        2 & 0 & 1/4 \\
        \hline
    \end{tabular}\bigskip
    \begin{enumerate}[(a)]
        \item Are $X$ and $Y$ independent?
        \item Are the events $\{X = 0\}$ and $\{Y = 0\}$ independent of each other?\vspace{30pt}
    \end{enumerate}
    \item Interpreting independent random variables.
    \begin{itemize}
        \item We just saw that the random variables can be \blankul{2.5cm} even when specific \blankul{2cm} are \blankul{2.5cm}.
        \item[] This goes back to the definition of independence. In order for the entire random variables to be independent $P(X = x, Y = y) = P(X = x) \cdot P(Y = y)$ \blankul{2cm} pairs $(x,y)$.
        \item When $X$ and $Y$ are independent, observing $Y = y$ does not alter the probability model for $X$. Similarly, observing $X = x$ does not alter the probability model for $Y$.
        \item[] Therefore, learning that $Y = y$ provides no information about $X$ and learning that $X = x$ provides no information about $Y$.
    \end{itemize}
    \item More examples: Determine if $X$ and $Y$ independent in each of the following scenarios.
    \begin{enumerate}
        \item \hspace{10pt}\\
        \begin{tabular}{l l}
            $f(x, y) = 1/2$ & for $0 \le x \le y \le 2$ \\\\
            $f_X(x) = (2 - x)/2$ & for $0 \le x \le 2$ \\\\
            $f_Y(y) = y/2$ & for $0 \le y \le 2$ \\\\
        \end{tabular}
        \item \hspace{10pt}\\
        \begin{tabular}{l l}
            $f(x, y) = 4xy$ & for $0 \le x \le 1, \, 0 \le y \le 1$ \\\\
            $f_X(x) = 2x$ & for $0 \le x \le 1$ \\\\
            $f_Y(y) = 2y$ & for $0 \le y \le 1$ \\\\
        \end{tabular}
    \end{enumerate}
    \item What causes difference between the above examples?\vspace{100pt}
    \item We just saw that the range of $(X,Y)$ plays a crucial role in determining when random variables are independent. We can also look at the functional form of the joint pmf / pdf when checking independence.
    \item \textbf{Independence theorem}: $X$ and $Y$ are independent random variables if and only if 
    \[f(x,y) = g(x) \cdot h(y), \quad\quad a \le x \le b, \, c \le y \le d\]\smallskip
    \item[] where $g(x)$ is a nonnegative function of $x$ alone and $h(y)$ is a nonnegative function of $y$ alone.
    \item[] Note that $g(x)$ and $h(y)$ do not themselves need to be density functions.
    \item Checking independence by inspection.
    \begin{itemize}
        \item Let $f(x, y) = 2x$ \quad for $0 \le x \le 1, \, 0 \le y \le 1$.
        \item Process:
        \begin{enumerate}
            \item Is the range rectangular?
            \item Find any $g(x)$ and $h(y)$ such that $f(x,y) = g(x) \cdot h(y)$
            \item[**] Conclusions:\bigskip\\
        \begin{tabular}{l l l l l l}
            Range & \hspace{80pt} & Range & \hspace{80pt} & Range & \hspace{80pt}\\
            Separable & \hspace{80pt} &  Separable & \hspace{80pt} & Separable & \hspace{80pt}\\
        \end{tabular}\bigskip
        \item[] Rectangular range is a \blankul{2.5cm}, but not \blankul{2.5cm} condition for independence.
        \end{enumerate}
    \end{itemize}
    \item More examples: Determine if $X$ and $Y$ independent in each of the following scenarios.
    \begin{enumerate}\setcounter{enumi}{2}
        \item If the joint pdf is $\e^{xy}$ with a rectangle range.\vspace{20pt}
        \item If the joint pdf is $\e^{x+y}$ with a rectangle range.\vspace{20pt}
        \item If the joint pdf is $\log(x+y)$ with a rectangle range.\vspace{20pt}
        \item Let $(X,Y)$ be the numbers on die 1 and die 2, respectively, when a pair of fair 6-sided dice are thrown.
        \begin{figure}[H]
            \includegraphics[scale=0.45]{{"test-1/roll-dice-range"}.png}
        \end{figure}
        \item[] In the discrete case, ``rectangular`` means that the range of $(X,Y)$ must equal the product set (aka cartesian product) of the individual ranges of $X$ and $Y$: $\cal{X} \times \cal{Y} = (\cal{X}, \cal{Y})$
        \begin{figure}[H]
            \centering\includegraphics[scale=0.5]{{"test-1/x-y-range"}.png}
        \end{figure}
        \item[] In other words, need \blankul{3.5cm} $f(x,y) > 0$ for all possible $(x,y)$ combos.
        \item A fair coin is tossed. If heads is tossed then one fair 6-sided die is thrown and if tails is tossed two fair 6-sided dice are thrown. Let $X = 1$ for heads and $X = 2$ for tails and let $Y$ be the total number of dots on the dice.
        \begin{figure}[H]
            \includegraphics[scale=0.4]{{"test-1/coin-roll-dice-range"}.png}
        \end{figure}
        \item Define the joint pmf of $(X,Y)$ by\bigskip\\
        \begin{tabular}{| c || c | c | c |}
            \hline
            \backslashbox{$y$}{$x$} & -1 & 0 & 1\\
            \hline\hline
            -1 & 1/18 & 1/9 & 1/6 \\
            \hline
            0 & 1/9 & 0 & 1/6 \\
            \hline
            1 & 1/6 & 1/9 & 1/9 \\
            \hline
        \end{tabular}\bigskip
        \item[] NOTE: If have pmf table (and not equations) and have rectangular range  $\Longrightarrow$ Need to check $f(x,y) = f_X(x) \cdot f_Y(y)$ for ALL pairs.
        \item Let $\displaystyle f(x,y) = \frac{x + y}{21}$ \quad for $x = 1, 2, 3$ and $y = 1, 2$.\vspace{90pt}
    \end{enumerate}
\end{itemize}\bigskip

Conditional distributions and independence\bigskip
\begin{itemize}
    \item Recall if events $A$ and $B$ are independent, then $P(A \mid B) = P(A)$ and \\$P(B \mid A) = P(B)$. Let's see if this holds for distributions as well.
    \item Example: An analyst is studying traffic accidents in two adjacent towns. The random variables $S$ and $T$ represents the waiting time between accidents in towns $X$ and $Y$, respectively. The joint probability function for $S$ and $T$ is given by:
    \item[] $f(s,t) =  \e^{-(s + t)}$ \quad for $s \ge 0$ and $t \ge 0$.
    \begin{enumerate}[(a)]
        \item Find the marginal distributions $f_S(s)$ and $f_T(t)$.\vspace{70pt}
        \item Find the conditional distributions $f(s \mid t)$ and $f(t \mid s)$.\vspace{90pt}
    \end{enumerate}
    \item Theorem: If $X$ and $Y$ are independent,
    \[f(x \mid y) = f_X(x) \hspace{25pt} \text{and} \hspace{25pt} f(y \mid x) = f_Y(y)\]
    \item Proof:\vspace{60pt}
\end{itemize}\bigskip

Using independence\bigskip
\begin{itemize}
    \item We can always find the marginal distributions from the joint distribution, but the converse is not always true (so the joint distribution has more information).
    \item But when $X$ and $Y$ are\blankul{2.5cm}, the joint distribution and marginal distributions contain the equal amount of information about $X$ and $Y$.
    \item[] That means we can also find the \blankul{2cm} distribution from \blankul{2cm} distributions.
    \item Example: Suppose that $X \follow{Exp}(\lambda = 1)$ and $Y \follow{Uniform}(0,1)$ and $X \ind Y$.
    \item[] Find the joint pdf of $X$ and $Y$.\vspace{110pt}
    \item Theorem: Let $X$ and $Y$ be independent random variables.
    \begin{enumerate}[(a)]
         \item For any $A \subset \mathbb{R}$ and $B\subset \mathbb{R}$, 
         \[P(X \in A,Y \in B) = P(X \in A) \cdot P(Y \in B)\]
         \item[] That is, the events $\{X \in A\}$ and $\{Y \in B\}$ are independent events.\bigskip 
        \item Let $g(x)$ be a function only of $x$ and $h(y)$ be a function only of $y$. Then
        \[E[g(X) \cdot h(Y)] = E[g(X)] \cdot E[h(Y)]\]
        \item[] Proof for $E(XY)$ for the discrete case:\vspace{100pt}
    \end{enumerate}

    \item When applying this theorem, a bivariate question reduces to a univariate question, making it a lot simpler.
    \item[] Example: Let $X$ and $Y$ be independent exponential ($\lambda = 1)$ random variables.
    \begin{enumerate}
        \item Find $P(X \ge 4, Y < 3)$.\vspace{80pt}
        \item Find $E(X^2Y)$ and $E(X + Y)$.\vspace{80pt}
    \end{enumerate}
\end{itemize}\bigskip


\bu{Covariance}\bigskip
 
Introduction\bigskip
\begin{itemize}
    \item One of main purposes of studying bivariate random vectors is to study the dependence between two random variables.
    \item[] Recall that the advantage of using the joint pmf / pdf over the respective marginal distributions is that it usually contains additional information about interaction between the two random variables.
    \item[] We can use this joint distribution to check how much two random variables change together.\bigskip
    \item More specifically, covariance is how we will study this dependence. Covariance is a special expected value with two very useful applications.
    \begin{enumerate}
        \item Finding the variance of $X + Y$.\vspace{30pt}
        \item Quantify linear dependence.
    \end{enumerate}\bigskip
    \item There are two different scenarios that we can calculate covariance for: on a sample of data and on a probability distribution.
    \item[] Covariance is easier to conceptualize if we have a sample of data. So we will start with this. But technically we are still in a probability context, so then we will shift to working with distributions (i.e. population information) rather than data points.
\end{itemize}\bigskip

\newpage%so picture is on same page of explanations

Visualizing dependence\bigskip
\begin{itemize}
    \begin{figure}[H]
            \centering\includegraphics[scale=0.4]{{"test-1/correlation-types"}.png}
    \end{figure}
    \item In the first plot above:
    \begin{itemize}
        \item Large values ($> \mu$) of $X$ mainly correspond with \blankul{1.5cm} values of $Y$.
        \item[] Small values ($< \mu$) of $X$ mainly correspond with \blankul{1.5cm} values of $Y$.
        \item In this case, the two random variables are \blankul{2.25cm} dependent / correlated.
        \item[] In statistics, correlated = linear relationship.
    \end{itemize}\vspace{30pt}
    \item In the second plot above:
    \begin{itemize}
        \item Large values of $X$ mainly correspond with \blankul{1.5cm} values of $Y$.
        \item[] Small values of $X$ mainly correspond with \blankul{1.5cm} values of $Y$.
        \item In this case, the two random variables are \blankul{2.25cm} dependent / correlated.
    \end{itemize}\bigskip
    \item In the third plot above:
    \begin{itemize}
        \item Large values of $X$ mainly correspond with \blankul{3cm} values of $Y$.
        \item[] Small values of $X$ mainly correspond with \blankul{3cm} values of $Y$.
        \item In this case, the two random variables appear to \blankul{1cm} be dependent / correlated.
    \end{itemize}
\end{itemize}\bigskip

Quantifying dependence\bigskip
\begin{itemize}
    \item So we know how dependence shows up in a scatterplot, so then how to we quantify (measure) it?
    \item[] On the plots above, we were looking at where $X$ and $Y$ were in relation to their respective means and then also how these related (interacted) with each other.
    \item[] In statistics / modeling, interaction means multiplying terms. So here is the function that we will start with:
    \begin{figure}[H]
            \centering\includegraphics[scale=0.5]{{"test-1/correlation-types"}.png}
    \end{figure}
    \item If $X$ and $Y$ are positively dependent (plot 1), then
    \[(X - \mu_X)(Y - \mu_Y)\]
    \item[] will be mostly \blankul{2cm}.
    \item[] For example, height and weight. Most of people taller than the average weigh more than the average. Most of people shorter than the average weigh less than the average.
    \item If $X$ and $Y$ are negatively dependent (plot 2), then 
    \[(X - \mu_X)(Y - \mu_Y)\]
    \item[] will be mostly \blankul{2cm}.
    \item[] For example, stress (e.g. on a mechanical part or system) and time to failure. More stress often results in shorter time to failure and less stress often leads to longer time to failure.
    \item If $X$ and $Y$ appear to not be dependent (plot 3), then 
    \[(X - \mu_X)(Y - \mu_Y)\]
    \item[] the positives and negatives will almost \blankul{3cm} (spread evenly in all ``quadrants'').
    \item So our function ``results'' match our visual explanation of dependence, but also want to emphasize ``mostly negative and mostly positive''. Some values could have the different sign from the most of others.
    \item[] We want to measure \textbf{overall tendency}. So we define the \blankul{3cm} of $(X - \mu_X)(Y - \mu_Y)$ as a measure of \textbf{linear dependence} between $X$ and $Y$.
\end{itemize}\bigskip

\newpage

Definition, theorems and properties of covariance\bigskip
\begin{itemize}
    \item Definition: The \textbf{covariance} of $X$ and $Y$ is the number defined by 
    \[\cov{X,Y} = E[(X - \mu_X)(Y - \mu_Y)]\]
    \item Notes:
    \begin{itemize}
        \item Covariance is a measure of how much two random variables change
        together.
        \item If the $\cov{X,Y} > 0$, then $X$ and $Y$ are \blankul{2.5cm} correlated. 
        \item If the $\cov{X,Y} < 0$, then $X$ and $Y$ are \blankul{2.5cm} correlated. 
        \item If the $\cov{X,Y} = 0$, then $X$ and $Y$ are \blankul{2.5cm}, which means there is \textbf{no linear dependence} between $X$ and $Y$.
        \item Covariance can measure only the \textbf{linear} dependence between two random variables.
        \item[] May not pick up on non-linear relationships (i.e. curvature).
    \end{itemize}
    \item Properties / theorems of covariance:\bigskip
    \begin{enumerate}[(i)]
        \item \textbf{Calculation}: If $(X,Y)$ is discrete, then
        \[E[(X - \mu_X)(Y - \mu_Y)] = \hspace{100pt}\]
        \item[] Examples: Calculate $\cov{X,Y}$ for the following two joint pmfs.\bigskip
        \begin{enumerate}[(a)]
            \item \hspace{10pt}
            \begin{tabular}{| c || c | c |}
                \hline
                \backslashbox{$y$}{$x$} & 1 & 2\\
        	\hline\hline
                1 & 4/8 & 1/8\\
                \hline
                2 & 1/8 & 2/8 \\
                \hline
            \end{tabular}
                
            \begin{figure}[H]
                \centering\includegraphics[scale=0.5]{{"test-1/covariance-demo"}.png}
            \end{figure}
            
            \item \hspace{10pt}
            \begin{tabular}{| c || c | c |}
                \hline
                \backslashbox{$y$}{$x$} & 1 & 2\\
        	\hline\hline
                1 & 1/16 & 6/16\\
                \hline
                2 & 7/16 & 2/16 \\
                \hline
            \end{tabular}\bigskip
        \end{enumerate}\bigskip
        \item Recall how with variance we had a definition and an alternate form that is easier to calculate by hand: $V(X) = E[(X - \mu_X)^2] = E(X^2) - [E(X)]^2$. We have a similar theorem for covariance:
        \item[] \textbf{Alternate calculation for covariance}: For any random variables $X$ and $Y$,
        \[\cov{X,Y} = E(XY) - E(X) \cdot E(Y)\]
        \item[] Proof:\vspace{100pt}
        \item[] Example: Back to the investor with two asset random variables $X$ and $Y$. We have previously calculated each of these pieces and thus:\vspace{30pt}
        \item[] The \blankul{2.5cm} covariance means that as one investment performs above average, the other tends to perform \blankul{2cm} average.\bigskip
        \item \textbf{Variance is a special case of covariance}:
        \[V(X) = \cov{X,X}\]
        \item[] Proof:\vspace{40pt}
        \item Order in covariance does not matter (i.e. \textbf{symmetric}).
        \[\cov{X,Y} = \cov{Y,X}\]
        \item[] Proof:\vspace{30pt}
        \item Covariance of a random variable and a constant is zero.\bigskip
        \[\text{If $c$ is a constant, then } \cov{X,c} = 0\]
        \item[] Proof:\vspace{20pt}\newpage%to force this line on same page
        \item Can factor out coefficients in covariance.
        \[\cov{aX,bY} = ab \cdot \cov{X,Y}\]
        \item[] Proof:\vspace{80pt}
        \item Can factor out coefficients, but added constants disappear.
        \[\cov{aX + c,bY + d} = ab \cdot \cov{X,Y}\]
        \item[] Like variance, the location shift does not influence covariance, which means it \\does not impact the \blankul{4cm} between $X$ and $Y$.
        \item[] Example: Suppose investment $X$ is now performing 1.2 times better than previously and they added 5 to investment $Y$. Find the new covariance of the two investments.\vspace{20pt}
        \item \textbf{Distributive property} of covariance.
        \[\cov{X, Y + Z} = \cov{X,Y} + \cov{X,Z}\]
        \item[] Proof:\vspace{100pt}
    \end{enumerate}
\end{itemize}\bigskip

Independence and covariance\bigskip
\begin{itemize}
    \item Theorem: If $X$ and $Y$ are independent random variables, then
    \[\cov{X,Y} = 0\]
    \item Proof:\vspace{100pt}
    \item Independent vs uncorrelated:
    \begin{itemize}
        \item If $X$ and $Y$ are independent, there does not exist \textbf{any type of dependence} between $X$ and $Y$.
        \item If $X$ and $Y$ are uncorrelated (i.e. $\cov{X,Y} = 0$), there is no \blankul{2cm} dependence between $X$ and $Y$.
        \item[] But, there may be some other type of relationship.
        \item Thus, independent random variables \blankul{1cm} be uncorrelated, but uncorrelated random variables \blankul{2cm} be independent.
        \item[] Summary:\vspace{100pt}
    \end{itemize}
    \item Example: Let $f(x,y) = 1/3$ \quad for $(x,y) = (0,1), (1,0), (2,1)$.\vspace{20pt}
    \begin{enumerate}[(a)]
        \item Are $X$ and $Y$ independent?\vspace{50pt}
        \item Are $X$ and $Y$ uncorrelated?\vspace{100pt}
    \end{enumerate}
\end{itemize}\bigskip

\newpage%so example is all on same page

Interpreting covariance\bigskip
\begin{itemize}
    \item We have already shown that we can determine the \textbf{direction} of the relationship based on the sign of the covariance, this is a useful interpretation.
    \item[] However, we cannot use covariance to measure the \textbf{strength} of the relationship.
    \item[] This is because its value depends on the scale of measurement.
    \item Example demonstrating this: Suppose $(X,Y)$ = (income, savings) in dollars and $(X',Y') = (100X,100Y)$ = (income, savings) in cents. Further, let $\cov{X,Y} = 3$.\bigskip
    \[\cov{X',Y'} = \hspace{300pt}\]
    \item[] We \blankul{1.5cm} say that the linear dependence between income and savings in cents is stronger than that in dollars because \blankul{6cm}, all we know is \blankul{2cm}.
    \item Because of this, covariance cannot be used as an absolute measure of linear dependence (i.e. a single measure that tells us everything, direction and strength).
    \item[] So how can we improve it?
\end{itemize}\bigskip

\bu{Correlation}\bigskip

Definition\bigskip
\begin{itemize}
    \item Motivation: The problem can be eliminated by \blankul{3cm} the covariance value.
    \item Definition: As an absolute measure of dependence, the \textbf{correlation coefficient} of $X$ and $Y$ is the number defined by
    \[\rho_{XY} = \corr{X,Y} = \frac{\cov{X,Y}}{\sqrt{V(X) V(Y)}} = \frac{\cov{X,Y}}{\sigma_X \sigma_Y}\]
    \item Continuing previous example:\bigskip
    \[\corr{X',Y'} = \frac{\cov{X',Y'}}{\sqrt{V(X') V(Y')}} = \hspace{200pt}\]
    \item[] Thus, the correlation is unit-free and \blankul{2.5cm} by change in scale or location.
    \item Back to investor example: It can be shown that $V(X) = 40$ and $V(Y) = 25$ and we previously found $\cov{X,Y} = -13$. Thus\vspace{40pt}
\end{itemize}\bigskip

Properties of correlation\bigskip
\begin{itemize}
    \item We are going derive these!
    \item Covariance measures linear dependence, so lets see what happens to correlation when there is a perfect linear relationship, i.e. $Y = aX + b$.
    \begin{itemize}
        \item Sidenote: This is called a deterministic (or functional) relationship / model as opposed to stochastic (or probabilistic or statistical), which is when there is some randomness involved.\vspace{20pt}
    \end{itemize}
    \item[] If $Y = aX + b$,\vspace{100pt}
    \item[] Thus when $X$ and $Y$ are linearly related, the correlation coefficient is \blankul{1cm} when the slope of the straight line is positive and \blankul{1cm} when the slope is negative.
    \item To see what might happen when $X$ and $Y$ are not linearly related, we can look at the extreme case when $X$ and $Y$ are independent and have no systematic dependence.\vspace{40pt}
    \item[] Obviously $\rho_{XY} = \hspace{15pt}$ whenever $\cov{X,Y} = \hspace{15pt}$. Keep in mind that variables can be uncorrelated without being independent.
    \item Putting these two pieces together, it can be shown that for any random variables $X$ and $Y$,
    \[ \le \rho_{XY} \le \hspace{30pt}\]
    \begin{figure}[H]
            \centering\includegraphics[scale=0.4]{{"test-1/correlation-scale"}.png}
    \end{figure}
    \begin{itemize}
        \item Possible values of $\rho_{XY}$ lie on a continuum between -1 and 1.
        \item Values of $\rho_{XY}$ close to $\pm 1$ are interpreted as an indication of a high level of linear association between $X$ and $Y$.
        \item Values of $\rho_{XY}$ near 0 are interpreted as implying little or no linear relationship between $X$ and $Y$.
    \end{itemize}
    \item Formally, we can state these properties in the following theorem:
    \item[] Theorem: For any random variable $X$ and $Y$,
    \begin{enumerate}[(i)]
        \item $-1 \le \rho_{XY} \le 1$ 
        \item[] The sign represents the direction of linear dependence between $X$ and $Y$.
        \item[] $\lvert \rho_{XY} \rvert$ represents the magnitude of dependence. 
        \item $\rho_{XY} = 1$ if and only if there exist numbers $a > 0$ and $b$ such that \\$P(Y = aX + b) = 1$.
        \item[] $X$ and $Y$ have perfect positive correlation. As $X$ increases, $Y$ \blankul{3cm}.
        \item $\rho_{XY} = -1$ if and only if there exist numbers $a < 0$ and $b$ such that \\$P(Y = aX + b) = 1$. 
        \item[] $X$ and $Y$ have perfect negative correlation. As $X$ increases, $Y$ \blankul{3cm}.
        \item When $\rho_{XY} = 0$, $X$ and $Y$ are \blankul{3cm}.
    \end{enumerate}
\end{itemize}\bigskip

Examples\bigskip
\begin{enumerate}
    \item Is the dependence between $X_1$ and $X_2$ stronger than the dependence between $Y_1$ and $Y_2$?
    \begin{enumerate}
        \item $\cov{Y_1,Y_2} = 0.4$, $\cov{X_1,X_2} = 0.6$
        \item $\cov{Y_1,Y_2} = 0.4$, $\cov{X_1,X_2} = -0.6$
        \item $\corr{Y_1,Y_2} = 0.4$, $\corr{X_1,X_2} = 0.6$
        \item $\corr{Y_1,Y_2} = 0.4$, $\corr{X_1,X_2} = -0.6$
    \end{enumerate}\bigskip
    \item Find $\rho_{XY}$:
    \begin{enumerate}
        \item[] (a) $\corr{X,Y} = $ \hspace{80pt} (b) $\corr{X,Y} = $
        \begin{figure}[H]
            \centering\includegraphics[scale=0.5]{{"test-1/correlation-example-one"}.png}
        \end{figure}\newpage
        \item[] (c) $\corr{X,Y} = $ \hspace{80pt} (d) $\corr{X,Y} = $
        \begin{figure}[H]
            \centering\includegraphics[scale=0.5]{{"test-1/correlation-example-zero"}.png}
        \end{figure}
    \end{enumerate}
\end{enumerate}\vspace{20pt}

 \bu{Variance of $\boldsymbol{X + Y}$}\bigskip

Derivation\bigskip
\begin{itemize}
    \item We stated at the start that $V(X +Y) \ne V(X) + V(Y)$. Now that we understand covariance, we can see why!
    \item Let $X$ and $Y$ be two random variables:\vspace{150pt}
    \item Theorem: \textbf{Variance of a sum of two random variables}
    \[V(X + Y) = \hspace{100pt}\]
    \item Examples:
    \begin{enumerate}
        \item Back to the investor with two assets $X$ and $Y$, find the $V(X + Y)$:\vspace{30pt}\newpage
        \item Let $f(x,y) = x + \frac{3}{2}y^2$ \quad for $0 \le x \le 1, 0 \le y \le 1$. Find the variance of $X + Y$.\vspace{300pt}
    \end{enumerate}
\end{itemize}\bigskip

Variance of $X + Y$ when independent\bigskip
\begin{itemize}
    \item Derivation: If $X$ and $Y$ are independent, then\vspace{50pt}
    \item Theorem: \textbf{Variance of a sum of two independent random variables}
    \[\text{If } X \ind Y, \text{ then } V(X + Y) = \hspace{100pt}\]
    \item[] $\Longrightarrow$ Can only JUST add variances when independent.
    \item Example: Let $X \follow{Exp}(\lambda = 1)$ and $Y \follow{Exp}(\lambda = 3)$. If $X \ind Y$, find $V(X + Y)$.\vspace{30pt}
\end{itemize}\bigskip


\end{document}